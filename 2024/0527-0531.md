# Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models
[Meteor: 基于 Mamba 的大型语言和视觉模型的推理遍历](https://arxiv.org/abs/2405.15574)

大型语言和视觉模型 (LLVMs) 的快速发展得益于视觉指令调优的进步。最近，开源的 LLVMs 整理了高质量的视觉指令调优数据集，并利用额外的视觉编码器或多个计算机视觉模型来缩小与强大的闭源 LLVMs 的性能差距。这些进步归因于多方面信息对多种能力的需求，包括基本的图像理解、关于常识和非对象概念（例如图表、图解、符号、标志和数学问题）的现实世界知识，以及解决复杂问题的逐步过程。基于多方面信息，我们提出了一种新的高效 LLVM，称为 Meteor，利用多方面推理来增强理解和回答能力。为了嵌入包含丰富信息的冗长推理，我们采用了 Mamba 架构，能够以线性时间复杂度处理顺序数据。我们引入了一种新的推理遍历概念，有助于高效地嵌入推理。随后，骨干多模态语言模型 (MLM) 通过推理生成答案进行训练。通过这些方法，Meteor 在多个需要多种能力的评估基准上实现了显著的视觉语言性能提升，而无需扩大模型规模或使用额外的视觉编码器和计算机视觉模型。

# ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models
[ConvLLaVA: 用作大型多模态模型视觉编码器的分层骨干](https://arxiv.org/abs/2405.15738)

高分辨率大型多模态模型 (LMMs) 面临着过多视觉 Token 和二次视觉复杂度的挑战。目前的高分辨率 LMMs 解决了二次复杂度问题，但仍然生成过多的视觉 Token。然而，视觉 Token 的冗余是关键问题，因为它会导致更多的计算量。为了缓解这个问题，我们提出了 ConvLLaVA，采用 ConvNeXt（一种分层骨干）作为 LMM 的视觉编码器来替代 Vision Transformer (ViT)。ConvLLaVA 将高分辨率图像压缩为信息丰富的视觉特征，有效防止了过多视觉 Token 的生成。为了增强 ConvLLaVA 的能力，我们提出了两个关键优化。由于低分辨率预训练的 ConvNeXt 在直接应用于高分辨率时表现不佳，我们对其进行了更新以弥合差距。此外，由于 ConvNeXt 原始的压缩比不足以处理更高分辨率的输入，我们训练了一个连续阶段来进一步压缩视觉 Token，从而减少冗余。这些优化使 ConvLLaVA 能够支持 1536x1536 分辨率的图像输入，仅生成 576 个视觉 Token，能够处理任意纵横比的图像。实验结果表明，我们的方法在主流基准上达到了与最先进模型竞争的性能。ConvLLaVA 模型系列可在 https://github.com/alibaba/conv-llava 公共获得。

# Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization
[Grokked Transformers 是隐式推理者：通向泛化边缘的机制之旅](https://arxiv.org/abs/2405.15071)

我们研究了 transformers 是否可以学会在参数知识上进行隐式推理，这是一项即使是最强大的语言模型也难以胜任的技能。我们专注于两种代表性推理类型：组合和比较，结果一致发现 transformers 可以学会隐式推理，但只有通过 grokking，即远超过拟合的长期训练。不同推理类型的泛化水平也有所不同：面对分布外的例子时，transformers 在组合上无法系统地泛化，但在比较上成功泛化。我们在训练过程中深入研究了模型的内部机制，进行了分析实验，揭示了：1）grokking 背后的机制，如泛化电路的形成及其与泛化和记忆电路相对效率的关系，2）系统性与泛化电路配置之间的联系。我们的研究结果指导了数据和训练设置，以更好地诱导隐式推理，并建议对 transformer 架构进行潜在改进，如鼓励跨层知识共享。此外，我们展示了在具有大搜索空间的挑战性推理任务中，无论是使用 GPT-4-Turbo 还是基于非参数记忆的 Gemini-1.5-Pro，都表现不佳，而完全 grokked 的 transformer 可以达到接近完美的准确性，展示了参数记忆在复杂推理中的强大能力。

# An Introduction to Vision-Language Modeling
[视觉-语言模型导论](https://arxiv.org/abs/2405.17247)

随着大型语言模型 (LLMs) 最近的流行，已经有多次尝试将它们扩展到视觉领域。从能在陌生环境中引导我们的视觉助手，到只需高级文本描述即可生成图像的生成模型，视觉-语言模型 (VLM) 的应用将显著影响我们与技术的关系。然而，要提高这些模型的可靠性，还有许多挑战需要解决。语言是离散的，而视觉在一个更高维的空间中演化，概念并不总是容易离散化。为了更好地理解将视觉映射到语言的机制，我们介绍了这篇关于 VLM 的导论，希望能帮助任何想进入该领域的人。首先，我们介绍 VLM 是什么，它们如何工作，以及如何训练它们。接着，我们讨论了评估 VLM 的方法。虽然这项工作主要关注将图像映射到语言，但我们也讨论了将 VLM 扩展到视频的可能性。

# Transformers Can Do Arithmetic with the Right Embeddings
[Transformers 通过正确的嵌入可以做算术](https://arxiv.org/abs/2405.17399)

transformers 在算术任务上的表现不佳，主要是因为它们无法跟踪大量数字中的每个数字的确切位置。我们通过为每个数字添加一个嵌入来解决这个问题，该嵌入编码了其相对于数字起始位置的位置。除了这些嵌入本身带来的提升之外，我们还展示了这种修正使架构修改（如输入注入和循环层）能够进一步提高性能。解决了位置问题后，我们可以研究 transformers 的逻辑外推能力。它们能解决比训练数据中更大、更复杂的算术问题吗？我们发现，仅用一个 GPU 训练一天，使用 20 位数字，就可以达到最先进的性能，在 100 位数字的加法问题上达到 99% 的准确率。最后，我们展示了这些在数值上的提升也改进了其他多步推理任务，包括排序和乘法。

# Phased Consistency Model
[相位一致性模型](https://arxiv.org/abs/2405.18407)

一致性模型 (CM) 最近在加速扩散模型生成方面取得了显著进展。然而，其在潜在空间中基于文本的高分辨率图像生成 (LCM) 的应用仍然不尽如人意。在本文中，我们识别了当前 LCM 设计中的三个关键缺陷。我们调查了这些限制背后的原因，并提出了相位一致性模型 (PCM)，它扩展了设计空间并解决了所有识别出的限制。我们的评估表明，PCM 在 1-16 步生成设置中显著优于 LCM。虽然 PCM 专为多步优化而设计，但它在单步生成结果上甚至优于或媲美之前专门设计的单步方法。此外，我们展示了 PCM 方法的多功能性和视频生成的适用性，使我们能够训练出最先进的少步文本到视频生成器。更多详细信息可在 https://g-u-n.github.io/projects/pcm/ 查看。

# MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series
[MAP-Neo: 高效且透明的双语大型语言模型系列](https://arxiv.org/abs/2405.19327)

大型语言模型 (LLMs) 近年来在不同任务中取得了前所未有的性能。然而，由于商业利益，最具竞争力的模型如 GPT、Gemini 和 Claude 被封闭在专有接口后，未公开训练细节。最近，许多机构开源了几个强大的 LLMs，如 LLaMA-3，可与现有的闭源 LLMs 相媲美。然而，仅提供了模型的权重，而大多数细节（如中间检查点、预训练语料库和训练代码等）未公开。为了提高 LLMs 的透明度，研究界已经形成了开源真正开源 LLMs (如 Pythia、Amber、OLMo) 的社区，提供了更多细节（如预训练语料库和训练代码）。这些模型大大推动了对这些大型模型的科学研究，包括它们的优点、缺点、偏见和风险。然而，我们观察到现有的真正开源 LLMs 在推理、知识和编码任务上仍然逊色于具有类似模型规模的现有最先进 LLMs。为此，我们开源了 MAP-Neo，一个高效且透明的双语语言模型，具有 70 亿参数，从头开始在 4.5 万亿高质量 Token 上训练。我们的 MAP-Neo 是第一个完全开源的双语 LLM，性能可与现有的最先进 LLMs 相媲美。此外，我们开源了所有细节以重现我们的 MAP-Neo，包括清理后的预训练语料库、数据清理管道、检查点和优化良好的训练/评估框架。最后，我们希望 MAP-Neo 能增强开源研究社区，并激发更多创新和创造力，以促进 LLMs 的进一步改进。

# Jina CLIP: Your CLIP Model Is Also Your Text Retriever
[Jina CLIP: 你的 CLIP 模型也是你的文本检索器](https://arxiv.org/abs/2405.20204)

对比语言-图像预训练 (CLIP) 广泛用于训练模型，将图像和文本对齐到一个共同的嵌入空间，通过将它们映射到固定大小的向量。这些模型是多模态信息检索和相关任务的关键。然而，CLIP 模型在仅文本任务中的表现通常不如专门的文本模型。这为信息检索系统带来了效率低下的问题，这些系统为仅文本和多模态任务分别保持不同的嵌入和模型。我们提出了一种新的多任务对比训练方法来解决这个问题，我们使用它训练了 jina-clip-v1 模型，在文本-图像和文本-文本检索任务上达到了最先进的性能。