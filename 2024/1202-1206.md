## GRAPE: Generalizing Robot Policy via Preference Alignment
[GRAPE: 通过偏好对齐泛化机器人策略](https://arxiv.org/abs/2411.19309)

尽管视觉-语言-动作 (VLA) 模型在多种机器人任务中取得了进展，但其泛化能力受限，主要因完全依赖成功轨迹的行为克隆。此外，这些模型常针对不同设置下的专家演示进行微调，导致分布偏差，限制了其对多样化操作目标（如效率、安全性和任务完成度）的适应性。为此，我们提出 GRAPE: 通过偏好对齐泛化机器人策略。具体来说，GRAPE 在轨迹层面对齐 VLA，并从成功与失败试验中隐式建模奖励，以提升对多样化任务的泛化能力。同时，GRAPE 将复杂任务分解为独立阶段，并通过大型视觉-语言模型提出的关键点，利用定制时空约束自动引导偏好建模。这些约束灵活，可根据不同目标（如安全性、效率或任务成功）进行定制。我们在真实与模拟环境中广泛评估 GRAPE。实验显示，GRAPE 显著提升最先进 VLA 模型的性能，领域内与未见任务的成功率分别提高 51.79% 和 60.36%。此外，GRAPE 可与多种目标对齐，如安全性与效率，分别降低碰撞率 44.31% 和轨迹步长 11.15%。所有代码、模型及数据均可在 https://grape-vla.github.io/ 获取。

## Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS
[超越示例：通过 MCTS 实现上下文学习中的高层自动化推理范式](https://arxiv.org/abs/2411.18478)

上下文学习 (ICL) 使大语言模型 (LLMs) 能够通过复杂的提示和高品质的演示来处理下游任务。然而，传统 ICL 范式在面对复杂数学推理任务时显示出局限性，主要原因在于其对示例质量的严重依赖以及在挑战性场景中需要人为干预。为解决这些局限，本文提出 HiAR-ICL，这是一种在 ICL 中的高层自动化推理范式，它将重点从具体示例转移到抽象思维模式，扩展了 ICL 中传统上下文的概念。HiAR-ICL 引入了五个原子推理动作作为构建链式结构模式的基本组件。我们使用蒙特卡洛树搜索 (MCTS) 探索推理路径，并构建思维卡片以指导后续推理。随后，我们开发了一个认知复杂度框架，该框架动态地将问题与适当的思维卡片匹配。实验结果表明 HiAR-ICL 的有效性，在 MATH 基准测试中使用 Qwen2.5-7B-Instruct 达到了最先进的准确率 (79.6%)，超过了 GPT-4o (76.6%) 和 Claude 3.5 (71.1%)。

## Video Depth without Video Models
[无视频模型的视频深度估计](https://arxiv.org/abs/2411.19189)

视频深度估计通过推断每一帧的密集深度，将单目视频片段提升至3D。近年来，随着大型基础模型的兴起和合成训练数据的使用，单图像深度估计的进展重新激发了对视频深度的兴趣。然而，简单地将单图像深度估计器应用于视频的每一帧忽略了时间连续性，这不仅会导致闪烁，而且在相机运动导致深度范围突然变化时也可能失效。一个明显且合理的解决方案是基于视频基础模型构建，但这些模型本身存在局限性；包括昂贵的训练和推理、不完美的3D一致性，以及固定长度（短）输出的拼接程序。我们退一步，展示了如何将单图像潜在扩散模型（LDM）转化为最先进的视频深度估计器。我们的模型，称为RollingDepth，有两个主要成分：（i）一个多帧深度估计器，源自单图像LDM，将非常短的视频片段（通常是帧三元组）映射到深度片段。（ii）一个基于优化的鲁棒配准算法，最佳地将以不同帧率采样的深度片段组装成一致的视频。RollingDepth能够高效处理包含数百帧的长视频，并提供比专用视频深度估计器和表现优异的单帧模型更准确的深度视频。项目页面：rollingdepth.github.io。

## X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models
[X-Prompt: 在自回归视觉语言基础模型中实现通用上下文图像生成](https://arxiv.org/abs/2412.01824)

上下文生成是大语言模型 (LLMs) 开放任务泛化能力的关键组成部分。通过利用少量示例作为上下文，LLMs 可以执行域内和域外任务。基于 LLMs 构建的最新自回归视觉语言模型 (VLMs) 在文本到图像生成方面展示了令人印象深刻的性能。然而，上下文学习在通用图像生成任务中的潜力在很大程度上仍未被探索。为了解决这个问题，我们引入了 X-Prompt，这是一个纯粹的自回归大视觉语言模型，旨在在统一的上下文学习框架内，在广泛的已见和未见图像生成任务中提供竞争性能。X-Prompt 包含一个专门设计，能够高效压缩上下文示例中的重要特征，支持更长的上下文 Token 序列，并提高其对未见任务的泛化能力。统一的文本和图像预测训练任务使 X-Prompt 能够通过上下文示例增强的任务理解来处理通用图像生成。广泛的实验验证了模型在多样化的已见图像生成任务中的性能及其对先前未见任务的泛化能力。

## o1-Coder: an o1 Replication for Coding
[o1-Coder: 面向编码的 o1 模型复制](https://arxiv.org/abs/2412.00154)

该技术报告介绍了 O1-CODER，这是一个专注于编码任务的 OpenAI o1 模型复制尝试。它结合了强化学习 (RL) 和蒙特卡洛树搜索 (MCTS)，以提升模型的系统 2 (System-2) 推理能力。框架包括：训练一个测试用例生成器 (TCG) 进行标准化代码测试；利用 MCTS 生成包含推理过程的代码数据；并通过迭代微调策略模型，先产生伪代码，再生成完整代码。报告还探讨了在实际应用中部署类似 o1 模型的机遇与挑战，建议转向系统 2 (System-2) 范式，并强调环境状态更新的重要性。模型进展与实验结果将在后续版本中报告。所有源代码、精选数据集及衍生模型将在 https://github.com/ADaM-BJTU/O1-CODER 公开。

## Open-Sora Plan: Open-Source Large Video Generation Model
[Open-Sora计划：开源大型视频生成模型](https://arxiv.org/abs/2412.00131)

我们介绍Open-Sora计划，这是一个开源项目，旨在开发一个能够根据用户输入生成高分辨率长视频的大型生成模型。该项目涵盖了视频生成的全过程，包括Wavelet-Flow变分自编码器、联合图像-视频稀疏去噪器以及多种条件控制器。此外，我们还设计了多种辅助策略以提高训练和推理效率，并构建了一个多维数据整理管道，以获取高质量的数据。得益于这些高效的设计，Open-Sora计划在定性和定量评估中均取得了显著的视频生成效果。我们希望这些精心设计和实践经验能够为视频生成研究领域带来启发。所有代码和模型权重均已公开，详见https://github.com/PKU-YuanGroup/Open-Sora-Plan。

## Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis
[Switti: 为文本到图像合成设计尺度感知的 Transformer](https://arxiv.org/abs/2412.01819)

本文介绍 Switti，一种用于文本到图像生成的尺度感知 Transformer。我们从现有的下一尺度预测自回归模型出发，首先探索其应用于 T2I 生成，并提出架构修改以提升其收敛性和整体性能。我们发现，预训练的尺度感知自回归模型的自注意力图对先前尺度依赖性较弱。基于此，我们提出了一种非自回归模型，该模型在略微提升生成质量的同时，采样速度提高约 11%，内存使用量更低。此外，我们发现高分辨率尺度上的无分类器引导通常是不必要的，甚至可能损害性能。通过在这些尺度上禁用引导，我们额外实现了约 20% 的采样加速，并显著改进了细粒度细节的生成。广泛的人类偏好研究和自动化评估显示，Switti 优于现有的 T2I 自回归模型，并与最先进的 T2I 扩散模型竞争，同时速度最高可达 7 倍。

## VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation
[VideoGen-of-Thought: 一个用于多镜头视频生成的协作框架](https://arxiv.org/abs/2412.02259)

当前的视频生成模型擅长生成短片段，但在创建多镜头、电影般的视频方面仍面临挑战。现有模型在丰富的计算资源支持下，通过大规模数据训练，但由于通常以单镜头为目标进行训练，因此在保持连贯剧本的逻辑情节和视觉一致性方面仍然不足。为此，我们提出了 VideoGen-of-Thought (VGoT)，这是一个专门为多镜头视频生成设计的协作且无需训练的架构。VGoT 的设计考虑了以下三个目标。多镜头视频生成：我们将视频生成过程分为结构化的模块化序列，包括 (1) 剧本生成，将简短的故事转化为每个镜头的详细提示；(2) 关键帧生成，负责创建忠实于角色描绘的视觉一致的关键帧；(3) 镜头级视频生成，将剧本和关键帧中的信息转化为镜头；(4) 平滑机制，确保一致的多镜头输出。合理的叙事设计：受电影剧本写作启发，我们的提示生成方法涵盖五个关键领域，确保整个视频的逻辑一致性、角色发展和叙事流程。跨镜头一致性：我们通过利用跨镜头的身份保持 (IP) 嵌入来确保时间和身份一致性，这些嵌入自动从叙事中创建。此外，我们引入了一个跨镜头平滑机制，该机制集成了一个重置边界，有效结合相邻镜头的潜在特征，实现平滑过渡并保持视频的视觉连贯性。我们的实验表明，VGoT 在生成高质量、连贯的多镜头视频方面超越了现有的视频生成方法。

## Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability
[关键 Token 的重要性：Token 级别的对比估计增强大语言模型的推理能力](https://arxiv.org/abs/2411.19943)

大语言模型 (LLMs) 在推理任务中表现优异。它们通过自回归 Token 生成构建推理轨迹，形成连贯的思维链。本研究探讨了个别 Token 对推理任务最终结果的影响。我们发现存在“关键 Token”，这些 Token 会导致推理轨迹出错。具体而言，当强制 LLMs 解码非关键 Token 时，它们通常会产生积极结果。基于此观察，我们提出了一种新方法——cDPO——旨在在对齐过程中自动识别并奖励关键 Token。具体来说，我们采用对比估计方法自动识别关键 Token，通过比较正模型和负模型的生成概率实现。为此，我们对正模型和负模型分别在不同推理轨迹上进行微调，使其能够识别错误轨迹中导致错误结果的关键 Token。此外，为在对齐过程中进一步使模型与关键 Token 信息对齐，我们将传统 DPO 算法扩展至 Token 级别 DPO，并利用正模型和负模型的差异概率作为 Token 级别 DPO 学习的重要权重。在 GSM8K 和 MATH500 基准测试中，使用 Llama-3 (8B 和 70B) 及 deepseek-math (7B) 模型的实验结果验证了 cDPO 方法的有效性。

## MALT: Improving Reasoning with Multi-Agent LLM Training
[MALT: 通过多智能体大语言模型训练提升推理能力](https://arxiv.org/abs/2412.01928)

实现大语言模型之间的有效协作是开发能够解决复杂问题的自主系统的关键步骤。虽然大语言模型通常被用作单模型生成器，人类对其输出进行批评和改进，但联合训练的协作模型的潜力在很大程度上仍未被探索。虽然多智能体通信和辩论环境中取得了有希望的结果，但在训练模型共同完成任务方面进展甚微。在本文中，我们迈出了“多智能体大语言模型训练”（MALT）在推理问题上的第一步。

我们的方法采用了一种顺序多智能体设置，其中异构大语言模型被分配了专门的角色：生成器、验证器和细化模型迭代地解决问题。我们提出了一种基于轨迹扩展的合成数据生成过程，以及一种由联合结果驱动的奖励分配策略。这使得我们的训练后设置能够利用正负轨迹来自主地提升每个模型在联合顺序系统中的专门能力。

我们在 MATH、GSM8k 和 CQA 上评估了我们的方法，其中 MALT 在 Llama 3.1 8B 模型上分别实现了相对于相同基线模型的 14.14%、7.12% 和 9.40% 的相对改进。这展示了在数学和常识推理问题上多智能体协作能力方面的早期进展。更广泛地说，我们的工作为多智能体大语言模型训练方法的研究提供了具体方向。

## PaliGemma 2: A Family of Versatile VLMs for Transfer
[PaliGemma 2: 一个用于迁移的多功能 VLM 家族](https://arxiv.org/abs/2412.03555)

PaliGemma 2 是基于 Gemma 2 系列语言模型的 PaliGemma 开放 VLM 的升级版。我们结合了 PaliGemma 也使用的 SigLIP-So400m 视觉编码器与 Gemma 2 全系列模型，从 2B 模型一直到 27B 模型。我们在多个阶段以三种分辨率 (224px, 448px, 和 896px) 训练这些模型，以通过微调赋予它们广泛的迁移能力。由此产生的多尺寸和多分辨率基模型家族使我们能够研究影响迁移性能的因素（如学习率），并分析任务类型、模型大小和分辨率之间的相互作用。我们进一步扩展了迁移任务的数量和广度，超出了 PaliGemma 的范围，包括不同的 OCR 相关任务，如表格结构识别、分子结构识别、乐谱识别，以及长细粒度描述和放射报告生成，在这些任务上 PaliGemma 2 表现出色。

## SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance
[SNOOPI: 强化一步扩散蒸馏与精准引导](https://arxiv.org/abs/2412.02687)

最近的研究在将多步文本到图像扩散模型蒸馏为一步模型方面取得了显著进展。领先的高效蒸馏技术，如 SwiftBrushv2 (SBv2)，甚至在资源有限的情况下超越了教师模型的性能。然而，我们的研究发现，由于在变分分数蒸馏 (VSD) 损失中使用了固定的引导尺度，SBv2 在处理不同扩散模型骨干时表现出不稳定性。此外，现有的一步扩散模型不支持负提示引导，这在实际图像生成中至关重要。本文提出了 SNOOPI，一种新颖的框架，旨在通过在训练和推理过程中增强一步扩散模型的引导来解决这些限制。首先，我们通过适当引导-SwiftBrush (PG-SB) 提高了训练稳定性，该方法采用了随机尺度的无分类器引导策略。通过调整教师模型的引导尺度，我们扩展了其输出分布，从而形成了一个更鲁棒的 VSD 损失，使得 SB 在保持竞争性能的同时，能够有效适应多种骨干模型。其次，我们提出了一种无需训练的方法，称为负向远离引导注意力 (NASA)，该方法通过交叉注意力机制将负提示融入一步扩散模型，以抑制生成图像中的不期望元素。实验结果显示，我们提出的方法在多个指标上显著提升了基线模型的性能。特别地，我们实现了 31.08 的 HPSv2 分数，为一​​步扩散模型设定了新的最先进基准。

## Imagine360: Immersive 360 Video Generation from Perspective Anchor
[Imagine360: 从透视锚点生成沉浸式360度视频](https://arxiv.org/abs/2412.03552)

360度视频提供了一种超沉浸式体验，允许观众从360度全方位探索动态场景。为了在360度视频格式中实现更用户友好和个性化的内容创作，我们寻求将标准透视视频提升为360度等距矩形视频。为此，我们引入了Imagine360，这是首个从透视视频生成高质量360度视频的框架，通过视频锚点生成具有丰富多样运动模式的360度视频。Imagine360通过几个关键设计，从有限的360度视频数据中学习细粒度的球面视觉和运动模式。1) 首先，我们采用了双分支设计，包括一个透视视频和一个全景视频去噪分支，为360度视频生成提供局部和全局约束，并通过在扩展的网络360度视频上微调的运动模块和空间LoRA层。2) 此外，设计了一个对映掩码来捕捉长程运动依赖关系，增强了对映像素在半球之间的反向相机运动。3) 为了处理多样化的透视视频输入，我们提出了适应不同视频掩码的仰角感知设计，这是由于帧间仰角变化引起的。大量实验表明，Imagine360在现有最先进的360度视频生成方法中实现了卓越的图形质量和运动连贯性。我们相信Imagine360有潜力推动个性化、沉浸式360度视频创作的发展。

## TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation
[TokenFlow: 多模态理解和生成的统一图像编码器](https://arxiv.org/abs/2412.03069)

我们提出了 TokenFlow，这是一种新颖的统一图像编码器，它弥合了多模态理解和生成之间长期存在的差距。先前的研究尝试使用单一的重建导向向量量化 (VQ) 编码器来统一这两个任务。我们观察到，理解和生成需要截然不同的视觉信息粒度。这导致了一个关键的权衡，特别是在多模态理解任务中显著影响了性能。TokenFlow 通过一种创新的二重码本架构来应对这一挑战，该架构分别学习语义和像素级特征，同时通过共享映射机制保持它们的对齐。这种设计使得通过共享索引可以直接访问对理解任务至关重要的高级语义表示和生成任务所需的细粒度视觉特征。我们的广泛实验证明了 TokenFlow 在多个维度上的优越性。利用 TokenFlow，我们首次证明了离散视觉输入在理解性能上可以超越 LLaVA-1.5 13B，平均提高了 7.2%。对于图像重建，我们在 384*384 分辨率下达到了 0.63 的强 FID 分数。此外，TokenFlow 在自回归图像生成方面建立了最先进的性能，在 256*256 分辨率下达到了 0.55 的 GenEval 分数，与 SDXL 取得了可比的结果。

## VisionZip: Longer is Better but Not Necessary in Vision Language Models
[VisionZip: Longer is Better but Not Necessary in Vision Language Models](https://arxiv.org/abs/2412.04467)

近期视觉语言模型的进展通过增加视觉 Token 的长度提升了性能，使其长度远超文本 Token，并显著提高了计算成本。然而，我们观察到，流行的视觉编码器如 CLIP 和 SigLIP 生成的视觉 Token 存在显著冗余。为此，我们提出了 VisionZip，一种简单而有效的方法，通过选择一组信息丰富的 Token 输入语言模型，减少视觉 Token 的冗余，提高效率的同时保持模型性能。VisionZip 可广泛应用于图像和视频理解任务，特别适合现实场景中的多轮对话，在这些场景中，现有方法往往表现不佳。实验结果表明，VisionZip 在几乎所有设置下性能提升至少 5%。此外，我们的方法显著提升了模型推理速度，将预填充时间提升至 8 倍，并使 LLaVA-Next 13B 模型在推理速度上超过 LLaVA-Next 7B 模型，同时性能更优。进一步地，我们分析了这种冗余的原因，并鼓励社区关注于提取更好的视觉特征，而非仅仅增加 Token 长度。我们的代码可在 https://github.com/dvlab-research/VisionZip 获取。

## Florence-VL: 增强视觉-语言模型的生成式视觉编码器与深度-广度融合
[Florence-VL: 增强视觉-语言模型的生成式视觉编码器与深度-广度融合](https://arxiv.org/abs/2412.04424)
我们提出了 Florence-VL，这是一个新的多模态大语言模型 (MLLMs) 家族，其视觉表示由 Florence-2 生成式视觉基础模型增强。与广泛使用的 CLIP 风格视觉 Transformer 不同，后者通过对比学习训练，Florence-2 能够捕捉不同层次和方面的视觉特征，这些特征更具适应性，可以应用于多样化的下游任务。我们提出了一种新颖的特征融合架构和一种创新的训练方法，能够有效地将 Florence-2 的视觉特征整合到预训练的大语言模型中，如 Phi 3.5 和 LLama 3。特别是，我们提出了“深度-广度融合 (DBFusion)”，以融合从不同深度和多个提示下提取的视觉特征。我们的模型训练包括对整个模型的端到端预训练，随后在精心设计的多样化开源数据集上对投影层和大语言模型进行微调，这些数据集包括高质量的图像标题和指令微调对。我们对 Florence-VL 视觉特征的定量分析和可视化显示，在视觉-语言对齐方面，它优于流行的视觉编码器，其中丰富的深度和广度发挥了重要作用。Florence-VL 在涵盖一般 VQA、感知、幻觉、OCR、图表、知识密集型理解等多个多模态和视觉中心基准测试中，显著优于现有的最先进 MLLMs。为了促进未来的研究，我们的模型和完整的训练方法已开源。
https://github.com/JiuhaiChen/Florence-VL

## NVILA: Efficient Frontier Visual Language Models
[NVILA: 高效前沿视觉语言模型](https://arxiv.org/abs/2412.04468)

视觉语言模型 (VLMs) 近年来在准确度方面取得了显著进展。然而，它们的效率较少受到关注。本文介绍了 NVILA，这是一系列旨在优化效率和准确度的开源 VLMs。在 VILA 的基础上，我们首先通过提高空间和时间分辨率来改进其模型架构，然后压缩视觉 Token。这种“先扩展再压缩”的方法使 NVILA 能够高效处理高分辨率图像和长视频。我们还系统地研究了如何提高 NVILA 在其整个生命周期中的效率，从训练和微调到部署。NVILA 在广泛的图像和视频基准测试中与许多领先的开放和专有 VLMs 相比，其准确度相当或超越。同时，它将训练成本降低了 4.5 倍，微调内存使用量减少了 3.4 倍，预填充延迟减少了 1.6-2.2 倍，解码延迟减少了 1.2-2.8 倍。我们即将发布我们的代码和模型，以促进可重复性。

## Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection
[Code-as-Monitor: 约束感知的视觉编程用于反应性和前瞻性机器人故障检测](https://arxiv.org/abs/2412.04455)

在闭环机器人系统中，自动检测和预防开放集 (open-set) 故障至关重要。最近的研究往往难以同时实现对发生后意外故障的反应性 (reactive) 检测和预防可预见故障的前瞻性 (proactive) 检测。为此，我们提出了 Code-as-Monitor (CaM)，一种利用视觉语言模型 (VLM) 进行开放集反应性和前瞻性故障检测的新范式。我们方法的核心是将这两项任务制定为统一的时空约束满足问题集 (spatio-temporal constraint satisfaction problems)，并使用 VLM 生成的代码对其进行实时评估。为了提高监控的准确性和效率，我们进一步引入了约束元素，将约束相关实体或其部分抽象为紧凑的几何元素。这种方法提供了更大的通用性，简化了跟踪，并通过将这些元素作为视觉提示来促进约束感知的视觉编程 (constraint-aware visual programming)。实验表明，与三个模拟器和真实世界环境中的基线 (baselines) 相比，CaM 在严重干扰下实现了 28.7% 的成功率提升和 31.8% 的执行时间减少。此外，CaM 可以与开环控制策略集成，形成闭环系统，从而在动态环境中实现杂乱场景中的长期任务。

## Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction
[Aguvis: 统一纯视觉智能体用于自主图形用户界面交互](https://arxiv.org/abs/2412.04454)

图形用户界面 (GUIs) 是人与计算机交互的关键，但视觉环境的复杂性和多变性使得自动化 GUI 任务仍具挑战。现有方法常依赖 GUI 的文本表示，导致泛化性、效率和可扩展性受限。本文介绍 Aguvis，一个基于统一纯视觉的框架，用于跨平台自主 GUI 智能体。我们的方法利用图像观察，将自然语言指令与视觉元素对齐，并采用一致动作空间确保跨平台泛化。为解决先前工作局限，我们在模型中集成显式规划和推理，增强其自主导航和与复杂数字环境交互的能力。我们构建大规模 GUI 智能体轨迹数据集，结合多模态推理和对齐，采用两阶段训练流水线，先关注通用 GUI 对齐，后进行规划和推理。通过全面实验，我们展示 Aguvis 在离线和真实世界在线场景中均超越先前最先进方法，据我们所知，实现首个独立执行任务的完全自主纯视觉 GUI 智能体，无需外部闭源模型协作。我们开源所有数据集、模型和训练配方，以促进未来研究，网址为 https://aguvis-project.github.io/。

## Evaluating Language Models as Synthetic Data Generators
[Evaluating Language Models as Synthetic Data Generators](https://arxiv.org/abs/2412.03679)

鉴于合成数据在语言模型 (LM) 训练后的应用日益广泛，LM 生成高质量数据的能力与其直接解决问题的能力同样重要。尽管先前研究侧重于开发有效的数据生成方法，但它们缺乏在统一环境下对不同 LM 作为数据生成器的系统比较。为填补这一空白，我们提出了 AgoraBench，这是一个基准，提供了标准化的环境和指标来评估 LM 的数据生成能力。通过使用 6 个 LM 生成 126 万个训练实例并训练 99 个学生模型，我们揭示了关于 LM 数据生成能力的关键见解。首先，我们观察到 LM 展现出不同的优势。例如，GPT-4o 擅长生成新问题，而 Claude-3.5-Sonnet 在改进现有问题方面表现更好。此外，我们的分析表明，LM 的数据生成能力与其问题解决能力并不必然相关。相反，多个内在的数据质量特征——包括响应质量、困惑度 (perplexity) 和指令难度——共同作为更好的指标。最后，我们证明了在输出格式和成本敏感模型选择方面的策略选择显著影响数据生成效果。

## A Noise is Worth Diffusion Guidance
[噪声即扩散引导](https://arxiv.org/abs/2412.03895)

扩散模型在生成高质量图像方面表现优异。然而，当前的扩散模型若无引导方法（如无分类器引导 (CFG)），则难以生成可靠图像。引导方法真的不可或缺吗？我们观察到，通过扩散反演获得的噪声即便在没有引导的情况下，也能重建出高质量图像。因此，我们将研究焦点转向去噪管道的初始噪声。通过将高斯噪声转换为“无引导噪声”，我们发现，微小的低幅度低频成分能显著提升去噪效果，从而无需引导，同时提升推理效率和内存利用率。基于此，我们提出了一种新方法 \ours，通过单一的初始噪声细化替代传统引导方法。这种经过细化的噪声使得在同一扩散流程中，无需引导即可生成高质量图像。我们的噪声细化模型采用高效的噪声空间学习策略，仅需 50K 文本-图像对数据，便能迅速收敛并展现出卓越性能。我们通过多维度指标验证了其有效性，并深入分析了细化噪声如何消除对引导的依赖。更多详情，请访问我们的项目页面：https://cvlab-kaist.github.io/NoiseRefine/。

