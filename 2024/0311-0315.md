# Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
[Gemini 1.5：跨数百万Token实现多模态理解](https://arxiv.org/abs/2403.05530)

在本报告中，我们展示了 Gemini 系列最新的模型，Gemini 1.5 Pro，这是一个计算高效的多模态混合专家模型。它能够从包含数百万 Token 的上下文中，如多个长文档及数小时的视频和音频，检索和推理细节信息。Gemini 1.5 Pro 在多模态长上下文检索任务中实现了几乎完美的检索能力，提升了长文档问答、长视频问答和长上下文自动语音识别的技术水平，并在一系列广泛的基准测试中达到或超过了 Gemini 1.0 Ultra 的表现。在探索 Gemini 1.5 Pro 长上下文处理能力的极限时，我们发现其在下一 Token 预测方面持续改进，并能在至少一千万 Token 的范围内实现超过 99% 的准确检索，比现有的模型如 Claude 2.1（200k）和 GPT-4 Turbo（128k）有着质的飞跃。最后，我们发现大语言模型在新的领域具有惊人的能力：当输入不到 200 名使用者的 Kalamang 语言的语法手册时，模型能够将英文翻译成 Kalamang，其水平类似于通过相同内容学习的人。

# ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment
[ELLA：将LLM集成到扩散模型中以增强语义对齐](https://arxiv.org/abs/2403.05135)

扩散模型在文本到图像生成领域展现出了卓越的性能。但多数常用模型仍采用 CLIP 作为文本编码器，限制了它们处理含有多个对象、详细属性、复杂关系和长文本对齐等密集提示的能力。在这篇文章中，我们介绍了一个名为 ELLA 的高效大语言模型适配器，它能够让文本到图像的扩散模型借助大语言模型 (LLM) 强化文本对齐，而不需要对 U-Net 或 LLM 进行额外训练。我们通过探索多种语义对齐连接设计，并提出了一个创新组件——时间步感知语义连接器 (TSC)，它能动态地从 LLM 提取依赖时间步的条件。这种方法使得在去噪过程的不同阶段，能够调整语义特征，帮助扩散模型更准确地解释复杂和长篇的提示信息。此外，ELLA 可以轻松地整合到社区模型和工具中，从而提高它们遵循提示的能力。为了评估文本到图像模型在处理密集提示的能力，我们引入了一个包含 1000 个密集提示的具有挑战性的基准测试，称为密集提示图基准 (DPG-Bench)。大量实验证明，相较于现有的先进方法，ELLA 在处理涉及多种属性和关系的复杂对象组合的密集提示方面表现更为出色。

# DeepSeek-VL: Towards Real-World Vision-Language Understanding
[DeepSeek-VL：实现现实世界的视觉-语言理解](https://arxiv.org/abs/2403.05525)

我们推出 DeepSeek-VL，一款面向现实世界视觉与语言理解应用的开源视觉-语言（VL）模型。这个模型围绕三大核心维度设计：多样性、可扩展性及广泛覆盖现实场景，包括网页截图、PDF、OCR、图表及知识型内容，以实现对实践环境的全面呈现。基于真实用户场景，我们建立了用例分类，并创建了相应的指令调整数据集，这大幅提升了模型在实际应用中的用户体验。DeepSeek-VL 针对现实世界的需求，引入了一种混合视觉编码器，能高效处理高分辨率（1024 x 1024）图像，同时保持较低的计算成本，确保模型能在不同视觉任务中捕捉关键的语义和细节信息。我们认为高效的视觉-语言模型应优先具备强大的语言处理能力。为了在预训练过程中保持大语言模型（LLM）的功能，我们探索了有效的 VL 预训练策略，从一开始就将 LLM 训练整合进来，并妥善处理视觉与语言之间的竞争动态。DeepSeek-VL 系列（1.3B 和 7B 模型）在实际应用中作为视觉-语言聊天机器人，提供了优秀的用户体验，它们在同等模型规模下在多种视觉-语言基准上展现了顶尖或具有竞争力的性能，并在语言中心基准测试中保持了稳健的表现。我们已将这两个模型公开，以促进基于这一基础模型的创新。

# Stealing Part of a Production Language Model
[窃取生产中的语言模型片段](https://arxiv.org/abs/2403.06634)

我们首次提出了一种模型窃取攻击方法，能从如 OpenAI 的 ChatGPT 或 Google 的 PaLM-2 等黑盒生产语言模型中精确提取重要信息。具体来说，在典型的 API 访问条件下，我们的攻击成功获取了变换器模型的嵌入投影层，并显现其对称性。仅需不到 20 美元，我们便能提取 OpenAI 的 Ada 和 Babbage 语言模型的完整投影矩阵。通过这种方式，我们首次验证了这些黑盒模型的隐藏维度分别为 1024 和 2048。我们还确定了 gpt-3.5-turbo 模型的确切隐藏维度，并估算出提取完整投影矩阵的成本不超过 2000 次查询。文章最后讨论了可能的防御措施和缓解方法，并探讨了未来可能对此攻击方法进行扩展研究的影响。


# Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU
[通过添加NVMe SSD在单GPU上实现100B模型的高效微调](https://arxiv.org/abs/2403.06504)

大语言模型的最新进展极大地推动了科技界的发展，这些模型之所以表现出色，是因为它们依赖大量的参数。然而，即使是最高内存容量达到 80GB 的 GPU 也难以支撑这些巨大的参数及其在执行随机梯度下降优化时的相关优化器状态。一种解决方案是集合多个 GPU 的内存来容纳这些大模型，但这对于预算有限的学术研究者来说成本过高。本文聚焦于在商用服务器上使用单个低端 GPU 来进行大模型微调，这对大多数 AI 研究者来说是可行的。在这种情境下，现有的先进技术 ZeRO-Infinity 面临两大问题：低效的数据交换导致 GPU 利用率低，以及受限的 CPU 内存容量限制了模型的训练规模。ZeRO-Infinity 主要是为高端 GPU 服务器优化的。针对这一挑战，我们提出了 Fuyou，一个低成本训练框架，它能在只有低端 GPU 和有限 CPU 内存的服务器上高效地微调高达 100B 参数的巨型模型。核心策略是将 SSD 与 CPU 之间的通信作为优化的一部分，系统地共同优化计算和数据交换，以最大化 GPU 的利用率。实验结果表明，Fuyou 能在消费级 GPU RTX 4090 上高效微调 175B 参数的 GPT-3 模型，而 ZeRO-Infinity 则无法完成这一任务；在训练 13B 参数的 GPT-3 小模型时，Fuyou 在 RTX 4090 GPU 上实现了 156 TFLOPS 的性能，相比之下 ZeRO-Infinity 只能达到 45 TFLOPS。


# MoAI: Mixture of All Intelligence for Large Language and Vision Models
[MoAI：整合各种智能的大型语言视觉模型](https://arxiv.org/abs/2403.07508)

大语言模型（LLMs）和指令调优的发展促成了当前大型语言视觉模型（LLVMs）指令调优的趋势。这一趋势包括为特定目标定制大量指令调优数据集，或扩展 LLVMs 来处理庞大的视觉语言（VL）数据。然而，现有 LLVMs 未能充分利用专业计算机视觉（CV）模型在图像分割、检测、场景图生成（SGG）和光学字符识别（OCR）等任务中对实际场景的深入理解。相反，它们主要依靠自身大语言模型的庞大容量和新兴功能。为此，我们引入了新的 LLVM——全智能混合体（MoAI），它利用从外部分割、检测、SGG 和 OCR 模型得到的辅助视觉信息。MoAI 通过两个新模块——MoAI-Compressor 和 MoAI-Mixer 来实现这一功能。MoAI-Compressor 会将外部 CV 模型的输出转化并压缩，以有效地用于 VL 任务。MoAI-Mixer 则将视觉特征、外部 CV 模型的辅助特征和语言特征结合起来，运用专家混合（Mixture of Experts）理念。通过这样的整合，MoAI 在众多零样本 VL 任务上显著胜过其他开源和闭源 LLVMs，尤其是在与现实世界场景理解相关的任务上，如对象的存在、位置、关系和 OCR，且无需增大模型规模或额外制定视觉指令调优数据集。


# Chronos: Learning the Language of Time Series
[Chronos：解码时间序列的语言](https://arxiv.org/abs/2403.07815)

我们推出了 Chronos，这是一个简单而有效的框架，专用于预训练的概率时间序列模型。Chronos 通过缩放和量化方法将时间序列数据转换为固定的词汇集，并利用交叉熵损失，在这些标记化的时间序列数据上训练现有的基于 Transformer 的语言模型架构。我们基于 T5 系列（参数规模从 20M 到 710M）预训练了 Chronos 模型，这些模型是在广泛的公共数据集上训练的，并且我们还生成了一个合成数据集，通过高斯过程来增强其泛化能力。在一个包括 42 个数据集的综合基准测试中，既包含传统的本地模型也包含深度学习方法，Chronos 模型展现了：（a）在训练语料库中包含的数据集上，显著优于其他方法；（b）相对于那些专门为这些新数据集训练的方法，Chronos 在新数据集上的零样本表现有时甚至更好。我们的结果证明了 Chronos 模型能够利用来自多个领域的时间序列数据，以提高在未见预测任务上的零样本精度，使预训练模型成为极大简化预测流程的有效工具。


# Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM
[Branch-Train-MiX：融合专家级大语言模型以形成混合专家系统](https://arxiv.org/abs/2403.07816)

我们研究了如何高效地训练大型语言模型（LLMs），使其在多个专业领域，如编程、数学推理和世界知识等方面具有能力。我们提出的方法，称为 Branch-Train-MiX (BTX)，从一个种子模型出发，该模型分支出来并以高效并行的方式训练专家模型，实现高吞吐率和低通信成本。在专家模型异步完成训练后，BTX 把它们的前向传播参数合并到混合专家（MoE）层中，并对其他参数进行平均处理，然后进入 MoE 微调阶段，学习 Token 级的路由。BTX 包含了两种特殊情形：Branch-Train-Merge 方法，它不包括学习路由的 MoE 微调阶段；以及稀疏再生，它忽略了异步训练专家的过程。相比于其他方法，BTX 在准确性和效率之间实现了最佳平衡。


# Gemma: Open Models Based on Gemini Research and Technology
[Gemma：基于Gemini研究与技术的开放模型](https://arxiv.org/abs/2403.08295)

本项工作推出了 Gemma，这是一系列轻量级、尖端的开放模型，基于创建 Gemini 模型的研究和技术开发而成。Gemma 模型在语言理解、推理和安全性方面的学术评测中表现出色。我们推出了两个版本的模型，分别拥有 2B 和 7B 参数，并提供了预训练和微调的检查点。在 18 项基于文本的任务中，Gemma 在 11 项中超越了同等规模的开放模型。我们对这些模型的安全性和责任性进行了详尽评估，并详细描述了模型开发过程。我们认为，负责任地发布大语言模型是提升模型安全性和推动大语言模型创新的关键。


# Simple and Scalable Strategies to Continually Pre-train Large Language Models
[持续预训练大语言模型的简单可扩展方法](https://arxiv.org/abs/2403.08763)

大语言模型（LLMs）通常会在数十亿个Token上进行预训练，而当新数据出现时，它们需要从头开始重新训练。一种更高效的方法是不断地对这些模型进行预训练，与完全重新训练相比，这样可以显著节约计算资源。然而，由于新数据的引入导致的分布变化往往会使模型在旧数据上的性能下降或者对新数据的适应性变差。在本研究中，我们证明了通过简单且可扩展的方法，即调整学习率（Learning Rate, LR）的重新预热、重新衰减，以及重复使用旧数据，就能够达到与从零开始、利用所有可用数据进行全面训练相同的性能水平，这一点通过最终损失和语言模型（Language Model, LM）评估标准来验证。特别是，我们针对两种常见的LLM预训练数据集的语言变化（从英语到英语和从英语到德语）进行了实证研究，这些数据集在405M参数模型规模下涵盖了大量（数百亿个）Token。在选择了较为微弱但现实的英语到英语变化进行大规模实验后，我们还发现我们的持续学习策略能够与10B参数LLM的全面重新训练性能持平。我们的研究结果表明，利用简单且可扩展的持续学习策略，LLM能够成功进行更新，且仅需使用一小部分的计算资源即可达到与完全重新训练相当的效果。最后，受先前研究启发，我们提出了一种替代传统余弦学习率调度的方法，这种方法有助于避免由于学习率重新预热引起的遗忘问题，并且不受固定Token数量限制。



# MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training
[MM1：多模态大语言模型预训练的方法、分析与见解](https://arxiv.org/abs/2403.09611)

本研究探讨了如何构建高效的多模态大语言模型（MLLMs）。我们特别关注了架构组件和数据选择的重要性。通过对图像编码器、视觉-语言连接器及多种预训练数据的细致全面分析，我们提炼出了几条关键设计原则。例如，对于大规模的多模态预训练来说，精心选择图像-文本配对、图文交错及纯文本数据是非常重要的，这有助于在多个评估基准上达到最先进的少样本学习效果，优于其他已发布的预训练成果。进一步地，我们发现图像编码器、图像分辨率和图像Token数量对性能有重大影响，而视觉-语言连接器的设计则相对不那么关键。通过扩展我们的策略，我们开发了MM1，这是一个包含多达30B参数的多模态模型系列，既包括密集型模型，也包括混合专家（MoE）模型，它们在预训练指标上表现卓越，并在多种权威的多模态基准测试中经过监督微调后表现出色。得益于大规模预训练，MM1展现了如增强的上下文学习和多图像推理等优异特性，支持少样本链式推理。


# Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset
[利用WebSight数据集实现网页截图向HTML代码的转换](https://arxiv.org/abs/2403.09029)

在Web开发中应用视觉-语言模型（VLMs）是一种提升效率和推动无代码解决方案的有效策略。通过输入用户界面（UI）的截图或草图，VLM能够生成相应的代码，如HTML。虽然VLM在多项任务上已经取得了显著进展，但将截图转换成对应HTML代码的特定挑战还未得到充分研究。我们认为这主要是由于缺乏合适的高质量数据集所致。在这项工作中，我们介绍了WebSight，这是一个包含200万个HTML代码与相应截图对的合成数据集。我们对该数据集进行了VLM的微调训练，证明了它在将网页截图转换为实用HTML代码方面的高效能力。为了推动这个领域的研究发展，我们将WebSight开源。
