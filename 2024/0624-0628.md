# LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs
[LongRAG: 使用长上下文大语言模型增强检索增强生成](https://arxiv.org/abs/2406.15319)

在传统的检索增强生成 (RAG) 框架中，基本的检索单元通常都很短。常见的检索器如 DPR 通常处理100字的 Wikipedia 段落。这种设计使得检索器必须在庞大的语料库中找到“针”一样的小单元。相比之下，阅读器只需从这些短小的检索单元中提取答案。这种不平衡的“重”检索器和“轻”阅读器设计可能导致次优的性能。为了缓解这种不平衡，我们提出了一种新的框架 LongRAG，它包含一个“长检索器”和一个“长阅读器”。LongRAG 将整个 Wikipedia 处理成 4K-token 的单元，比之前增加了30倍。通过增加单元的大小，我们将总单元数从 2200 万显著减少到 70 万。这显著降低了检索器的负担，从而带来了显著的检索性能提升：在 NQ 数据集上，答案召回率@1达到了71%（之前为52%），在 HotpotQA（全维基）上，答案召回率@2达到了72%（之前为47%）。然后，我们将前k个检索到的单元（大约 30K tokens）输入到现有的长上下文大语言模型 (LLM) 中进行零样本回答抽取。在不需要任何训练的情况下，LongRAG 在 NQ 数据集上的 EM 达到了62.7%，这是已知的最佳结果。LongRAG 在 HotpotQA（全维基）上也达到了64.3%，与目前的最优模型相当。我们的研究为将 RAG 与长上下文大语言模型结合的未来发展方向提供了宝贵的见解。

# Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges
[评判评判者：评估作为评判者的大语言模型的一致性和脆弱性](https://arxiv.org/abs/2406.12624)

为了解决与人类评估相关的可扩展性挑战，作为评判者的大语言模型 (LLM-as-a-judge) 范式正在迅速崭露头角，成为评估大语言模型 (LLMs) 的方法。然而，这一范式的优缺点及其可能存在的偏见仍有许多未解之谜。在本文中，我们对充当评判者的各种 LLMs 的性能进行了全面研究。我们利用 TriviaQA 作为评估 LLMs 客观知识推理能力的基准，并与我们发现具有高评注者一致性的人类注释进行了比较。我们的研究包括 9 个评判模型和 9 个考试模型——既有基础模型，也有指令调优模型。我们评估了评判模型在不同模型大小、系列和评判提示上的一致性。在其他结果中，我们的研究重新发现了使用 Cohen’s kappa 作为一致性度量标准的重要性，而不是简单的百分比一致性，显示出高百分比一致性的评判者仍可能给出完全不同的分数。我们发现 Llama-3 70B 和 GPT-4 Turbo 与人类评判的高度一致，但在考试模型的排名中，它们被 JudgeLM-7B 和词汇评判 Contains 超过，这两者的人类一致性低至34点。通过错误分析和其他各种研究，包括指令长度和宽容偏见的影响，我们希望为未来使用 LLMs 作为评判者提供有价值的经验教训。

# DreamBench++: A Human-Aligned Benchmark for Personalized Image Generation
[DreamBench++: 面向个性化图像生成的人类对齐基准](https://arxiv.org/abs/2406.16855)

个性化图像生成在通过创意生成个性化内容以帮助日常工作和生活方面具有巨大潜力。然而，目前的评估要么是自动化的，但与人类不一致，要么需要耗时且昂贵的人类评估。在这项工作中，我们提出了 DreamBench++，一个由先进的多模态 GPT 模型自动化实现的人类对齐基准。具体来说，我们系统地设计了提示，使 GPT 既能与人类对齐又能自我对齐，并通过任务增强赋能。此外，我们构建了一个包含多样化图像和提示的综合数据集。通过对7种现代生成模型进行基准评估，我们证明 DreamBench++ 提供了显著更符合人类评估标准的结果，帮助社区取得创新性的发现。

# Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs
[Cambrian-1: 全面开放、以视觉为中心的多模态大语言模型探索](https://arxiv.org/abs/2406.16860)

我们介绍了 Cambrian-1，这是一个以视觉为中心设计的多模态大语言模型 (MLLM) 家族。尽管更强大的语言模型可以增强多模态能力，但视觉组件的设计选择往往研究不足，并且与视觉表征学习的研究脱节。这一差距阻碍了在实际场景中的精确感知落地。我们的研究通过使用 LLMs 和视觉指令调优作为界面，评估了多种视觉表征，提供了关于不同模型和架构（包括自监督、强监督及其结合）的新见解。我们对现有的 MLLM 基准进行了严格审查，解决了整合和解释不同任务结果的困难，并引入了一个新的视觉为中心的基准 CV-Bench。为了进一步改善视觉感知，我们提出了空间视觉聚合器 (SVA)，这是一个动态且具有空间感知的连接器，能够在减少 Token 数量的同时，将高分辨率的视觉特征与 LLMs 集成。此外，我们讨论了从公开资源中策划高质量视觉指令调优数据的重要性，强调了数据源平衡和分布比例的重要性。总体而言，Cambrian-1 不仅实现了最先进的性能，还作为一个全面开放的指令调优 MLLMs 指南。我们提供了模型权重、代码、支持工具、数据集以及详细的指令调优和评估方案。我们希望我们的发布能激发和加速多模态系统和视觉表征学习的进步。

# BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions
[BigCodeBench: 通过多样的函数调用和复杂指令进行代码生成的基准评估](https://arxiv.org/abs/2406.15877)

自动化软件工程在编程领域的大语言模型 (LLMs) 的最新进展中得到了极大的推动。虽然当前的基准评估显示 LLMs 能像人类开发者一样完成各种软件工程任务，但大多数评估仍然局限于短小且独立的算法任务。要解决具有挑战性和实际意义的编程任务，需要能够利用多种函数调用工具来高效实现数据分析和网页开发等功能。此外，使用多种工具解决任务需要通过准确理解复杂指令来进行综合推理。这两个特性对于 LLMs 是极大的挑战。为评估 LLMs 解决复杂和实际编程任务的能力，我们引入了 Bench，这是一个基准评估框架，挑战 LLMs 从139个库和7个领域中调用多个函数来完成1,140个细粒度编程任务。为了严格评估 LLMs，每个编程任务包含平均5.6个测试用例，代码覆盖率为99%。此外，我们提出了 Bench 的自然语言导向的变体 Benchi，它自动将原始文档字符串转换为仅包含关键信息的简短指令。我们对60个 LLMs 进行了广泛评估，结果显示 LLMs 目前还不能准确遵循复杂指令使用函数调用，得分最高为60%，显著低于人类的97%。这些结果强调了该领域需要进一步的进展。

# Evaluating D-MERIT of Partial-annotation on Information Retrieval
[部分标注在信息检索中的 D-MERIT 评估](https://arxiv.org/abs/2406.16048)

检索模型通常在部分标注的数据集上进行评估。每个查询只映射到一些相关文本，剩余的语料库被假定为不相关。因此，成功检索到假阴性的模型在评估中会受到惩罚。然而，为每个查询完全标注所有文本并不具备资源效率。在这项工作中，我们展示了在评估中使用部分标注的数据集可能会导致评估结果失真。我们策划了 D-MERIT，一个来自 Wikipedia 的段落检索评估集，旨在包含每个查询的所有相关段落。查询描述一个组（例如，“关于语言学的期刊”），相关段落提供实体属于该组的证据（例如，一个段落表明《Language》是一本关于语言学的期刊）。我们展示了在仅包含部分相关段落注释的数据集上进行评估可能会导致误导性的检索系统排名，并且随着更多相关文本被纳入评估集，排名结果趋于一致。我们提出我们的数据集作为评估资源，并建议在标注文本检索评估集时，在资源效率和可靠评估之间取得平衡。

# Long Context Transfer from Language to Vision
[从语言到视觉的长上下文转移](https://arxiv.org/abs/2406.16852)

视频序列提供了宝贵的时间信息，但现有的大型多模态模型 (LMMs) 在理解超长视频方面表现不佳。许多研究通过使用视觉重采样器 (visual resamplers) 来减少视觉 token 的数量以解决这一问题。而在本文中，我们从语言模型的角度来应对这一挑战。通过简单地延长语言骨干网络的上下文长度 (context length)，我们使得 LMMs 能够在无需任何视频训练的情况下理解数量级更多的视觉 token。我们将这种现象称为长上下文转移，并仔细分析了其特性。为了有效衡量 LMMs 在视觉模式下对长上下文的泛化能力，我们开发了 V-NIAH (Visual Needle-In-A-Haystack)，这是一个纯粹的合成长视觉基准，其灵感来自于语言模型的 NIAH 测试。我们提出的长视频助手 (LongVA) 可以处理2000帧或超过200K视觉 token，而不需要额外的复杂操作。凭借其扩展的上下文长度，LongVA 在 7B 规模模型中，通过密集采样更多输入帧，在 Video-MME 上实现了最先进的性能。我们的工作已在 https://github.com/EvolvingLMMs-Lab/LongVA 开源。

# The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale
[FineWeb 数据集：大规模提取高质量文本数据](https://arxiv.org/abs/2406.17557)

大语言模型 (LLM) 的性能在很大程度上依赖于其预训练数据集的质量和规模。然而，最先进的开放式大语言模型如 Llama 3 和 Mixtral 的预训练数据集并未公开，人们对它们的创建过程知之甚少。在这项工作中，我们介绍了 FineWeb，这是一个从96个 Common Crawl 快照中提取的15万亿 token 数据集，其性能优于其他开放预训练数据集。为了深入理解如何策划高质量的预训练数据集，我们详细记录并分析了 FineWeb 中使用的所有设计选择，包括对去重和过滤策略的深入研究。此外，我们介绍了 FineWeb-Edu，这是一个从 FineWeb 中筛选出的包含1.3万亿 token 的教育文本集合。基于 FineWeb-Edu 预训练的 LLMs 在 MMLU 和 ARC 等知识和推理密集型基准测试中表现显著更好。我们公开了数据集、数据策划代码库以及所有在消融实验 (ablation experiments) 中训练的模型。

# YouDream: Generating Anatomically Controllable Consistent Text-to-3D Animals
[YouDream: 生成解剖学可控的一致性文本到3D动物](https://arxiv.org/abs/2406.16273)

通过文本到图像扩散模型指导的3D生成技术，使得创建视觉上引人注目的资产成为可能。然而，之前的方法主要基于图像或文本进行生成，创造力的边界受限于文字的描述或可获得的图像。我们提出了 YouDream，一种能够生成高质量、解剖结构可控的动物的方法。YouDream 使用由3D姿势先验 (pose prior) 的2D视图控制的文本到图像扩散模型进行指导。我们的方法能够生成以前的文本到3D生成方法无法实现的3D动物。此外，我们的方法能够保持生成动物的解剖一致性，这在之前的文本到3D方法中常常是难以实现的领域。我们还设计了一条完全自动化的生成常见动物的流程。为避免生成3D姿势时需要人工干预，我们提出了一种多智能体大语言模型 (LLM)，它可以从有限的动物3D姿势库中调整姿势以表示所需的动物。对 YouDream 生成结果进行的用户研究表明，与其他方法相比，用户更偏爱我们的方法生成的动物模型。旋转展示结果 (turntable results) 和代码已在 https://youdream3d.github.io/ 发布。

# Adam-mini: Use Fewer Learning Rates To Gain More
[Adam-mini: 用更少的学习率获得更大的收益](https://arxiv.org/abs/2406.16793)

我们提出了 Adam-mini，这是一种优化器，其性能与 AdamW 相当或更优，但内存使用量减少了45%到50%。Adam-mini 通过减少 Adam 中的学习率资源（即1/v）来降低内存使用量。我们发现，如果我们 (1) 按照我们提出的 Hessian 结构原则仔细地将参数划分为块；(2) 为每个参数块分配一个合适的单一学习率，那么这些学习率中有90%以上可以无害地删除。我们进一步发现，对于每个参数块，存在一个高质量的单一学习率，如果有足够的资源来找到它，可以超越 Adam 的表现。我们提供了一种高性价比的方法来找到合适的学习率，并提出了 Adam-mini。通过实验证明，在各种从125M到7B的语言模型上进行预训练、监督微调和 RLHF 时，Adam-mini 的表现与 AdamW 相当或更好。Adam-mini 减少的内存使用量也缓解了 GPU 和 CPU 之间的通信开销，从而提高了吞吐量。例如，当在2个 A800-80GB GPU 上预训练 Llama2-7B 时，Adam-mini 实现了比 AdamW 高49.6%的吞吐量，节省了33%的预训练时间。

# Octo-planner: On-device Language Model for Planner-Action Agents
[Octo-planner: 用于规划-行动智能体的设备端语言模型](https://arxiv.org/abs/2406.18082)

AI 智能体在各个领域变得越来越重要，能够自主决策和解决问题。为了有效运作，这些智能体需要一个规划过程来确定最佳行动方案，并执行计划中的行动。在本文中，我们提出了一种高效的设备端 Planner-Action 框架，将规划和行动执行分为两个独立的部分：一个基于 Phi-3 Mini 的规划智能体，这是一个为边缘设备 (edge devices) 优化的38亿参数大语言模型 (LLM)，以及一个使用 Octopus 模型进行功能执行的行动智能体。规划智能体首先通过将任务分解为一系列子步骤来响应用户查询，这些子步骤由行动智能体执行。为了优化资源受限设备上的性能，我们采用模型微调 (fine-tuning) 而不是上下文学习，从而降低计算成本和能耗，同时提高响应时间。我们的方法使用 GPT-4 生成基于可用功能的多样化规划查询和响应，随后进行验证以确保数据质量。我们在这个精心策划的数据集上微调了 Phi-3 Mini 模型，在我们的域内测试环境中达到了97%的成功率。为了解决多领域规划挑战，我们开发了一种多 LoRA 训练方法，将分别在不同功能子集上训练的 LoRA 权重合并在一起。此方法能够在资源受限设备上灵活处理复杂的多领域查询，同时保持计算效率。为了支持进一步研究，我们在 https://huggingface.co/NexaAIDev/octopus-planning 开源了我们的模型权重。演示请参考 https://www.nexa4ai.com/octo-planner。

# OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding

[OMG-LLaVA: 融合图像级、物体级和像素级推理与理解](https://arxiv.org/abs/2406.19389)

当前的通用分割方法在像素级图像和视频理解方面表现出强大的能力。然而，它们缺乏推理能力，无法通过文本指令进行控制。相比之下，大型视觉语言多模态模型表现出强大的基于视觉的对话和推理能力，但缺乏像素级理解，并且难以接受视觉提示进行灵活的用户交互。本文提出了 OMG-LLaVA，这是一个结合了强大的像素级视觉理解和推理能力的新框架。它能够接受各种视觉和文本提示，以实现灵活的用户交互。具体来说，我们使用通用分割方法作为视觉编码器，将图像信息、感知先验和视觉提示整合到提供给 LLM 的视觉 token 中。LLM 负责理解用户的文本指令，并基于视觉信息提供文本响应和像素级分割结果。我们提出了感知先验嵌入 (perception prior embedding)，以更好地将感知先验与图像特征整合在一起。OMG-LLaVA 在单一模型中实现了图像级、物体级和像素级的推理和理解，在多个基准测试中达到了或超过了专用方法的性能。我们的目标是在一个编码器、一个解码器和一个 LLM 上进行端到端训练，而不是使用 LLM 连接每个专家。代码和模型已经发布以供进一步研究。