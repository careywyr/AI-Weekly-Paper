## xGen-MM (BLIP-3): A Family of Open Large Multimodal Models
[xGen-MM (BLIP-3): 一个开放的大型多模态模型家族](https://arxiv.org/abs/2408.08872)
本报告介绍了 xGen-MM（又称 BLIP-3），这是一个用于开发大型多模态模型（LMMs）的框架。该框架整合了精心策划的数据集、训练方案、模型架构以及一系列由此产生的 LMMs。xGen-MM，简称 xGen-MultiModal，是 Salesforce xGen 在基础 AI 模型领域倡议的扩展。我们的模型在包括单图像和多图像基准在内的多种任务上进行了严格的评估。我们的预训练基础模型展现了强大的情境学习能力，并且指令调整模型在相同模型大小的开源 LMMs 中展现了竞争性的性能。此外，我们引入了一个采用 DPO 进行安全调整的模型，旨在减少如幻觉等有害行为并增强安全性。我们开放了我们的模型、精心策划的大规模数据集以及我们的微调代码库，以推动 LMM 研究的进一步发展。相关资源将在我们的项目页面上提供。

## JPEG-LM: LLMs as Image Generators with Canonical Codec Representations
[JPEG-LM: 利用大语言模型生成图像与标准编解码器表示](https://arxiv.org/abs/2408.08459)
近期，图像和视频生成领域的工作开始采用自回归大语言模型架构，主要是因为其通用性以及可能的易于集成到多模态系统中的特点。将自回归训练从语言生成应用到视觉生成的核心在于离散化——即将连续的图像和视频数据转换为离散的Token。常见的离散化方法包括直接建模原始像素值，这种方法过于冗长；或者采用向量量化，这需要复杂的预训练过程。在本研究中，我们提出了一种新方法，即直接将图像和视频建模为通过标准编解码器（例如JPEG, AVC/H.264）压缩的文件。我们使用默认的Llama架构，无需任何视觉特定修改，从零开始预训练JPEG-LM来生成图像（并作为概念验证生成视频的AVC-LM），通过直接输出JPEG和AVC格式的压缩文件字节。评估结果显示，这种简单直接的方法在图像生成上比基于像素的建模和复杂的向量量化方法更为有效，我们的方法在这些基线上实现了31%的FID降低。分析表明，JPEG-LM在生成长尾视觉元素方面相比向量量化模型具有显著优势。总体而言，我们展示了使用标准编解码器表示可以有效降低语言生成与视觉生成之间的技术壁垒，为未来多模态语言/图像/视频大语言模型的研究铺平道路。

## Automated Design of Agentic Systems
[智能体系统的自动化设计](https://arxiv.org/abs/2408.08435)
研究人员正在投入大量精力开发强大的通用智能体，其中基础模型被用作智能体系统中的模块（例如，Chain-of-Thought (思维链), Self-Reflection (自我反思), Toolformer）。然而，机器学习的历史告诉我们，手工设计的解决方案最终会被学习型解决方案所取代。我们提出一个新的研究领域，智能体系统自动化设计 (ADAS)，旨在自动创建强大的智能体系统设计，包括发明新的构建模块和/或以新的方式组合它们。我们进一步证明，在ADAS中存在一种未被探索但有前景的方法，其中智能体可以用代码定义，并且可以通过一个元智能体在代码中编程出更好的新智能体来自动发现新智能体。鉴于编程语言是图灵完备的，这种方法理论上可以学习任何可能的智能体系统：包括新颖的提示、工具使用、控制流程及其组合。我们提出了一种简单而有效的算法，称为元智能体搜索，以展示这一想法，其中元智能体基于不断增长的先前发现档案迭代编程出有趣的新智能体。通过在多个领域（包括编码、科学和数学）进行广泛的实验，我们展示了我们的算法可以逐步发明具有新颖设计的智能体，这些智能体大大优于最先进的手工设计智能体。重要的是，我们始终观察到一个令人惊讶的结果，即由元智能体搜索发明的智能体在跨领域和模型转移时保持优越的性能，展示了它们的稳健性和普遍性。如果我们安全地开发它，我们的工作展示了自动设计越来越强大的智能体系统以造福人类的新研究方向的潜力。

## LongVILA: Scaling Long-Context Visual Language Models for Long Videos
[LongVILA: 扩展长上下文视觉语言模型以处理长视频](https://arxiv.org/abs/2408.10188)
长上下文处理能力对于多模态基础模型至关重要。我们推出了 LongVILA，这是一个针对长上下文视觉语言模型的全面解决方案，涵盖系统架构、模型训练以及数据集构建。在系统层面，我们首创了多模态序列并行 (MM-SP) 系统，该系统不仅支持长上下文的训练与推理，还能在 256 个 GPU 上实现 2M 上下文长度的训练。MM-SP 系统表现出卓越的效率，其速度比环形序列并行快 2.1 至 5.7 倍，在纯文本环境下比 Megatron-LM 快 1.1 至 1.4 倍。此外，MM-SP 系统还能与 Hugging Face 的 Transformers 库无缝对接。在模型训练环节，我们设计了一个包含五个阶段的流程，包括对齐、预训练、上下文扩展以及长短联合监督微调。针对数据集，我们精心打造了大规模视觉语言预训练数据集和长视频指令遵循数据集，以全面支撑我们的多阶段训练需求。LongVILA 的全栈解决方案不仅将 VILA 的可处理帧数提升了 128 倍（从 8 帧扩展至 1024 帧），还显著提升了长视频的字幕生成质量，评分从 2.00 跃升至 3.26（提升 1.6 倍），并在 1400 帧视频（上下文长度达 274k）中实现了 99.5% 的精准度。LongVILA-8B 模型在 VideoMME 基准测试中，随着视频帧数的增加，其在长视频任务上的性能也持续提升。

## MeshFormer: High-Quality Mesh Generation with 3D-Guided Reconstruction Model
[MeshFormer: 利用3D引导重建模型生成高质量网格](https://arxiv.org/abs/2408.10198)
近期，开放领域的3D重建模型受到了极大的关注。然而，由于缺乏足够的3D归纳偏置，现有方法往往需要高昂的训练成本，并且难以生成高质量的3D网格。在本研究中，我们提出了MeshFormer，这是一种稀疏视图重建模型，它明确地利用了3D本征结构、输入引导和训练监督。具体来说，我们摒弃了传统的三平面表示方法，转而在3D稀疏体素中存储特征，并结合Transformer与3D卷积，以充分利用显式的3D结构和投影偏置。除了稀疏视图的RGB输入外，我们还要求网络处理输入并生成相应的法线图。这些输入法线图可以通过2D扩散模型预测，从而显著增强几何学习的引导和细化。此外，通过将有符号距离函数（SDF）监督与表面渲染相结合，我们能够直接学习生成高质量的网格，无需经历复杂的多阶段训练过程。通过融入这些显式的3D偏置，MeshFormer能够高效地进行训练，并输出具有精细几何细节的高质量纹理网格。它还可以与2D扩散模型集成，以实现快速的单图像到3D和文本到3D转换任务。项目页面：https://meshformer3d.github.io

## TableBench: A Comprehensive and Complex Benchmark for Table Question Answering
[TableBench: 一个全面且复杂的表格问答基准](https://arxiv.org/abs/2408.09174)
大语言模型 (LLMs) 的最新进展显著提升了对表格数据的解释和处理能力，引入了前所未有的功能。尽管取得了这些成就，LLMs 在工业场景中的应用仍面临重大挑战，尤其是由于现实世界表格数据处理所需的推理复杂性增加，这突显了学术基准与实际应用之间的显著差距。为了解决这一差异，我们对表格数据在工业场景中的应用进行了详细调查，并提出了一个全面且复杂的基准 TableBench，涵盖了表格问答 (TableQA) 能力的四个主要类别中的18个领域。此外，我们引入了 TableLLM，该模型在我们精心构建的训练集 TableInstruct 上进行训练，实现了与 GPT-3.5 相当的性能。在 TableBench 上进行的大量实验表明，无论是开源还是专有的 LLMs，仍有显著的改进空间以满足现实世界的需求，其中最先进的模型 GPT-4 与人类相比仅获得了一个适度的分数。

## Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model
[Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](https://arxiv.org/abs/2408.11039)
我们介绍 Transfusion，这是一种在离散和连续数据上训练多模态模型的方法。Transfusion 结合了语言建模损失函数与扩散过程，用于在混合模态序列上训练单一的 Transformer。我们从零开始预训练了多个 70 亿参数的 Transfusion 模型，这些模型在文本和图像数据的混合上进行训练，并针对各种单模态和跨模态基准建立了缩放规律。我们的实验表明，Transfusion 在缩放方面显著优于将图像量化并在离散图像 Token 上训练语言模型的方法。通过引入模态特定的编码和解码层，我们可以进一步提高 Transfusion 模型的性能，甚至可以将每张图像压缩至仅 16 个 Patch。我们进一步证明，将 Transfusion 方法扩展到 70 亿参数和 2 万亿多模态 Token，可以产生一个能够与类似规模的扩散模型和语言模型相媲美的模型，从而同时获得两者的优势。

## To Code, or Not To Code? Exploring Impact of Code in Pre-training
[编码与否？探索预训练中代码的影响](https://arxiv.org/abs/2408.10914)
在大型语言模型 (LLMs) 的预训练过程中，即使模型并非专门用于处理代码，将代码纳入预训练数据已成为一种普遍做法。尽管业界普遍认为代码数据对通用大型语言模型 (LLMs) 的性能至关重要，但关于代码对非代码任务的具体影响分析却相对较少。本研究旨在系统地探讨代码数据对模型通用性能的影响，特别是它如何影响广泛下游任务，而不仅仅是代码生成。我们通过广泛的消融实验，评估了从470M到2.8B参数大小的模型在自然语言推理、世界知识、代码基准以及作为评判者的胜率等多个方面的表现。结果显示，代码是实现广泛泛化的关键因素，其质量的提升对所有任务都有显著影响。具体而言，相较于仅使用文本预训练的模型，加入代码后，自然语言推理能力提升了8.2%，世界知识提升了4.2%，生成任务胜率提高了6.6%，代码性能更是提升了12倍。这些发现表明，提升代码质量和在预训练中保留代码对模型性能有积极作用。

## TWLV-I: Analysis and Insights from Holistic Evaluation on Video Foundation Models
[TWLV-I: 视频基础模型的整体评估分析与洞察](https://arxiv.org/abs/2408.11318)
在本研究中，我们探讨了如何公正且稳健地评估视频基础模型。与语言或图像基础模型不同，视频基础模型的评估参数（如采样率、帧数、预训练步骤等）往往不统一，这使得进行公平且稳健的比较颇具挑战。为此，我们设计了一套精细的评估框架，旨在衡量视频理解的两大核心能力：外观与运动理解。研究结果显示，无论是采用文本监督的UMT或InternVideo2，还是自监督的V-JEPA，现有视频基础模型在至少一项能力上存在不足。为此，我们推出了TWLV-I，这是一种新型视频基础模型，能够为基于运动和外观的视频构建出强健的视觉表示。在仅使用公开数据集预训练的情况下，基于五个动作识别基准上的线性探测平均top-1准确率，我们的模型相较于V-JEPA（ViT-L）提升了4.6%p，相较于UMT（ViT-L）提升了7.7%p。即便与更大规模的模型相比，我们的模型也表现出色，相较于DFN（ViT-H）提升了7.2%p，相较于V-JEPA（ViT-H）提升了2.7%p，以及相较于InternVideo2（ViT-g）提升了2.8%p。我们提供了由TWLV-I从多个常用视频基准中提取的嵌入向量，以及可直接利用这些嵌入的评估源代码，代码已公开在"https://github.com/twelvelabs-io/video-embeddings-evaluation-framework"。

## LLM Pruning and Distillation in Practice: The Minitron Approach
[LLM Pruning and Distillation in Practice: The Minitron Approach](https://arxiv.org/abs/2408.11796)
我们详细介绍了如何通过剪枝和蒸馏技术，将Llama 3.1 8B和Mistral NeMo 12B模型分别压缩至4B和8B参数。我们采用了两种剪枝方法：一是深度剪枝，二是联合隐藏/注意力/MLP（宽度）剪枝，并在LM Evaluation Harness的基准测试中评估了其性能。随后，这些模型通过NeMo Aligner进行对齐，并在指令调优版本中进行了测试。结果显示，从Llama 3.1 8B中得到了一个性能优异的4B模型，而从Mistral NeMo 12B中则得到了一个顶尖的Mistral-NeMo-Minitron-8B（简称MN-Minitron-8B）模型。此外，我们发现，即使无法访问原始数据，对教师模型在蒸馏数据集上进行轻微微调也能带来益处。我们已在Hugging Face上以宽松许可开源了这些基础模型权重。

## Sapiens: Foundation for Human Vision Models
[Sapiens: 人类视觉模型的基础](https://arxiv.org/abs/2408.12569)
我们推出了 Sapiens，这是一系列针对四个核心人类视觉任务的模型——2D 姿态估计、身体部位分割、深度估计和表面法线预测。Sapiens 模型自然支持 1K 高分辨率推理，并且通过简单地微调在超过 3 亿张野外人类图像上预训练的模型，即可轻松适应不同任务。我们发现，在相同的计算资源下，对精选的人类图像数据集进行自监督预训练能显著提升多种人类视觉任务的性能。这些模型在野外数据上表现出极强的泛化能力，即使在标记数据稀少或完全合成的情况下也是如此。我们的模型设计简洁，具有良好的可扩展性——随着模型参数从 0.3 亿增加到 20 亿，其在各任务上的性能均得到提升。Sapiens 在各类人类视觉基准测试中持续领先，例如在 Humans-5K (姿态) 上提升了 7.6 mAP，在 Humans-2K (部位分割) 上提升了 17.1 mIoU，在 Hi4D (深度) 上相对 RMSE 提升了 22.4%，在 THuman2 (法线) 上相对角度误差提升了 53.5%。

## Controllable Text Generation for Large Language Models: A Survey
[大语言模型可控文本生成：综述](https://arxiv.org/abs/2408.12599)
在自然语言处理 (NLP) 领域，大语言模型 (LLMs) 已展现出高质量的文本生成能力。然而，在实际应用中，LLMs 面临着日益复杂的要求。除了避免误导性或不当内容外，LLMs 还需满足特定用户需求，例如模仿特定写作风格或生成富有诗意的文本。这些多样化的需求推动了可控文本生成 (CTG) 技术的发展，确保输出符合预定义的控制条件——如安全性、情感、主题一致性和语言风格——同时保持高水平的实用性、流畅性和多样性。

本文系统回顾了 LLMs 领域 CTG 的最新进展，全面阐述了其核心概念，并明确了控制条件和文本质量的要求。我们将 CTG 任务分为两大类：内容控制和属性控制。讨论了关键方法，包括模型重训练、微调、强化学习、提示工程、潜在空间操作和解码时干预。我们分析了每种方法的特点、优势和局限性，为实现生成控制提供了深入的见解。此外，我们还回顾了 CTG 评估方法，总结了其在各领域的应用，并指出了当前研究中的关键挑战，包括流畅性和实用性的降低。我们还提出了若干呼吁，例如未来研究应更加重视实际应用。本文旨在为该领域的研究人员和开发者提供有价值的指导。我们的参考列表和中文版本已在 [GitHub](https://github.com/IAAR-Shanghai/CTGSurvey) 上开源。

## Show-o: One Single Transformer to Unify Multimodal Understanding and Generation
[Show-o: 单一 Transformer 实现多模态理解和生成的一体化](https://arxiv.org/abs/2408.12528)
我们提出了一种名为 Show-o 的统一 Transformer，它整合了多模态理解和生成的能力。与纯粹的自回归模型不同，Show-o 融合了自回归与（离散）扩散建模技术，从而能够灵活处理多种及混合模态的输入和输出。该模型广泛支持视觉-语言领域的多种任务，如视觉问答、文本到图像生成、文本引导的图像修复/外推以及混合模态生成等。在多项基准测试中，Show-o 的表现与那些针对特定任务优化且参数规模相当或更大的模型相比，毫不逊色甚至更胜一筹。这充分展示了其作为未来基础模型的巨大潜力。相关代码和模型已发布于 https://github.com/showlab/Show-o。

## xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations
[xGen-VideoSyn-1: 使用压缩表示的高保真文本到视频合成](https://arxiv.org/abs/2408.12590)
我们提出了 xGen-VideoSyn-1，这是一个能够根据文本描述生成逼真场景的文本到视频 (T2V) 生成模型。借鉴了 OpenAI 的 Sora 等近期进展，我们探索了潜在扩散模型 (LDM) 架构，并引入了视频变分自编码器 (VidVAE)。VidVAE 在空间和时间上对视频数据进行压缩，显著减少了视觉 Token 的长度以及生成长序列视频所需的计算量。为了进一步降低计算成本，我们采用了一种分治合并策略，确保视频片段间的时间一致性。我们的扩散 Transformer (DiT) 模型整合了空间和时间自注意力层，能够在不同时间框架和宽高比下实现稳健的泛化。我们从项目初期就构建了一个数据处理流水线，并成功收集了超过 1300 万对高质量的视频-文本数据。该流水线涵盖了剪辑、文本检测、运动估计、美学评分以及基于我们内部视频大语言模型 (video-LLM) 的密集标注等多个步骤。训练 VidVAE 和 DiT 模型分别耗时约 40 和 642 个 H100 天。我们的模型能够以端到端的方式生成超过 14 秒的 720p 视频，并在性能上与当前最先进的 T2V 模型相媲美。

