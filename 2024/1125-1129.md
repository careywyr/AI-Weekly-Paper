## TÜLU 3: Pushing Frontiers in Open Language Model Post-Training
[TÜLU 3: 推动开放语言模型后训练的前沿](https://arxiv.org/abs/2411.15124)

语言模型后训练用于精炼行为并解锁近期语言模型的新技能，但应用这些技术的开放配方落后于专有配方。底层训练数据和后训练配方既是最重要的拼图部分，也是透明度最低的部分。为了弥合这一差距，我们引入了T\"ULU 3，这是一系列完全开放的、最先进的后训练模型，连同其数据、代码和训练配方，作为现代后训练技术的综合指南。T\"ULU 3基于Llama 3.1基础模型构建，其性能超越了Llama 3.1、Qwen 2.5、Mistral的指令版本，甚至超越了如GPT-4o-mini和Claude 3.5-Haiku等封闭模型。我们的模型训练算法包括监督微调（SFT）、直接偏好优化（DPO），以及我们称之为可验证奖励强化学习（RLVR）的新方法。通过T\"ULU 3，我们引入了一个多任务评估方案，用于后训练配方的开发和未见评估，标准基准实现，以及对所述基准上现有开放数据集的实质去污染。最后，我们对未能可靠提升性能的训练方法进行了分析和讨论。
  除了T\"ULU 3模型权重和演示，我们还发布了完整的配方——包括用于多样化核心技能的数据集、用于数据整理和评估的强大工具包、训练代码和基础设施，最重要的是，一份详细的报告，用于重现和进一步适应T\"ULU 3方法到更多领域。

## Material Anything: Generating Materials for Any 3D Object via Diffusion
[Material Anything: 生成任意3D对象材质的扩散框架](https://arxiv.org/abs/2411.15138)

我们提出了 Material Anything，这是一个全自动、统一的扩散框架，旨在为3D对象生成基于物理的材质。与现有方法相比，Material Anything 不依赖复杂的管道或特定案例的优化，而是提供了一个稳健的、端到端的解决方案，适用于各种光照条件下的对象。我们的方法利用了一个预训练的图像扩散模型，通过三头架构和渲染损失的增强，提高了稳定性和材质质量。此外，我们引入了置信度掩码作为扩散模型内的动态切换器，使其能够有效处理各种光照条件下有纹理和无纹理的对象。通过由置信度掩码引导的渐进式材质生成策略，以及一个UV空间材质精炼器，我们的方法确保了材质输出的一致性和UV就绪性。实验结果表明，我们的方法在多种物体类别和光照条件下均优于现有方法。

## OminiControl: Minimal and Universal Control for Diffusion Transformer
[OminiControl: 扩散Transformer的极简与通用控制](https://arxiv.org/abs/2411.15098)

本文介绍OminiControl，一个高度通用且参数高效的框架，将图像条件集成到预训练的扩散Transformer (DiT)模型中。OminiControl的核心机制是参数重用，使DiT能够利用自身作为强大的骨干网络编码图像条件，并通过其灵活的多模态注意力处理器进行处理。与依赖复杂架构的额外编码器模块的现有方法不同，OminiControl (1) 仅用约0.1%的额外参数高效整合了注入的图像条件，(2) 以统一方式处理多种图像条件任务，包括主体驱动的生成和空间对齐的条件（如边缘、深度等）。特别地，这些能力是通过在DiT自身生成的图像上训练实现的，这对主体驱动的生成尤为有利。广泛评估显示，OminiControl在主体驱动和空间对齐的条件生成方面优于现有的基于UNet和DiT适应的模型。此外，我们发布了包含超过200,000张身份一致图像的多样化数据集Subjects200K，以及一个高效的数据合成管道，以推动主体一致生成研究的发展。

## Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator
[大规模文本到图像模型结合修复功能：零样本主体驱动图像生成](https://arxiv.org/abs/2411.15466)

主体驱动的文本到图像生成旨在通过准确捕捉主体的视觉特征和文本提示的语义内容，在特定上下文中生成新主体的图像。传统方法需要耗费大量时间和资源进行微调以实现主体对齐，而最近的零样本方法则通过即时图像提示，往往在主体对齐上做出妥协。本文提出了一种名为双联画提示法的新型零样本方法，通过利用大规模文本到图像模型中双联画生成的特性，将其重新解释为具有精确主体对齐的修复任务。双联画提示法将参考图像置于左侧面板，并在右侧面板上进行文本条件下的修复。我们通过移除参考图像中的背景来避免不必要的内容干扰，并在修复过程中增强面板间的注意力权重以提升生成主体的细节质量。实验结果表明，我们的方法在零样本图像提示方法中表现优异，生成的图像更符合用户视觉偏好。此外，该方法不仅支持主体驱动的图像生成，还适用于风格化图像生成和主体驱动的图像编辑，展现了其在多种图像生成应用中的广泛适用性。项目页面：https://diptychprompting.github.io/

## Star Attention: Efficient LLM Inference over Long Sequences
[Star Attention: 高效的大语言模型在长序列上的推理](https://arxiv.org/abs/2411.17116)

基于 Transformer 的大语言模型 (LLMs) 在长序列上的推理既耗费资源又缓慢，这是由于自注意力机制的二次复杂性。我们引入了 Star Attention，这是一种两阶段的块稀疏近似方法，通过在多个主机上分片注意力来提高计算效率，同时最小化通信开销。在第一阶段，上下文使用跨主机的块级本地注意力并行处理。在第二阶段，查询和响应 Token 通过序列全局注意力机制参与所有先前缓存的 Token。Star Attention 与大多数使用全局注意力训练的基于 Transformer 的大语言模型无缝集成，将内存需求和推理时间减少了多达 11 倍，同时保持了 95-100% 的准确性。

## From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge
[LLM-as-a-judge：从生成到判断的机遇与挑战](https://arxiv.org/abs/2411.16594)

在人工智能 (AI) 和自然语言处理 (NLP) 领域，评估与评价一直是长期存在的难题。传统方法，无论是基于匹配还是嵌入，往往难以捕捉细微差异并提供理想结果。大语言模型 (LLM) 的最新进展催生了“LLM-as-a-judge”范式，即利用 LLM 在各类任务和应用中进行评分、排序或选择。本文对基于 LLM 的判断与评估进行了全面综述，旨在推动这一新兴领域的发展。首先，我们从输入和输出视角详细阐述了相关定义。接着，引入了一个多维分类法，从“判断什么”、“如何判断”及“在哪里判断”三个方面深入探讨 LLM-as-a-judge。最后，我们整理了评估 LLM-as-a-judge 的基准，并指出了关键挑战与未来方向，以期为该领域的研究提供有益启示。关于 LLM-as-a-judge 的更多论文与资源，请访问 https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge 和 https://llm-as-a-judge.github.io。

## GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI
[GMAI-VL & GMAI-VL-5.5M: 面向通用医疗AI的大型视觉-语言模型与综合多模态数据集](https://arxiv.org/abs/2411.14522)

尽管通用人工智能（如 GPT-4）取得了显著进展，但在医疗领域（通用医疗AI，GMAI）的应用仍因缺乏专业医疗知识而受限。为此，我们推出了 GMAI-VL-5.5M，这是一个通过将数百个专业医疗数据集转化为精心构建的图像-文本对而创建的综合多模态医疗数据集。该数据集不仅涵盖了全面的任务，还包含了多样化的模态和高品质的图像-文本数据。基于此数据集，我们进一步提出了 GMAI-VL，一个采用渐进式三阶段训练策略的通用医疗视觉-语言模型。通过整合视觉与文本信息，该模型显著提升了处理多模态数据的能力，从而在诊断和临床决策方面表现出色。实验结果显示，GMAI-VL 在视觉问答、医疗图像诊断等广泛的医疗多模态任务中，均达到了业界领先水平。我们的贡献包括 GMAI-VL-5.5M 数据集的开发、GMAI-VL 模型的引入，以及在多个医疗领域设立的新基准。代码和数据集将在 https://github.com/uni-medical/GMAI-VL 发布。

## O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?
[O1 复制之旅 -- 第二部分：简单蒸馏超越 O1-preview，是进步还是教训？](https://arxiv.org/abs/2411.16489)

本文对当前复制 OpenAI 的 O1 模型能力的方法进行了深入分析，特别关注了广泛使用但通常未公开的知识蒸馏技术。在前作探讨了复制 O1 的基本技术路径后，本研究揭示了通过从 O1 的 API 进行简单蒸馏，结合监督微调，可以在复杂的数学推理任务上实现优于 O1-preview 的性能。实验表明，仅使用数万条 O1 蒸馏的长思维链样本进行微调的基础模型，在美国的邀请数学考试（AIME）上超越了 O1-preview，且技术复杂度极低。此外，研究还探索了 O1 蒸馏模型在多样化任务中的泛化能力，包括幻觉、安全性和开放领域问答。尽管仅在数学问题解决数据上训练，模型在开放式问答任务上表现出了强大的泛化能力，并在微调后显著减少了谄媚行为。我们公开这一发现，旨在促进 AI 研究的透明度，并挑战当前领域中技术声明不透明的趋势。研究内容包括：（1）蒸馏过程及其有效性的详细技术阐述，（2）评估和分类 O1 复制尝试的全面基准框架，（3）对过度依赖蒸馏方法的局限性和潜在风险的批判性讨论。最终，我们得出一个关键教训：尽管追求更强大的 AI 系统很重要，但培养基于第一性原理思维的研究人员的发展至关重要。

## ROICtrl: Boosting Instance Control for Visual Generation
[ROICtrl: 提升视觉生成的实例控制](https://arxiv.org/abs/2411.17949)

自然语言在将位置和属性信息准确关联到多个实例上存在困难，这使得当前基于文本的视觉生成模型只能处理包含少数主导实例的简单组合。为解决这一问题，本研究通过引入区域实例控制来增强扩散模型，每个实例由一个边界框和自由形式的描述共同控制。以往的方法通常依赖隐式位置编码或显式注意力掩码来分离感兴趣区域 (ROI)，导致坐标注入不准确或计算开销巨大。受物体检测中 ROI-Align 的启发，我们提出了一种称为 ROI-Unpool 的互补操作。ROI-Align 和 ROI-Unpool 共同实现了对高分辨率特征图的显式、高效且准确的 ROI 操作，从而应用于视觉生成。基于 ROI-Unpool，我们提出了 ROICtrl，这是一种适用于预训练扩散模型的适配器，能够实现精确的区域实例控制。ROICtrl 不仅兼容社区微调的扩散模型，还能与现有的基于空间的外接组件（如 ControlNet、T2I-Adapter）和基于嵌入的外接组件（如 IP-Adapter、ED-LoRA）协同工作，扩展了它们在多实例生成中的应用。实验结果显示，ROICtrl 在区域实例控制方面表现优异，同时大幅降低了计算成本。

## ShowUI: One Vision-Language-Action Model for GUI Visual Agent
[ShowUI: 一个用于GUI视觉智能体的视觉-语言-动作模型](https://arxiv.org/abs/2411.17465)

开发图形用户界面 (GUI) 助手具有巨大的潜力，能够显著提升人类工作流程的效率。尽管当前大多数智能体基于语言，依赖于包含丰富文本元信息的闭源API（如HTML或可访问性树），但它们在感知UI视觉效果方面存在不足，无法达到人类的感知水平，这凸显了对GUI视觉智能体的需求。在本研究中，我们提出了一种数字世界中的视觉-语言-动作模型，名为ShowUI，其创新点包括：(i) UI引导的视觉Token选择，通过将截图构建为UI连接图，自适应识别冗余关系，并在自注意力块中作为Token选择的依据，从而降低计算成本；(ii) 交错的视觉-语言-动作流，灵活整合GUI任务中的多样化需求，有效管理导航中的视觉-动作历史或为每个截图配对多轮查询-动作序列，提升训练效率；(iii) 小规模高质量的GUI指令跟随数据集，通过精心数据整理和重采样策略，解决数据类型不平衡问题。基于这些创新，ShowUI，一个使用256K数据的轻量级2B模型，在零样本截图定位中达到了75.1%的准确率。其UI引导的Token选择进一步减少了训练过程中33%的冗余视觉Token，并使性能提升了1.4倍。在Web Mind2Web、移动AITW和在线MiniWob环境中的导航实验进一步验证了我们的模型在推进GUI视觉智能体方面的有效性和潜力。模型可在https://github.com/showlab/ShowUI获取。

## Pathways on the Image Manifold: Image Editing via Video Generation
[图像流形路径：基于视频生成的图像编辑](https://arxiv.org/abs/2411.16819)

近期，图像编辑领域在图像扩散模型的推动下取得了显著进展。然而，这些模型在处理复杂编辑指令时仍面临重大挑战，往往通过改变原始图像的关键元素来牺牲保真度。与此同时，视频生成领域也取得了显著进步，出现了能够有效模拟一致且连续世界的模型。本文提出，通过利用图像到视频模型进行图像编辑，将这两个领域结合起来。我们将图像编辑视为一个时间过程，利用预训练的视频模型创建从原始图像到所需编辑的平滑过渡。这种方法连续地遍历图像流形，确保编辑的一致性同时保留原始图像的关键方面。我们的方法在基于文本的图像编辑方面达到了最先进的结果，显著提升了编辑准确性和图像保留效果。

## CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models
[CAT4D: 使用多视角视频扩散模型在4D中生成任何内容](https://arxiv.org/abs/2411.18613)

我们提出了CAT4D，一种从单目视频生成4D（动态3D）场景的方法。CAT4D利用在多样化数据集组合上训练的多视角视频扩散模型，能够在任意指定的相机姿态和时间戳上实现新视角合成。结合一种新颖的采样方法，该模型可以将单个单目视频转换为多视角视频，通过优化可变形3D高斯表示实现稳定的4D重建。我们在新视角合成和动态场景重建基准上展示了竞争性能，并突出了从真实或生成视频中生成4D场景的创造性能力。访问我们的项目页面以获取结果和交互式演示：cat-4d.github.io。

