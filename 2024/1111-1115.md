## LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation
[LLM2CLIP: 强大的语言模型解锁更丰富的视觉表示](https://arxiv.org/abs/2411.04997)

CLIP 是当今最重要的多模态基础模型之一。自然语言提供的丰富监督信号，作为人类知识的载体，塑造了一个强大的跨模态表示空间，这是 CLIP 能力的核心。然而，随着 GPT-4 和 LLaMA 等大语言模型 LLM 的快速发展，语言理解和生成的边界不断被推动。这自然引出了一个问题：LLM 的能力能否被利用来进一步改进多模态表示学习？将 LLM 融入 CLIP 的潜在好处显而易见。LLM 强大的文本理解能力可以根本性地提升 CLIP 处理图像描述的能力，显著增强其处理长而复杂文本的能力，这是原始 CLIP 的一个明显短板。此外，LLM 是在大量文本语料库上训练的，拥有开放世界的知识。这使得它们在训练过程中能够扩展描述信息，从而提高学习过程的效率。在本文中，我们提出了 LLM2CLIP，一种利用 LLM 力量解锁 CLIP 潜力的新方法。通过在描述空间中使用对比学习对 LLM 进行微调，我们将其文本能力融入输出嵌入，显著提高了输出层的文本区分能力。然后，我们设计了一个高效的训练过程，其中微调后的 LLM 作为 CLIP 视觉编码器的强大教师。得益于 LLM 的存在，我们现在可以在不受原始 CLIP 文本编码器上下文窗口和能力限制的情况下，加入更长和更复杂的描述。我们的实验表明，这种方法在跨模态任务中带来了显著的改进。

## Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models
[Add-it: 使用预训练扩散模型实现图像中对象的无训练插入](https://arxiv.org/abs/2411.07232)

基于文本指令将对象添加到图像中是语义图像编辑中的一个具有挑战性的任务，需要在保留原始场景和在新对象的合适位置无缝集成之间取得平衡。尽管付出了大量努力，现有模型在这方面往往难以达到平衡，特别是在复杂场景中找到自然的位置来添加对象。我们引入了 Add-it，一种无需额外训练的方法，该方法改进了扩散模型的注意力机制，以整合来自三个关键来源的信息：场景图像、文本提示和生成的图像本身。我们的加权改进注意力机制在确保自然对象放置的同时，保持了结构一致性和细节精细度。无需特定任务的微调，Add-it 在真实和生成图像插入基准测试中均达到了最先进的结果，包括我们新构建的用于评估对象放置合理性的“Adding Affordance Benchmark”，优于监督方法。人类评估显示，Add-it 在超过 80% 的情况下被优先考虑，并且在各种自动化指标上也展示了改进。

## OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision
[OmniEdit: 通过专家监督构建图像编辑全能模型](https://arxiv.org/abs/2411.07199)

指令引导的图像编辑方法通过在自动合成的或手动标注的图像编辑对上训练扩散模型，展现了巨大的潜力。然而，这些方法在实际应用中仍显不足。我们识别出三个主要问题导致了这一差距。首先，现有模型因合成过程中的偏差而编辑能力有限。其次，这些方法使用的数据集含有大量噪声和伪影，主要是因为采用了像 CLIP-score 这样的简单过滤方法。第三，所有数据集都被限制在单一的低分辨率和固定的长宽比，这限制了其处理实际应用的多功能性。本文中，我们提出了 \omniedit，这是一个能够无缝处理七种不同图像编辑任务的全能编辑器。我们的贡献包括：(1) \omniedit 通过利用七个不同专家模型的监督来确保任务覆盖。(2) 我们采用基于大模态模型（如 GPT-4o）提供的分数进行重要性采样，而非 CLIP-score，以提升数据质量。(3) 我们提出了一种新的编辑架构，称为 EditNet，以大幅提高编辑成功率。(4) 我们提供不同长宽比的图像，确保模型能处理任何现实中的图像。我们设计了一个测试集，包含不同长宽比的图像，并附有涵盖不同任务的多样化指令。自动评估和人类评估均表明，\omniedit 显著优于所有现有模型。我们的代码、数据集和模型将在 https://tiger-ai-lab.github.io/OmniEdit/ 上提供。

## M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework
[M-Longdoc: 多模态超长文档理解和检索感知调优框架的基准](https://arxiv.org/abs/2411.06176)

理解和回答文档中的问题在众多商业和实际应用中具有重要价值。然而，文档往往包含冗长且多样化的多模态内容，如文本、图表和表格，这些内容对人类而言既耗时又难以全面阅读。因此，开发有效且自动化的方法以辅助人类完成此任务显得尤为迫切。

在本研究中，我们推出了 M-LongDoc，一个包含 851 个样本的基准，并构建了一个自动化框架以评估大型多模态模型的性能。此外，我们提出了一种检索感知调优方法，旨在实现高效且有效的多模态文档阅读。相较于现有研究，我们的基准涵盖了更近期且篇幅更长的文档，达数百页之多，并要求开放式解决方案而非仅限于提取答案。据我们所知，我们的训练框架是首个直接针对多模态长文档检索场景的框架。

为实现开源模型的调优，我们采用全自动方式构建了一个用于此类文档问答任务的训练语料库。实验结果显示，与基线开源模型相比，我们的调优方法使模型响应的正确性提升了 4.6%。我们的数据、代码和模型均可在 https://multimodal-documents.github.io 获取。

## Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models
[Chinese SimpleQA: 大语言模型中文事实性评估](https://arxiv.org/abs/2411.07140)

新的大语言模型评估基准对于跟上大语言模型的快速发展至关重要。在这项工作中，我们提出了 Chinese SimpleQA，这是首个全面的中文基准，用于评估语言模型回答简短问题的事实性能力。Chinese SimpleQA 主要具有五个特性：中文、多样化、高质量、静态、易于评估。具体来说，首先，我们专注于中文语言，涵盖了 6 个主要主题和 99 个多样化子主题。其次，我们进行了一个全面的质量控制过程，以确保问题和答案的高质量，其中参考答案是静态的，不会随时间变化。第三，遵循 SimpleQA，问题和答案都非常简短，评分过程基于 OpenAI API 易于评估。基于 Chinese SimpleQA，我们对现有大语言模型的事实性能力进行了全面评估。最后，我们期望 Chinese SimpleQA 能够指导开发者更好地理解其模型在中文事实性能力方面的表现，并促进基础模型的发展。

## Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models
[Edify Image: 使用像素空间拉普拉斯扩散模型生成高质量图像](https://arxiv.org/abs/2411.07126)

我们推出 Edify Image，这是一组能够生成具有像素级精度的逼真图像内容的扩散模型。Edify Image 采用级联像素空间扩散模型，通过新颖的拉普拉斯扩散过程进行训练，该过程中不同频率带的图像信号以不同速率衰减。Edify Image 广泛应用于文本到图像合成、4K 上采样、ControlNets、360 HDR 全景生成以及图像定制的微调。

## Large Language Models Can Self-Improve in Long-context Reasoning
[大语言模型在长上下文推理中可以自我改进](https://arxiv.org/abs/2411.08147)

大语言模型 (LLMs) 在处理长上下文方面取得了显著进展，但在长上下文推理方面仍面临挑战。现有的方法通常涉及使用合成数据对 LLMs 进行微调，这依赖于人类专家或像 GPT-4 这样的高级模型的标注，这限制了进一步的发展。为了解决这个问题，我们研究了 LLMs 在长上下文推理中自我改进的潜力，并提出了一种名为 \ours 的方法，专门为此目的设计。

该方法的步骤如下：我们对每个问题采样多个输出，使用最小贝叶斯风险对其进行评分，然后根据这些输出进行监督微调或偏好优化。在多个领先 LLMs 上进行的广泛实验证明了 \ours 的有效性，Llama-3.1-8B-Instruct 的绝对改进达到了 4.2 分。此外，\ours 的表现优于依赖人类专家或高级模型生成的数据的前期方法。我们相信这项工作将为长上下文场景中的自我改进技术开辟新的途径，这对于 LLMs 的不断进步至关重要。

## LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models
[LLaMA-Mesh: 将3D网格生成与语言模型统一](https://arxiv.org/abs/2411.09595)

这项工作探索了扩展在文本数据上预训练的大语言模型 (LLMs) 的能力，使其能够在一个统一的模型中生成3D网格。这带来了两个主要优势：(1) 利用LLMs中已嵌入的空间知识，这些知识来源于3D教程等文本源；(2) 实现对话式的3D生成和网格理解。一个主要的挑战是如何有效地将3D网格数据标记化为LLMs可以无障碍处理的离散Token。为了解决这个问题，我们引入了LLaMA-Mesh，这是一种新颖的方法，它将3D网格的顶点坐标和面定义表示为纯文本，从而可以直接与LLMs集成，而无需扩展词汇表。我们构建了一个监督微调 (SFT) 数据集，使预训练的LLMs能够 (1) 根据文本提示生成3D网格，(2) 根据需要生成文本与3D网格的交错输出，以及 (3) 理解和解释3D网格。我们的工作首次证明了LLMs可以通过文本格式的微调来获取复杂的3D网格生成的空间知识，有效地统一了3D和文本模态。LLaMA-Mesh在保持强大的文本生成性能的同时，实现了与从头开始训练的模型相当的网格生成质量。

## MagicQuill: An Intelligent Interactive Image Editing System
[MagicQuill: 一个智能交互式图像编辑系统](https://arxiv.org/abs/2411.09703)

图像编辑任务复杂多样，需要高效且精确的操作技术。本文介绍的 MagicQuill 是一个集成图像编辑系统，能够快速实现创意构思。该系统界面简洁而功能强大，用户只需少量输入即可执行编辑操作（如插入元素、擦除对象、调整颜色）。这些操作由多模态大语言模型 (MLLM) 实时监控，自动预测编辑意图，无需手动输入提示。此外，系统采用了一个经过精心训练的双分支插件模块增强的强大扩散先验，以实现对编辑请求的精确控制。实验结果显示，MagicQuill 能够有效完成高质量的图像编辑。欢迎访问 https://magic-quill.github.io 体验我们的系统。

