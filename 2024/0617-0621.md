# XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning
[XLand-100B: 一个大规模多任务数据集用于上下文强化学习](https://arxiv.org/abs/2406.08973)

在大规模语言模型和计算机视觉模型中，上下文学习范式取得了巨大成功，最近基于上下文的强化学习领域也迅速崛起。然而，由于缺乏具有挑战性的基准，这一领域的发展受到了限制，因为所有实验都在简单环境和小规模数据集上进行。我们提出了XLand-100B，一个基于XLand-MiniGrid环境的大规模数据集，作为解决这一问题的初步尝试。该数据集包含近30,000个不同任务的完整学习历史，涵盖了1000亿次转换和25亿个回合。收集这个数据集耗费了50,000 GPU小时，这超出了大多数学术实验室的能力范围。我们还提供了工具以再现或进一步扩展这个数据集。通过这一重大努力，我们希望促进基于上下文的强化学习研究的普及，并为进一步扩展提供坚实的基础。代码是开源的，可以在Apache 2.0许可证下使用，网址为https://github.com/dunno-lab/xland-minigrid-datasets。

# Make It Count: Text-to-Image Generation with an Accurate Number of Objects
[精确对象数量的文本到图像生成](https://arxiv.org/abs/2406.10210)

尽管文本到图像扩散模型取得了空前的成功，但通过文本控制图像中对象的数量却出乎意料地困难。这对于技术文档、儿童书籍以及烹饪食谱插图等各种应用都非常重要。生成正确数量的对象在根本上具有挑战性，因为生成模型需要对每个对象实例保持独立识别，即使多个对象看起来相同或重叠，并在生成过程中隐式地进行全局计算。目前尚不清楚这种表示是否存在。为了解决这一问题，我们首先识别出扩散模型中携带对象识别信息的特征。然后利用这些特征在去噪过程中分离和计数对象实例，并检测过度生成和不足生成的问题。我们通过训练一个模型来解决这些问题，该模型根据现有对象的布局预测缺失对象的形状和位置，并展示如何利用它引导去噪以生成正确数量的对象。我们的CountGen方法不依赖外部资源确定对象布局，而是利用扩散模型本身的先验，创建基于提示和种子的布局。在两个基准数据集上的评估显示，CountGen在数量准确性方面大大优于现有基线模型。

# ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation
[ChartMimic：通过图表到代码生成评估LMM的跨模态推理能力](https://arxiv.org/abs/2406.09961)

我们引入了一个新的基准ChartMimic，旨在评估大规模多模态模型（LMM）的跨模态推理能力。ChartMimic利用信息密集的视觉图表和文本指令作为输入，要求LMM生成相应的图表渲染代码。ChartMimic包括1,000个人工策划的（图表、指令、代码）三元组，代表了各种领域（如物理学、计算机科学、经济学等）科学论文中的真实图表使用案例。这些图表涵盖了18种常规类型和4种高级类型，细分为191个子类别。此外，我们提出了多级评估指标，以自动和全面地评估输出代码和渲染的图表。与现有的代码生成基准不同，ChartMimic强调评估LMM协调视觉理解、代码生成和跨模态推理等多种认知能力的能力。对3个专有模型和11个开放权重模型的评估突显了ChartMimic提出的重大挑战。即使是先进的GPT-4V和Claude-3-opus也仅分别达到73.2和53.7的平均分，这表明仍有很大的改进空间。我们预计ChartMimic将激发LMM的发展，推进通用人工智能的追求。

# Needle In A Multimodal Haystack
[Needle In A Multimodal Haystack](https://arxiv.org/abs/2406.07230)

随着多模态大语言模型 (MLLM) 的快速进展，评估方法也日益全面。然而，作为现实应用基础的长篇多模态文档理解能力，仍然未被充分探索。在这项工作中，我们提出了Needle In A Multimodal Haystack (MM-NIAH)，这是第一个专门设计用于系统评估现有MLLM理解长篇多模态文档能力的基准。我们的基准包括三类评估任务：多模态检索、计数和推理。在每项任务中，模型需要根据分布在给定多模态文档中的不同关键信息回答问题。我们对领先的MLLM在MM-NIAH上的评估表明，现有模型在这些任务上仍有显著的改进空间，尤其是在视觉为中心的评估方面。我们希望这项工作能够为长篇多模态文档理解的进一步研究提供一个平台，并推动MLLM的发展。代码和基准发布在https://github.com/OpenGVLab/MM-NIAH。

# BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack
[BABILong: 用长上下文推理测试大语言模型的极限](https://arxiv.org/abs/2406.10149)

近年来，大语言模型 (LLM) 的输入上下文大小显著增加。然而，现有评估方法未能全面评估模型处理长上下文的效率。为填补这一空白，我们引入了BABILong基准，旨在测试语言模型在极长文档中基于分布事实进行推理的能力。BABILong包括20个多样的推理任务，如事实链条、简单归纳、演绎、计数和处理列表/集合。这些任务本身具有挑战性，当所需事实分散在长篇自然文本中时更具挑战性。我们的评估显示，流行的LLM仅有效利用了10-20%的上下文，并且其性能在推理复杂性增加时急剧下降。作为上下文推理的替代方法，检索增强生成方法在单事实问答中独立于上下文长度实现了60%的准确率。在上下文扩展方法中，循环记忆Transformer展示了最高性能，能够处理长度达1100万tokens。BABILong基准可以扩展到任何长度，以支持新模型能力增加的评估，我们提供了长度达100万tokens的分割。

# MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs
[MMDU: 一个多轮多图像对话理解基准和LVLMs的指令微调数据集](https://arxiv.org/abs/2406.11833)

生成自然且有意义的响应以与多模态人类输入交流是大规模视觉语言模型 (LVLM) 的基本能力。虽然现有的开源LVLM在单轮单图像输入的简化场景中表现良好，但在长上下文历史中的多轮多图像实际对话场景中表现不佳。现有LVLM基准主要关注单选问题或短形式响应，无法充分评估LVLM在现实人机交互应用中的能力。因此，我们引入了MMDU，一个全面的基准，以及MMDU-45k，一个大规模指令微调数据集，旨在评估和提升LVLM在多轮多图像对话中的能力。我们使用聚类算法从开源维基百科中找到相关图像和文本描述，并由人类注释者在GPT-4o模型的帮助下构建问答对。MMDU最多包含18k图像+文本tokens、20幅图像和27轮对话，是以前基准长度的至少5倍，对现有LVLM提出了挑战。我们对15个代表性LVLM使用MMDU进行了深入分析，发现开源LVLM由于缺乏对话指令微调数据而落后于闭源模型。我们展示了对MMDU-45k进行微调显著弥合了这一差距，生成了更长且更准确的对话，并提高了MMDU和现有基准上的得分（MMStar：+1.1%，MathVista：+1.5%，ChartQA：+1.2%）。我们的贡献为弥合现有LVLM模型与现实应用需求之间的差距铺平了道路。该项目可在https://github.com/Liuziyu77/MMDU找到。

# mDPO: Conditional Preference Optimization for Multimodal Large Language Models
[mDPO: 用于多模态大语言模型的条件偏好优化](https://arxiv.org/abs/2406.11839)

直接偏好优化 (DPO) 已被证明是一种有效的大语言模型 (LLM) 调整方法。最近的一些研究尝试将 DPO 应用于多模态场景，但发现难以实现持续改进。通过比较实验，我们发现了多模态偏好优化中的无条件偏好优化问题，即模型忽视了图像条件。为了解决这个问题，我们提出了 mDPO，这是一种多模态 DPO 方法，通过同时优化图像偏好来防止仅语言偏好的过度优先。此外，我们引入了一种奖励锚点，确保选择的响应奖励为正，从而避免了相对偏好优化中固有的可能性下降问题。在对两种不同大小的多模态 LLM 和三个广泛使用的基准上的实验中，mDPO 有效解决了多模态偏好优化中的无条件偏好问题，并显著提高了模型性能，特别是在减少幻觉方面。

# THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation
[THEANINE: 通过时间线增强响应生成重新审视长期对话中的记忆管理](https://arxiv.org/abs/2406.10996)

大语言模型 (LLM) 能够在与用户的长时间交互过程中处理大量对话历史，而无需额外的记忆模块；然而，它们的响应往往忽视或错误回忆过去的信息。在本文中，我们重新审视了 LLM 时代的记忆增强响应生成。尽管先前的工作主要关注去除过时记忆，我们认为这些记忆可以提供上下文线索，帮助对话系统理解过去事件的发展，从而有助于生成响应。我们提出了 Theanine，一个通过记忆时间线增强 LLM 响应生成的框架，这些时间线展示了相关过去事件的发展和因果关系。我们还引入了 TeaFarm，这是一个解决长时间对话中 G-Eval 局限性的反事实驱动问答管道。我们的方法的补充视频和用于 TeaFarm 评估的 TeaBag 数据集可以在 https://theanine-693b0.web.app/ 找到。

# DataComp-LM: In search of the next generation of training sets for language models
[DataComp-LM: 寻找下一代语言模型训练集](https://arxiv.org/abs/2406.11794)

我们介绍了用于语言模型的 DataComp (DCLM)，这是一个用于控制数据集实验的测试平台，旨在改进语言模型。作为 DCLM 的一部分，我们提供了一个从 Common Crawl 提取的 240T tokens 的标准化语料库，基于 OpenLM 框架的有效预训练配方，以及广泛的 53 项下游评估。参与 DCLM 基准测试的研究人员可以在从 412M 到 7B 参数的模型规模上，尝试数据去重、过滤和数据混合等数据策划策略。作为 DCLM 的基线，我们进行了广泛的实验，发现基于模型的过滤是组建高质量训练集的关键。生成的数据集 DCLM-Baseline 使得可以从零开始训练一个 7B 参数的语言模型，在使用 2.6T 训练 tokens 的情况下在 MMLU 上达到 64% 的 5-shot 准确率。与以前最先进的开源数据语言模型 MAP-Neo 相比，DCLM-Baseline 在 MMLU 上提高了 6.6 个百分点，同时训练所需计算量减少了 40%。我们的基线模型在 MMLU 上与 Mistral-7B-v0.3 和 Llama 3 8B 相当 (分别为 63% 和 66%)，在 53 项自然语言理解任务的平均表现上也类似，但所需计算量仅为 Llama 3 8B 的 6.6 倍。我们的结果强调了数据集设计对于训练语言模型的重要性，并为进一步的数据策划研究提供了起点。

# DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence
[DeepSeek-Coder-V2: 打破闭源模型在代码智能上的壁垒](https://arxiv.org/abs/2406.11931)

我们介绍了DeepSeek-Coder-V2，这是一种开源的混合专家 (MoE) 代码语言模型，在特定代码任务中表现与GPT4-Turbo相当。具体来说，DeepSeek-Coder-V2在DeepSeek-V2的中间检查点基础上进一步预训练了6万亿tokens。通过这种持续预训练，DeepSeek-Coder-V2显著增强了DeepSeek-V2的编码和数学推理能力，同时在一般语言任务中保持了相当的性能。与DeepSeek-Coder-33B相比，DeepSeek-Coder-V2在各种代码相关任务以及推理和通用能力方面表现出显著进步。此外，DeepSeek-Coder-V2支持的编程语言从86种扩展到338种，并将上下文长度从16K扩展到128K。在标准基准评估中，DeepSeek-Coder-V2在编码和数学基准中表现优于封闭源模型，如GPT4-Turbo、Claude 3 Opus和Gemini 1.5 Pro。

# Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation
[Depth Anywhere: 通过透视蒸馏和未标注数据增强提升360度单目深度估计](https://arxiv.org/abs/2406.12849)

在360度图像中准确估计深度对于虚拟现实、自动导航和沉浸式媒体应用至关重要。现有为透视图像设计的深度估计方法在应用于360度图像时失效，因为不同的摄像机投影和失真，而360度方法由于缺乏标注数据对表现较差。我们提出了一种新的深度估计框架，能够有效利用未标注的360度数据。我们的方法使用最先进的透视深度估计模型作为教师模型，通过六面体投影技术生成伪标签，从而实现360度图像深度的高效标注。该方法利用了日益丰富的大型数据集。我们的方法包括两个主要阶段：无效区域的离线掩码生成和在线半监督联合训练机制。我们在Matterport3D和Stanford2D3D等基准数据集上测试了我们的方法，显示了深度估计准确性的显著改进，特别是在零样本场景中。我们提出的训练流程可以增强任何360单目深度估计器，并展示了在不同摄像机投影和数据类型之间的有效知识转移。有关结果，请参阅我们的项目页面：https://albert100121.github.io/Depth-Anywhere/

# Bootstrapping Language Models with DPO Implicit Rewards
[用DPO隐性奖励引导大语言模型](https://arxiv.org/abs/2406.09760)

大语言模型 (LLM) 的人类对齐是一个活跃的研究领域。最近的一项突破性工作，直接偏好优化 (DPO)，通过绕过人类反馈强化学习 (RLHF) 中的奖励学习阶段，大大简化了该过程。DPO在训练后提供了一个隐性奖励模型。在这项工作中，我们观察到这个隐性奖励模型本身可以用于进一步对齐LLM。我们的方法是使用当前LLM模型的奖励构建一个偏好数据集，然后在后续的DPO轮次中使用该数据集。我们结合了一些改进措施，消除了响应长度的偏见，并提高了偏好数据集的质量，以进一步改进我们的方法。我们的方法称为使用DPO隐性奖励的自我对齐 (DICE)，在对齐方面显示出显著改进，并在AlpacaEval 2上取得了优于Gemini Pro的表现，达到了27.55%的长度控制胜率，且仅使用8B参数且无外部反馈。我们的代码可在https://github.com/sail-sg/dice找到。

# Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models
[Multimodal Needle in a Haystack：评估多模态大语言模型的长上下文能力](https://arxiv.org/abs/2406.11230)

多模态大语言模型 (MLLM) 在各种应用中展现出巨大潜力，引起了研究人员和从业者的广泛关注。然而，对其长上下文能力的全面评估仍然不足。为解决这一问题，我们引入了Multimodal Needle in a Haystack (MMNeedle) 基准，专门设计用于评估 MLLM 的长上下文能力。除了多图像输入外，我们还采用图像拼接技术来进一步增加输入上下文长度，并开发了自动生成子图像级别检索标签的协议。基本上，MMNeedle 通过对 MLLM 进行压力测试，评估其在基于文本指令和图像内容描述中在一组图像 (大海) 中定位目标子图像 (Needle) 的能力。这种设置需要对广泛的视觉上下文有深入理解，并能够在长上下文图像输入中有效检索信息。通过该基准，我们评估了最先进的 MLLM，包括基于 API 和开源模型。结果表明，GPT-4o 在长上下文场景中始终优于其他模型，但在负样本（即当目标不在图像集中时）中出现了幻觉问题。我们对 MLLM 的全面长上下文评估也揭示了 API 基于模型与开源模型之间的显著性能差距。所有代码、数据和重现主要结果的说明都可在 https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack 获取。

# ∇^2DFT: A Universal Quantum Chemistry Dataset of Drug-Like Molecules and a Benchmark for Neural Network Potentials
[∇^2DFT：类药物分子的通用量子化学数据集和神经网络势的基准](https://arxiv.org/abs/2406.14347)

计算量子化学方法提供了准确的分子性质近似，这对于计算机辅助药物发现和其他化学科学领域至关重要。然而，高计算复杂度限制了其应用的可扩展性。神经网络势 (NNPs) 是量子化学方法的有力替代方案，但它们需要大量和多样化的数据集进行训练。本文介绍了一个新数据集和基准，称为 ∇^2DFT，基于 ∇DFT。该数据集包含两倍的分子结构，三倍的构象，新增的数据类型和任务，以及最先进的模型。数据集包括能量、力、17种分子性质、哈密顿矩阵和重叠矩阵以及波函数对象。所有计算均在每个构象的 DFT 级别 (omegaB97X-D/def2-SVP) 进行。此外，∇^2DFT 是第一个包含大量类药分子松弛轨迹的数据集。我们还引入了一个新基准，用于评估 NNPs 在分子性质预测、哈密顿预测和构象优化任务中的表现。最后，我们提出了一个可扩展的训练框架，并在其中实现了10个模型。

# The Devil is in the Details: StyleFeatureEditor for Detail-Rich StyleGAN Inversion and High Quality Image Editing
[细节决定成败：用于细节丰富的 StyleGAN 反演和高质量图像编辑的 StyleFeatureEditor](https://arxiv.org/abs/2406.10601)

通过StyleGAN反演操控真实图像属性的任务已经被广泛研究。该过程涉及从训练良好的StyleGAN生成器中搜索潜变量以合成真实图像，修改这些潜变量，然后合成带有期望编辑的图像。需要在重建质量和编辑能力之间取得平衡。早期研究利用低维W空间进行潜变量搜索，尽管编辑效果良好，但在重建细节方面表现欠佳。最近的研究转向高维特征空间F，成功反演输入图像但在编辑过程中丢失了许多细节。在本文中，我们介绍了StyleFeatureEditor——一种允许在w潜变量和F潜变量中进行编辑的新方法。该技术不仅允许重建更细致的图像细节，还能在编辑过程中保留这些细节。我们还提出了一种新的训练流程，专门用于训练模型以准确编辑F潜变量。我们的方法与最先进的编码方法进行了比较，证明我们的模型在重建质量方面表现出色，并能够编辑具有挑战性的域外示例。代码可在https://github.com/AIRI-Institute/StyleFeatureEditor 获取。

# Instruction Pre-Training: Language Models are Supervised Multitask Learners
[指令预训练：语言模型是监督多任务学习者](https://arxiv.org/abs/2406.14491)

自监督多任务预训练是最近语言模型 (LM) 成功的关键方法。然而，监督多任务学习仍然具有巨大的潜力，因为在训练后阶段进行扩展有助于更好的泛化。在本文中，我们通过提出指令预训练 (Instruction Pre-Training) 来探索监督多任务预训练，这一框架通过指令-响应对大规模增强了原始语料库，以预训练LM。指令-响应对由高效指令生成器生成，该生成器基于开源模型。在我们的实验中，我们合成了200M个覆盖40多种任务类别的指令-响应对，以验证指令预训练的有效性。在从零开始的预训练中，指令预训练不仅持续增强了预训练基准模型，还从进一步的指令调优中受益更多。在持续预训练中，指令预训练使Llama3-8B能够与Llama3-70B媲美，甚至超越。我们的模型、代码和数据可在https://github.com/microsoft/LMOps获取。