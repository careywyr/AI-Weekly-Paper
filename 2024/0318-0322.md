# Uni-SMART: Universal Science Multimodal Analysis and Research Transformer
[Uni-SMART: 通用科学的多模态分析研究 Transformer](https://arxiv.org/abs/2403.10301)

在科学研究及其应用领域，科学文献分析极为重要，它使研究人员得以在前人的基础上发展自己的工作。然而，科学知识的快速发展导致学术文章数量急剧增加，深入分析文献变得更加具有挑战性且耗时。大语言模型（LLM）的出现为这一挑战提供了新的解决方案。LLM 以其强大的文本摘要能力而著称，被认为是改进科学文献分析的有效工具。但是，现有的 LLM 对于包含分子结构、表格和图表等多模态元素的科学文献，理解和分析能力有限。这暴露了迫切需要能够全面理解和分析科学文献中多模态内容的新解决方案。为此，我们推出了 Uni-SMART（通用科学多模态分析与研究 Transformer），这是一个专门设计来深入理解多模态科学文献的创新模型。通过在几个领域进行严格的定量评估，Uni-SMART 的性能优于其他主流的文本中心 LLM。进一步地，我们将研究扩展到实际应用，如专利侵权检测和图表的深入分析。这些应用不仅证明了 Uni-SMART 的适应能力，还展示了它改变我们与科学文献互动方式的巨大潜力。

# RAFT: Adapting Language Model to Domain Specific RAG
[RAFT: 针对特定领域 RAG 的语言模型适配](https://arxiv.org/abs/2403.10131)

在大量文本数据上预训练大语言模型（LLM）已成为标准做法。在多种下游应用中，为了加入新知识（如时效性新闻或特定领域知识），常常通过 RAG 式提示或微调方式将其融入预训练模型。但如何让模型有效获取这些新知识，仍然是一个开放性问题。在这篇文章中，我们介绍了检索增强微调（RAFT），一种训练策略，能够提升模型在特定领域“开放书本”环境下的问答能力。RAFT 通过筛选出对回答问题有帮助的文档，并忽略无关（干扰）文档，训练模型准确引用相关文档的具体内容来回答问题。结合思维链式的回应方式，RAFT 增强了模型的推理能力。在特定领域的 RAG 应用中，RAFT 能够持续提高模型在 PubMed、HotpotQA 和 Gorilla 数据集上的表现，证明了它作为一种提升预训练 LLM 至领域内应用的有效后训练方法。RAFT 的代码和演示已在 github.com/ShishirPatil/gorilla 上开源。

# VideoAgent: Long-form Video Understanding with Large Language Model as Agent
[VideoAgent: 大语言模型驱动的长视频理解](https://arxiv.org/abs/2403.10517)

长篇视频理解是计算机视觉中的一个重大挑战，需要模型对长期的多模态序列进行深入推理。受人类在长篇视频理解过程中的认知启发，我们更注重于互动推理和规划，而不仅仅是处理大量视觉输入。我们推出了一种新型的基于智能体的系统，名为 VideoAgent，它以大语言模型为核心，通过迭代识别和整合关键信息来回答问题，并使用视觉语言基础模型作为转译和检索视觉信息的工具。在 EgoSchema 和 NExT-QA 这两个具有挑战性的基准上的评估显示，VideoAgent 分别达到了 54.1% 和 71.3% 的零样本准确率，平均只使用了 8.4 和 8.2 帧。这些成果不仅体现了我们方法的高效性和有效性，还突显了基于智能体方法在推动长篇视频理解进步方面的巨大潜力。

# Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations
[Alignment Studio: 使大语言模型符合特定情境规范](https://arxiv.org/abs/2403.09704)

通常，大语言模型的调整是由模型提供者来实现的，目的是增加或控制在各种用途和场景中普遍接受的行为。而在这篇文章中，我们提出一种方法和架构，允许应用开发者依据自己的价值观、社会规范、法律和规则来定制模型，并在不同需求间实现平衡。我们定义了对齐工作室架构的三个核心部分：构造者、指导者和审计者，它们共同确保语言模型行为的一致性。通过一个具体示例来阐述这种方法，即如何将一家公司的内部企业聊天机器人调整以符合其商业行为准则。

# Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation
[快速高分辨率图像合成与潜在对抗扩散蒸馏](https://arxiv.org/abs/2403.12015)

扩散模型是推动图像和视频合成发展的关键，但其推理速度缓慢。如最近提出的对抗性扩散蒸馏（ADD）等蒸馏方法试图使模型从多步推理转为单步推理，但因依赖于预训练的 DINOv2 判别器，这过程需要复杂且成本高昂的优化。我们提出了潜在对抗性扩散蒸馏（LADD），这是一种新颖的蒸馏方法，解决了 ADD 的局限性。与像素级的 ADD 不同，LADD 使用预训练潜在扩散模型的生成特性。LADD 简化了训练过程，并提升了性能，实现了高分辨率和多宽高比的图像合成。我们将 LADD 应用于 Stable Diffusion 3 (8B) 创建了 SD3-Turbo，这是一个高速模型，在仅用四步无引导采样的情况下达到了与最先进文本到图像生成器相媲美的性能。我们还系统地探究了其性能扩展性，并证明了 LADD 在图像编辑和修补等多种应用场景中的高效性。

# PERL: Parameter Efficient Reinforcement Learning from Human Feedback
[PERL: 基于人反馈的高效率参数强化学习](https://arxiv.org/abs/2403.10704)

从人类反馈中的强化学习（RLHF）已证明是一种有效的方法，用于让预训练大语言模型（LLM）与人类偏好相对齐。然而，RLHF 的训练成本高且过程复杂。本研究探讨了基于 Hu 等人 [2021] 提出的低秩适应（LoRA）方法的 RLHF，这是一种参数效率更高的训练方式。我们研究了参数效率强化学习（PERL）的设置，在此过程中利用 LoRA 进行奖励模型训练和强化学习。通过对 7 个基准测试（包括两个新数据集）的比较，我们发现 PERL 在性能上与传统的 RLHF 相媲美，且训练速度更快，内存需求更少。这不仅提升了 RLHF 的性能，同时也降低了将其作为大语言模型对齐技术的计算成本。此外，我们发布了两个新的偏好数据集：“Taskmaster Coffee”和“Taskmaster Ticketing”，旨在推动 RLHF 相关的研究。

# Larimar: Large Language Models with Episodic Memory Control
[Larimar: 具有情景记忆控制功能的大语言模型](https://arxiv.org/abs/2403.11901)

大语言模型（LLM）知识更新的高效性和准确性是当前最迫切的研究问题之一。本文提出了 Larimar，这是一种创新的、受大脑启发的架构，旨在通过分布式情景记忆提升 LLM 的功能。Larimar 实现了快速且一次性的知识更新，避免了成本高昂的重训练或微调。实验结果显示，即便在序列化编辑的复杂场景中，Larimar 在多个事实编辑基准上也能达到与最优基线相似的准确性，并显著提高速度，取决于基础 LLM，速度提升了 4 到 10 倍。其简洁通用的架构使得 Larimar 既灵活又通用。我们还展示了 Larimar 在选择性遗忘事实和输入上下文长度适应方面的有效性。

# mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding
[mPLUG-DocOwl 1.5: 面向无需 OCR 的文档理解的统一结构学习](https://arxiv.org/abs/2403.12895)

理解文本丰富的图片，如文档、表格和图表，关键在于结构信息。多模态大语言模型（MLLMs）虽具文本识别能力，但通常缺乏对这些图像的结构理解。在本研究中，我们强调结构信息在视觉文档理解中的作用，并提出统一结构学习方法来增强 MLLMs 的性能。该方法涵盖文档、网页、表格、图表和自然图像等五个领域的结构感知解析和多粒度文本定位任务。我们还开发了视觉到文本模块 H-Reducer，该模块通过卷积合并邻近图块来减少视觉特征长度，有效维持布局信息，从而提高了对高分辨率图像的理解效率。通过构建适用于文本丰富图像的结构感知文本序列和多粒度文本-边界框对，我们创建了全面的训练集 DocStruct4M 来支撑结构学习。此外，我们还构建了 DocReason25K 数据集，用于精细化的推理调优，进一步激发模型在文档领域的解释能力。我们的 DocOwl 1.5 模型在 10 个视觉文档理解基准上取得了领先性能，使得 7B LLM 的 MLLMs 性能在五个基准中提高了超过 10 个百分点。我们的代码、模型和数据集已在 https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5 公开。

# AnimateDiff-Lightning: Cross-Model Diffusion Distillation
[AnimateDiff-Lightning: 跨模型的扩散蒸馏技术](https://arxiv.org/abs/2403.12706)

我们推出 AnimateDiff-Lightning，实现极速视频生成。通过采用渐进式对抗性扩散蒸馏技术，我们的模型在少步骤视频生成领域达到了新的最先进水平。我们还改进了该模型，使其更适合视频内容。此外，我们提出一种方法，同时对多个基础扩散模型的概率流进行蒸馏，创造了一个风格适应性更广的蒸馏运动模块。我们高兴地向社区提供 AnimateDiff-Lightning 模型，以便大家使用。

# Mora: Enabling Generalist Video Generation via A Multi-Agent Framework
[Mora: 通过多智能体框架支持的通用视频生成](https://arxiv.org/abs/2403.13248)

Sora 是首个引起广泛关注的大规模通用视频生成模型，自 2024 年 2 月 OpenAI 发布后，尚无其他模型能匹敌其性能或支持的视频生成任务范围。鉴于完全公开的视频生成模型稀缺，大多数仍为闭源，本文提出了多智能体框架 Mora。Mora 融合了多个先进的视觉 AI 智能体，旨在达到 Sora 展示的通用视频生成水平。特别地，Mora 能在多种任务上模仿 Sora 的视频生成能力，包括文本到视频、文本条件下的图像到视频转换、视频扩展、视频编辑、视频链接及数字世界模拟等。广泛的实验结果表明，Mora 在这些任务上的性能接近 Sora。然而，从整体上看，我们的工作与 Sora 仍有一定的性能差距。我们期望本项目能指引视频生成领域通过协作 AI 智能体的未来方向。

# LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models
[LlamaFactory: 100 多个语言模型的统一高效微调](https://arxiv.org/abs/2403.13372)

为了让大语言模型（LLMs）适应下游任务，高效的微调工作至关重要。然而，将这些方法应用到不同的模型上需要相当的技术努力。我们推出了 LlamaFactory，这是一个集成了多种先进高效训练方法的统一框架。它使用户能够通过内置的 web UI LlamaBoard 灵活自定义 100 多个 LLMs 的微调过程，无需进行编程。我们通过语言建模和文本生成任务对框架的效率和有效性进行了实证检验。该框架已在 https://github.com/hiyouga/LLaMA-Factory 发布，目前已获得超过 13,000 个star和 1,600 次fork。

# Evolutionary Optimization of Model Merging Recipes
[模型合并配方的进化优化](https://arxiv.org/abs/2403.13187)

我们展示了进化算法在自动构建高性能基础模型方面的新应用。模型融合因其成本效益而被视为大语言模型（LLMs）开发的有前景的方法，但目前它仍然依赖于人的直觉和领域知识，这限制了其发展潜力。为此，我们提出了一种进化方法，它能自动探索多样化开源模型的有效组合，无需大量额外的训练数据或计算资源，便可发挥这些模型的集成优势。我们的方法同时涵盖参数空间和数据流空间的优化，超越了单个模型权重的调整。这种方法还支持跨领域融合，能生成如具备数学推理功能的日本LLM这样的模型。意外的是，尽管未经特定任务训练，我们的日本数学LLM在各种日本LLM基准测试中均展现了领先水平，甚至超越了参数更多的模型。此外，我们开发的文化感知型日本VLM在描述日本特有文化内容方面表现卓越，超越了以往的日本VLMs。这项工作不仅为开源社区带来了最新的顶尖模型，还开创了自动模型组合的新范式，为发展基础模型提供了探索更有效方法的可能。

# SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model
[SceneScript: 用自回归结构化语言模型重建场景](https://arxiv.org/abs/2403.13064)

我们推出了 SceneScript，一种自回归、基于 Token 的方法，能直接以结构化语言命令序列生成完整场景模型。受 Transformers 和大语言模型（LLMs）的最新成就启发，SceneScript 的场景表示方法转变了传统路径，不再依赖于网格、体素网格、点云或辐射场来描述场景。通过场景语言编解码器架构，我们的方法能直接从编码的视觉数据中推断结构化语言命令集。为了训练 SceneScript，我们开发了名为 Aria Synthetic Environments 的大规模合成数据集，包含 100,000 个高品质室内场景，带有逼真的、带有真实注释的第一人称视角渲染图。SceneScript 在建筑布局估计上实现了顶尖性能，在 3D 物体检测上也表现出竞争力。此外，SceneScript 的一个显著优势是它能够通过向结构化语言简单添加新命令，轻松适应新任务，如粗略的 3D 物体部件重构。

# MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?
[MathVerse: 你的多模态大语言模型是否真正理解视觉数学问题中的图表？](https://arxiv.org/abs/2403.14624)
多模态大语言模型（MLLMs）在视觉环境中的优异表现引起了前所未有的关注。但是，它们在解决视觉数学问题的能力还未被充分评价和理解。我们审视了现有基准测试，这些测试往往在文本问题中加入大量视觉内容，可能导致 MLLMs 在未真正解读图表的情况下就推出答案。为了解决这个问题，我们开发了 MathVerse，这是一个旨在对 MLLMs 进行全面和深入评价的视觉数学基准测试。我们从公开资源中精心收集了 2,612 个含有图表的高质量、多学科的数学问题，并将每个问题经人工标注转化为六个不同信息量的多模态版本，总计提供了 15,000 个测试样本。MathVerse 能够全面评估 MLLMs 真正理解视觉图表进行数学推理的能力。另外，我们设计了一种链式思维（CoT）评估策略，对输出答案进行更细致的评价。我们使用 GPT-4(V) 适应性地提取关键的推理步骤，通过详尽的错误分析对每一步进行评分，以此揭示 MLLMs 的中间 CoT 推理品质。我们期望 MathVerse 基准能为 MLLMs 的未来发展提供独到的洞见。项目页面：https://mathverse-cuhk.github.io

# DreamReward: Text-to-3D Generation with Human Preference
[DreamReward: 以人类偏好为基础的文本到 3D 转换](https://arxiv.org/abs/2403.14613)

最近，在文本提示的基础上生成 3D 内容取得了显著的进展。然而，现有的文本到 3D 方法经常产生与人类偏好不符的 3D 结果。在这篇论文中，我们介绍了一个名为 DreamReward 的综合框架，旨在利用人类偏好反馈来学习和优化文本到 3D 模型。我们首先通过一个系统的注释流程，包括评分和排名，收集了 25,000 份专家对比数据。然后，我们开发了 Reward3D —— 首个旨在有效捕捉人类偏好的通用文本到 3D 奖励模型。基于这一奖励模型，我们进一步进行了理论分析，并推出了 Reward3D 反馈学习（DreamFL），这是一个直接调整多视图扩散模型的算法，配合经过重新定义的评分器。在理论论证和广泛的实验对比基础上，DreamReward 成功产生了高保真且与 3D 一致的结果，并在与人类意图的匹配度上实现了显著提升。我们的研究显示，利用人类反馈来改进文本到 3D 模型具有巨大的潜力。

# Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference
[Cobra: 针对高效推理将 Mamba 扩展到多模态大语言模型](https://arxiv.org/abs/2403.14520)

近年来，多模态大语言模型（MLLM）在众多领域中已展现出显著的成功。尽管它们是众多下游任务的核心模型，但现有的 MLLMs 依赖于计算复杂度较高的 Transformer 网络。为此，我们开发了 Cobra，一个计算效率更高的线性复杂度 MLLM。Cobra 效率地将 Mamba 语言模型整合到视觉模态中。我们还研究了多种模态融合方案，以打造高效的多模态 Mamba。大量实验显示，Cobra 不仅在如 LLaVA-Phi、TinyLLaVA 和 MobileVLM v2 等计算效率先进的方法中表现出色，而且因其线性序列建模特性，速度更快。更有趣的是，Cobra 在应对视觉幻觉和判断空间关系的挑战性预测任务中也表现出色。特别地，Cobra 的性能与 LLaVA 相媲美，即使其参数数量仅为后者的约 43%。我们计划开源 Cobra 的全部代码，希望这一方法能促进未来 MLLM 面临的复杂性问题的研究。项目页面详见：https://sites.google.com/view/cobravlm。
