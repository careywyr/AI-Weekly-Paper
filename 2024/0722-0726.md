## Internal Consistency and Self-Feedback in Large Language Models: A Survey
[大语言模型中的内部一致性与自反馈：一项调查](https://arxiv.org/abs/2407.14507)

大语言模型 (LLMs) 本应提供准确答案，但往往出现推理不足或生成虚构内容的问题。为此，一系列以“自-”为前缀的研究，如自一致性 (Self-Consistency)、自改进 (Self-Improve) 和自精炼 (Self-Refine) 应运而生。这些研究共同点在于：利用 LLMs 自身的评估和更新机制来解决上述问题。然而，当前的调查研究多侧重于分类，而未深入探讨这些研究背后的动机，因此缺乏一个统一的总结视角。
本文中，我们提出一个名为内部一致性 (Internal Consistency) 的理论框架，该框架为诸如推理缺失和幻觉生成等现象提供了统一的解释。内部一致性通过采样方法，评估 LLMs 的潜在层、解码层和响应层之间的一致性。基于内部一致性框架，我们进一步提出一个简洁而有效的理论框架——自反馈 (Self-Feedback)，该框架能够深入挖掘内部一致性。自反馈框架包含两个核心模块：自我评估 (Self-Evaluation) 和自我更新 (Self-Update)，并已在多项研究中得到应用。
我们系统地根据任务类型和工作领域对这些研究进行分类；总结了相关的评估方法和基准；并深入探讨了“自反馈是否真的有效？”这一核心问题。我们提出了几个关键观点，包括“内部一致性的沙漏进化”、“一致性即（几乎）正确性”假设和“潜在与显式推理的悖论”。此外，我们还概述了未来研究的可能方向。相关实验代码、参考文献和统计数据已开源，可访问 [https://github.com/IAAR-Shanghai/ICSFSurvey](https://github.com/IAAR-Shanghai/ICSFSurvey) 获取。

## EVLM: An Efficient Vision-Language Model for Visual Understanding
[EVLM: 一种高效的视觉-语言模型用于视觉理解](https://arxiv.org/abs/2407.14177)

在多模态语言模型领域，大多数方法建立在类似于LLaVA的架构上。这些模型使用单层ViT特征作为视觉提示，直接将其与文本Token一起输入到语言模型中。然而，在处理长序列的视觉信号或视频等输入时，语言模型的自注意力机制可能导致显著的计算开销。此外，使用单层ViT特征使得大语言模型难以全面感知视觉信号。本文提出了一种高效的多模态语言模型，旨在最小化计算成本的同时使模型尽可能全面地感知视觉信号。我们的方法主要包括：(1) 采用类似于Flamingo的跨注意力机制进行图像-文本交互。(2) 利用层次化的ViT特征。(3) 引入专家混合(MoE)机制以增强模型效果。我们的模型在公共多模态基准测试中取得了竞争性的分数，并在图像描述和视频描述等任务中表现良好。

## LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference
[LazyLLM: 高效长上下文大语言模型推理的动态 Token 剪枝](https://arxiv.org/abs/2407.14057)

基于 Transformer 的大语言模型推理过程分为两个阶段依次进行：1) 预填充阶段，用于计算提示的 KV 缓存并生成第一个 Token；2) 解码阶段，用于生成后续 Token。对于长提示，预填充阶段必须为所有 Token 计算 KV 缓存，这会显著增加生成第一个 Token 的时间，从而可能成为生成过程的瓶颈。问题是，是否所有提示 Token 对于生成第一个 Token 都是必要的。为此，我们提出了一种新方法 LazyLLM，该方法在预填充和解码阶段选择性地计算对下一个 Token 预测重要的 Token 的 KV。与静态的一次性剪枝方法不同，LazyLLM 允许语言模型在不同的生成步骤中动态选择上下文中的不同子集 Token，即使它们可能在之前的步骤中被剪枝。在各种任务的标准数据集上进行的广泛实验表明，LazyLLM 是一种通用方法，可以无缝集成到现有语言模型中，显著加速生成过程而无需微调。例如，在多文档问答任务中，LazyLLM 将 LLama 2 7B 模型的预填充阶段加速了 2.34 倍，同时保持了准确性。

## SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models
[SlowFast-LLaVA：一种强大的无需训练的视频大语言模型基线](https://arxiv.org/abs/2407.15841)

我们提出 SlowFast-LLaVA（简称 SF-LLaVA），这是一种无需训练的视频大语言模型（LLM），能够同时捕捉详细的空间语义和长距离的时间上下文，且不会超出常用 LLM 的 Token 预算。该模型通过采用双流 SlowFast 输入设计，有效聚合从采样视频帧中提取的特征。具体而言，Slow 路径以低帧率提取特征，尽可能保留空间细节（例如，使用 24x24 Token）；而 Fast 路径以高帧率运行，通过更大的空间池化步幅（例如，下采样 6x），专注于捕捉运动线索。这种设计使我们能够充分捕捉对理解视频细节有益的空间和时间特征。实验结果表明，SF-LLaVA 在广泛的 video 任务上优于现有的无需训练的方法。在一些基准测试中，其性能可与甚至超越在视频数据集上微调的最先进的 Video LLMs。

## NNsight and NDIF: Democratizing Access to Foundation Model Internals
[NNsight 和 NDIF：实现基础模型内部访问的民主化](https://arxiv.org/abs/2407.14561)

最先进的基础模型因其巨大规模，限制了科学家的访问。在大模型规模上进行定制实验需要高成本且复杂的硬件和工程，这对大多数研究人员来说不切实际。为解决这些问题，我们推出了 NNsight，一个具有简单、灵活 API 的开源 Python 包，可通过构建计算图进行对任何 PyTorch 模型的干预。同时，我们推出了 NDIF，一个协作研究平台，通过 NNsight API 提供给研究人员对基础规模大语言模型 (LLM) 的访问。代码、文档和教程可于 https://www.nnsight.net 获取。

## Knowledge Mechanisms in Large Language Models: A Survey and Perspective
[大语言模型中的知识机制：综述与展望](https://arxiv.org/abs/2407.15017)
理解大语言模型 (LLMs) 中的知识机制对于实现可信赖的通用人工智能 (AGI) 至关重要。本文从知识利用和知识演化的新分类法出发，综述了知识机制的分析。知识利用部分详细探讨了记忆、理解、应用及创造的机制。知识演化则聚焦于个体和群体 LLMs 中知识的动态发展。此外，我们还探讨了 LLMs 已掌握的知识内容、参数知识脆弱性的原因，以及可能存在的难以处理的潜在暗知识 (假设)。我们期望这项工作能加深对 LLMs 中知识的理解，并为未来研究提供有价值的见解。

## Compact Language Models via Pruning and Knowledge Distillation
[通过剪枝和知识蒸馏实现紧凑的语言模型](https://arxiv.org/abs/2407.14679)

针对不同部署规模和大小的大语言模型 (LLMs)，目前是通过从头开始训练每个变体来生产的，这一过程极其消耗计算资源。本文探讨了是否可以通过对现有 LLM 进行剪枝，然后使用原始训练数据的一小部分（<3%）重新训练，作为重复全面重新训练的合适替代方案。为此，我们开发了一套实用且有效的压缩最佳实践，结合深度、宽度、注意力和 MLP 剪枝以及基于知识蒸馏的重新训练；这些最佳实践是通过详细的经验探索，针对每个轴的剪枝策略、轴组合方法、蒸馏策略以及达到最优压缩架构的搜索技术得出的。我们利用这一指南将 Nemotron-4 系列 LLM 压缩了 2-4 倍，并在各种语言建模任务上将其性能与类似大小的模型进行了比较。使用我们的方法从已经预训练的 15B 模型中衍生出 8B 和 4B 模型，与从头开始训练相比，每个模型所需的训练 Token 数量减少了多达 40 倍；这导致训练整个模型家族（15B、8B 和 4B）的计算成本节省了 1.8 倍。与从头开始训练相比，Minitron 模型在 MMLU 分数上提高了多达 16%，与 Mistral 7B、Gemma 7B 和 Llama-3 8B 等其他社区模型表现相当，并且超过了文献中的最先进压缩技术。我们在 Huggingface 上开源了 Minitron 模型权重，相应的补充材料包括可在 GitHub 上获得的示例代码。

## CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis
[CoD：基于诊断链构建可解释的医疗智能体](https://arxiv.org/abs/2407.13301)

大语言模型（LLMs）的兴起极大地推动了医学诊断领域的发展，但模型可解释性的问题依然突出。本研究提出了诊断链（CoD）方法，旨在提升基于LLM的医学诊断的可解释性。CoD将诊断流程模拟为医生的思考过程，形成一条清晰的诊断链，从而提供透明的推理过程。同时，CoD还输出疾病的置信度分布，确保诊断决策的透明度。这种可解释性不仅使诊断过程更加可控，还通过降低置信度的熵值来辅助识别关键症状，以便进一步询问。基于CoD，我们开发了DiagnosisGPT，该系统能够诊断多达9604种疾病。实验结果显示，DiagnosisGPT在诊断性能上超越了其他LLM，并且在保证诊断准确性的同时，提供了高度的可解释性。

## KAN or MLP: A Fairer Comparison
[KAN 与 MLP：更公正的对比](https://arxiv.org/abs/2407.16674)

本文并非提出新方法，而是致力于对 KAN 和 MLP 模型在多个领域的性能进行更为公正和全面的对比，涵盖机器学习、计算机视觉、音频处理、自然语言处理及符号公式表示等任务。我们通过控制参数数量和 FLOPs，确保了比较的公平性。主要发现表明，除符号公式表示任务外，MLP 在大多数任务中表现优于 KAN。我们对 KAN 进行了深入的消融研究，揭示其在符号公式表示任务中的优势主要归功于 B 样条激活函数。将 B 样条应用于 MLP 后，符号公式表示的性能大幅提升，甚至超越了 KAN。但在 MLP 原本就表现出色的其他任务中，B 样条并未带来显著的性能提升。此外，我们在标准类增量持续学习环境中发现，KAN 的遗忘问题比 MLP 更为突出，这一发现与 KAN 论文中的结论不一致。我们期待这些研究结果能为未来 KAN 及相关 MLP 替代方案的研究提供有价值的参考。项目链接：https://github.com/yu-rp/KANbeFair

## MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequence
[MovieDreamer: 分层生成用于连贯的长视觉序列](https://arxiv.org/abs/2407.16655)

近期视频生成技术的进展主要利用扩散模型处理短时长的内容。然而，这些方法在模拟复杂叙事和维持角色在长时间内的连贯性方面往往不足，这对于电影等长篇视频制作至关重要。我们提出MovieDreamer，一种新颖的分层框架，它结合了自回归模型的优势与基于扩散的渲染技术，首次实现了具有复杂情节发展和高度视觉保真度的长时视频生成。我们的方法利用自回归模型确保全局叙事连贯性，预测视觉Token序列，随后通过扩散渲染将其转化为高质量视频帧。这种方法类似于传统电影制作过程，其中复杂故事被分解为可管理的场景捕捉。此外，我们采用了一种多模态脚本，通过详细的角色信息和视觉风格丰富场景描述，增强了场景间的连续性和角色身份。我们在多种电影类型上进行了广泛实验，证明我们的方法不仅在视觉和叙事质量上达到优越水平，而且有效地将生成内容的时间长度显著扩展至当前技术能力之外。主页: https://aim-uofa.github.io/MovieDreamer/.

## OpenDevin: An Open Platform for AI Software Developers as Generalist Agents
[OpenDevin：面向 AI 软件开发者作为通用智能体的开源平台](https://arxiv.org/abs/2407.16741)

软件是人类所拥有的最强大工具之一；它使熟练的程序员能够以复杂而深刻的方式与世界互动。同时，得益于大语言模型 (LLMs) 的改进，AI 智能体与周围环境互动并产生影响的能力也迅速发展。在本文中，我们介绍了 OpenDevin，一个用于开发强大且灵活的 AI 智能体的平台，这些智能体以类似人类开发者的方式与世界互动：通过编写代码、与命令行交互以及浏览网页。我们描述了该平台如何实现新智能体的实施、安全地与沙盒环境进行代码执行的交互、多个智能体之间的协调以及评估基准的整合。基于我们目前整合的基准，我们对超过 15 项具有挑战性的任务进行了智能体评估，包括软件工程（例如，SWE-Bench）和网页浏览（例如，WebArena）等。OpenDevin 采用宽松的 MIT 许可证发布，是一个跨越学术界和工业界的社区项目，拥有超过 1.3K 的贡献来自 160 多名贡献者，并将持续改进。

## $VILA^2$: VILA Augmented VILA
[$VILA^2$: VILA 增强的 VILA](https://arxiv.org/abs/2407.17453)

视觉语言模型 (VLMs) 在大型语言模型 (LLMs) 成功的推动下迅速发展。尽管模型架构和训练基础设施迅速进步，数据管理仍未得到充分探索。当数据量和质量成为瓶颈时，现有工作要么直接从互联网上抓取更多原始数据，这些数据没有数据质量保证，要么从黑盒商业模型（例如 GPT-4V / Gemini）中提炼，导致性能受限于该模型。在这项工作中，我们引入了一种新颖的方法，包括自我增强步骤和专家增强步骤，以迭代地提高数据质量和模型性能。在自我增强步骤中，VLM 重新标注其自身的预训练数据以提高数据质量，然后使用这个精炼的数据集从头开始重新训练以提高模型性能。这个过程可以迭代多次。一旦自我增强饱和，我们采用几个从自我增强的 VLM 微调而来的具有特定领域专业知识的专家 VLM，通过任务导向的重新标注和重新训练，进一步将专家知识融入到通用 VLM 中。通过结合自我增强和专家增强的训练，我们引入了 VILA^2（VILA 增强的 VILA），这是一个 VLM 系列，在广泛的任务上持续提高准确性，并在开放源代码模型中的 MMMU 排行榜上实现了新的最先进结果。

## Diffree: Text-Guided Shape Free Object Inpainting with Diffusion Model
[Diffree: 基于扩散模型的文本引导形状自由物体修复](https://arxiv.org/abs/2407.16982)

本文针对仅通过文本引导进行图像物体添加的重要问题进行了研究。这一任务具有挑战性，因为新添加的物体需要无缝融入图像，并保持一致的视觉上下文，如光照、纹理和空间位置。尽管现有的文本引导图像修复方法能够添加物体，但它们往往无法保持背景的一致性，或者需要繁琐的人工干预来指定边界框或用户涂鸦的掩码。为了解决这一难题，我们提出了 Diffree，这是一个文本到图像 (T2I) 模型，它通过仅文本控制来实现文本引导的物体添加。为此，我们精心构建了 OABench，这是一个通过高级图像修复技术移除物体的精致合成数据集。OABench 包含 74K 个真实世界的元组，包括原始图像、移除物体的修复图像、物体掩码和物体描述。在 OABench 上使用带有额外掩码预测模块的 Stable Diffusion 模型进行训练，Diffree 能够独特地预测新物体的位置，并通过仅文本引导实现物体添加。大量实验表明，Diffree 在添加新物体方面表现出色，成功率高，同时保持背景一致性、空间适宜性和物体相关性与质量。

