## Apollo: An Exploration of Video Understanding in Large Multimodal Models
[Apollo: 大语言模型中视频理解探索](https://arxiv.org/abs/2412.10360)

尽管视频感知能力已迅速集成到大语言模型 (LMM) 中，但其驱动视频理解的基础机制仍未被充分理解。因此，该领域中的许多设计决策缺乏适当的依据或分析。训练和评估此类模型的高计算成本，加上有限的开放研究，阻碍了视频-LMM 的发展。为解决这一问题，我们进行了一项全面研究，旨在揭示有效驱动 LMM 中视频理解的因素。
  我们首先批判性地审视了与视频-LMM 研究相关的高计算需求的主要贡献因素，并发现了规模一致性 (Scaling Consistency)，即在较小模型和数据集 (达到临界规模) 上做出的设计和训练决策能有效迁移到更大模型上。基于这些见解，我们探索了视频-LMM 的许多视频特定方面，包括视频采样、架构、数据组成、训练计划等。例如，我们证明了训练期间的 fps (frames per second) 采样远优于均匀帧采样，并确定了哪些视觉编码器最适合视频表示。
  在这些发现指导下，我们引入了 Apollo，这是一系列在不同模型规模上实现卓越性能的先进 LMM。我们的模型能够高效感知长达一小时的视频，其中 Apollo-3B 在 LongVideoBench 上以 55.1 的分数超越了大多数现有 7B 模型。Apollo-7B 在与 7B LMM 的比较中处于领先地位，在 MLVU 上获得 70.9 分，在 Video-MME 上获得 63.3 分。

## GenEx: Generating an Explorable World
[GenEx: 生成可探索的世界](https://arxiv.org/abs/2412.09624)

理解和探索三维物理现实世界一直是人工智能发展的核心挑战。在本研究中，我们通过引入 GenEx 系统，向这一目标迈出了重要一步。GenEx 能够规划复杂的具身世界探索任务，其生成式想象力为周围环境建立了先验（期望）。

GenEx 从一张 RGB 图像中生成一个完整的 3D 一致的想象环境，并通过全景视频流使其栩栩如生。借助从 Unreal Engine 中精选的可扩展 3D 世界数据，我们的生成模型在物理世界中得到了完善。该模型轻松捕捉到连续的 360 度环境，为 AI 智能体提供了广阔的探索和互动空间。

GenEx 实现了高质量的世界生成、长时间轨迹上的稳健循环一致性，并展示了强大的 3D 能力，如一致性和主动 3D 映射。借助对世界的生成式想象力，GPT 辅助的智能体能够执行复杂的具身任务，包括无目标探索和目标导向导航。这些智能体利用对物理世界未见部分的预测期望来精炼其信念，基于潜在决策模拟不同结果，并做出更明智的选择。

总之，GenEx 为具身 AI 在想象空间中的发展提供了变革性平台，并展现了将其能力扩展到现实世界探索的潜力。

## SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding
[SynerGen-VL: 通过视觉专家和Token折叠实现图像理解和生成的协同研究](https://arxiv.org/abs/2412.09604)

大语言模型 (LLMs) 的显著成功已应用于多模态领域，在图像理解和生成方面取得了卓越的性能。最近，开发统一的多模态大语言模型 (MLLMs) 的努力取得了积极成果。但现有方法通常涉及模型架构或训练管道的复杂设计，增加了模型训练和扩展的难度。本文提出 SynerGen-VL，一种简单而强大的无编码器 MLLM，能够进行图像理解和生成。针对现有无编码器统一 MLLMs 中识别的挑战，我们引入了 Token 折叠机制和基于视觉专家的渐进对齐预训练策略。这两种策略有效地支持高分辨率图像理解，同时降低训练复杂性。通过大规模混合图像-文本数据训练并采用统一的下一个 Token 预测目标后，SynerGen-VL 在性能上达到或超过了现有具有可比或更小参数规模的无编码器统一 MLLMs，从而缩小了与特定任务最先进模型的差距，突显了未来统一 MLLMs 的潜力。我们的代码和模型将发布。

## Large Action Models: From Inception to Implementation
[大动作模型：从构想到实现](https://arxiv.org/abs/2412.10047)

随着 AI 技术的不断进步，市场对能够超越语言辅助、执行现实世界任务的智能系统需求日益增长。这一趋势促使我们从传统擅长文本生成的大语言模型 (LLM) 向专为动态环境中动作生成与执行设计的大动作模型 (LAM) 转变。借助智能体系统，LAM 有望将 AI 从被动语言理解升级为主动任务执行，成为迈向通用人工智能的重要里程碑。

在本论文中，我们提出了一套开发 LAM 的完整框架，系统性地涵盖了从概念到部署的全过程。首先，我们对 LAM 进行了概述，强调其独特性，并阐明了其与 LLM 的差异。以基于 Windows OS 的智能体为例，我们详细介绍了 LAM 开发的关键步骤，包括数据收集、模型训练、环境集成、基础构建及评估。这一通用流程可作为在多个应用领域构建功能性 LAM 的参考模板。最后，我们分析了 LAM 当前的局限性，探讨了未来研究与工业应用的方向，并强调了在实现 LAM 在实际应用中全部潜力时所面临的挑战与机遇。

本文所用数据收集过程的代码已公开发布：https://github.com/microsoft/UFO/tree/main/dataflow，相关详细文档可访问 https://microsoft.github.io/UFO/dataflow/overview/。

## BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities
[BiMediX2: 多模态生物医学专家大语言模型](https://arxiv.org/abs/2412.07769)

本文介绍 BiMediX2，这是一个双语 (阿拉伯语-英语) 生物医学专家大语言模型 (LMM)，采用统一架构，集成了文本和视觉模态，从而实现高级图像理解和医疗应用。BiMediX2 基于 Llama3.1 架构，整合了文本和视觉能力，支持英语和阿拉伯语中的无缝交互，包括基于文本的输入和涉及医学图像的多轮对话。

该模型在包含 160 万条多样化医学交互样本的广泛双语医疗数据集上进行训练，涵盖文本和图像模态，混合了阿拉伯语和英语。我们还提出了首个基于 GPT-4o 的双语医学 LMM 基准，名为 BiMed-MBench。

BiMediX2 在基于文本和图像的任务上进行了基准测试，在多个医学基准上实现了最先进的性能。与最近的先进模型相比，它在医学大语言模型评估基准中表现更优。此外，我们的模型在多模态医学评估中也树立了新的基准，在英语评估中提高了超过 9%，在阿拉伯语评估中提高了超过 20%。在 UPHILL 事实准确性评估中，它超越了 GPT-4 约 9%，并在医学视觉问答、报告生成和报告摘要等任务中表现出色。

项目页面包括源代码和训练模型，可访问 https://github.com/mbzuai-oryx/BiMediX2 获取。

## Byte Latent Transformer: Patches Scale Better Than Tokens
[Byte Latent Transformer: 补丁扩展优于 Token](https://arxiv.org/abs/2412.09871)

我们引入了 Byte Latent Transformer (BLT)，这是一种新的字节级大语言模型 (LLM) 架构，首次在规模扩展上实现了与基于 Token 的 LLM 性能相匹配，并在推理效率和鲁棒性方面取得了显著改进。BLT 将字节编码为动态大小的补丁，这些补丁作为计算的基本单元。补丁根据下一个字节的熵进行分段，在数据复杂度较高的地方分配更多的计算资源和模型容量。我们首次展示了针对字节级模型的 FLOP 控制扩展研究，参数规模高达 8B，训练数据量高达 4T。研究结果表明，无需固定词汇表即可扩展基于原始字节训练的模型是可行的。由于在数据可预测时动态选择长补丁，训练和推理效率均得到提升，同时在推理能力和长尾泛化方面也实现了定性改进。总体而言，在固定推理成本下，BLT 的扩展效果显著优于基于 Token 的模型，因为它同时增加了补丁和模型的大小。

## Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models
[评估智能体：视觉生成模型的高效与可提示评估框架](https://arxiv.org/abs/2412.09645)

视觉生成模型的最新进展已实现高质量的图像和视频生成，并推动了多样化应用的发展。然而，评估这些模型通常需要采样数百或数千张图像或视频，导致计算成本高昂，尤其是对于采样速度较慢的基于扩散的模型。此外，现有评估方法依赖于固定流程，忽视了特定用户的需求，仅提供缺乏解释的数值结果。相比之下，人类通过观察少数样本即可快速形成对模型能力的印象。为了模拟这一过程，我们提出了评估智能体框架，该框架采用类似人类的策略，通过每轮仅使用少量样本来实现高效、动态、多轮评估，同时提供详细且用户定制的分析。该框架具有四个关键优势：1) 高效性，2) 针对多样化用户需求的可提示评估，3) 超越单一数值评分的可解释性，以及 4) 跨多种模型和工具的可扩展性。实验表明，评估智能体将评估时间减少至传统方法的 10%，同时提供可比的结果。评估智能体框架完全开源，以推动视觉生成模型及其高效评估的研究。

## BrushEdit: All-In-One Image Inpainting and Editing
[BrushEdit: 一体化图像修复与编辑](https://arxiv.org/abs/2412.10316)

随着扩散模型的发展，基于反演和基于指令的图像编辑技术取得了显著进展。然而，现有的基于反演的方法在处理大幅修改（如添加或移除对象）时存在困难，这是由于反演噪声的结构化特性限制了实质性的变化。另一方面，基于指令的方法通常将用户限制在黑箱操作中，难以直接指定编辑区域和强度。

为了克服这些局限，我们提出了 BrushEdit，一种基于修复的指令引导图像编辑新范式。BrushEdit 结合了多模态大语言模型 (MLLMs) 和图像修复模型，旨在实现自主、用户友好且交互式的自由形式指令编辑。具体来说，我们设计了一个系统，通过在智能体协作框架中集成 MLLMs 和双分支图像修复模型，来执行编辑类别分类、主要对象识别、掩码获取和编辑区域修复，从而实现自由形式指令编辑。

大量实验表明，我们的框架成功整合了 MLLMs 和修复模型，在包括掩码区域保留和编辑效果一致性在内的七个指标上表现优异。

## RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation
[RetroLLM: 赋能大语言模型在生成过程中检索细粒度证据](https://arxiv.org/abs/2412.11919)

大语言模型 (LLMs) 虽然具备强大的生成能力，但往往会产生不准确的信息（即幻觉问题）。检索增强生成 (RAG) 通过引入外部知识提供了一种有效的解决方案，但现有方法仍存在一些不足：独立检索器的额外部署成本、从检索文本块中引入的冗余输入 Token，以及检索与生成之间缺乏协同优化。为解决这些问题，我们提出了 RetroLLM，一个将检索与生成整合为一体的统一框架，使 LLMs 能够直接从语料库中生成受限解码的细粒度证据。此外，为减少在受限证据生成过程中可能出现的错误剪枝，我们引入了 (1) 分层 FM-Index 约束，该约束通过生成语料库约束线索，在证据生成前识别相关文档子集，从而缩小无关解码空间；以及 (2) 前瞻性约束解码策略，该策略通过考虑未来序列的相关性，进一步提升证据的准确性。在五个开放域 QA 数据集上的广泛实验表明，RetroLLM 在域内和域外任务中均展现出卓越的性能。代码可在 https://github.com/sunnynexus/RetroLLM 获取。

## ColorFlow: Retrieval-Augmented Image Sequence Colorization
[ColorFlow: 检索增强的图像序列着色](https://arxiv.org/abs/2412.11815)
自动为黑白图像序列进行着色，同时保持角色和对象的身份（ID），是一项具有显著市场需求的复杂任务，尤其在卡通或漫画系列的着色中。尽管使用大规模生成模型（如扩散模型）在视觉着色方面取得了进展，但可控性和身份一致性方面的挑战仍然存在，使得当前的解决方案难以满足工业应用的需求。为此，我们提出了ColorFlow，一个专为工业应用中的图像序列着色设计的三阶段扩散模型框架。与现有方法需要针对每个ID进行微调或显式提取ID嵌入不同，我们提出了一种新颖、稳健且可泛化的检索增强着色流程，通过使用相关颜色参考来实现图像着色。我们的流程采用了双分支设计：一个分支用于颜色身份提取，另一个分支用于着色，充分利用了扩散模型的优势。我们利用扩散模型中的自注意力机制，实现了强大的上下文理解和颜色身份匹配。为了评估我们的模型，我们引入了ColorFlow-Bench，一个用于基于参考的着色的综合基准。结果表明，ColorFlow在多个指标上均优于现有模型，为序列图像着色设定了新的标准，并有望为艺术行业带来积极影响。我们在项目页面上发布了代码和模型：https://zhuang2002.github.io/ColorFlow/。

## Are Your LLMs Capable of Stable Reasoning?
[大语言模型能否实现稳定推理？](https://arxiv.org/abs/2412.13147)

大语言模型 (LLMs) 的快速发展在复杂推理任务中取得了显著进展。然而，基准测试结果与实际应用之间仍存在显著差距。我们认为，这一差距主要源于当前的评估协议和指标，这些协议和指标未能全面反映大语言模型的能力，尤其是在复杂推理任务中，准确性和一致性至关重要。本研究主要贡献如下：首先，我们提出了 G-Pass@k，这是一种新型评估指标，能够在多次采样尝试中连续评估模型性能，量化模型的峰值性能潜力及其稳定性。其次，我们推出了 LiveMathBench，这是一个动态基准，包含具有挑战性的当代数学问题，旨在在评估过程中最大限度地降低数据泄露风险。通过使用 G-Pass@k 对最先进的大语言模型进行广泛实验，并结合 LiveMathBench，我们全面分析了这些模型的最大能力和操作一致性。研究结果表明，大语言模型在“现实”推理能力方面仍有显著提升空间，凸显了开发更强大评估方法的必要性。基准测试和详细结果可通过以下链接获取：
https://github.com/open-compass/GPassK.

## Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models
[多维评估：在大规模多模态模型中基准测试现实世界的个性化](https://arxiv.org/abs/2412.12606)

大规模多模态模型 (Large Multimodal Models, LMMs) 领域快速发展，出现了多种具有显著能力的模型。然而，现有基准未能全面、客观、准确地评估 LMMs 是否符合现实场景中人类的多样化需求。为了弥补这一不足，我们提出了多维评估 (Multi-Dimensional Insights, MDI) 基准，该基准包含超过 500 张图片，涵盖人类生活的六个常见场景。值得注意的是，MDI-Benchmark 相比现有评估具有两大显著优势：(1) 每张图片附带两种类型的问题：简单问题用于评估模型对图片的理解，复杂问题用于评估模型分析和推理超越基本内容的能力。(2) 考虑到不同年龄段的人在面对相同场景时需求和视角各异，我们的基准将问题分为三个年龄类别：年轻人、中年人和老年人。这种设计从而能够详细评估 LMMs 满足不同年龄段偏好和需求的能力。使用 MDI-Benchmark，强大的模型如 GPT-4o 在年龄相关任务上达到 79% 的准确率，显示现有 LMMs 在应对现实应用方面仍有相当大的改进空间。未来，我们预计 MDI-Benchmark 将为 LMMs 中现实世界个性化的对齐开辟新的途径。MDI-Benchmark 数据和评估代码可在 https://mdi-benchmark.github.io/ 获取。

## OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain
[OmniEval: 金融领域全方位自动RAG评估基准](https://arxiv.org/abs/2412.13018)

作为大语言模型 (LLMs) 的典型应用之一，检索增强生成 (RAG) 技术在特定领域（尤其是大语言模型可能缺乏领域知识的领域）中受到了广泛关注。本文介绍了一个金融领域的全方位自动RAG基准——OmniEval。该基准具有多维度的评估框架，具体包括：
1. 基于矩阵的RAG场景评估系统，将查询分为五个任务类别和16个金融主题，从而对多样化的查询场景进行系统化评估；
2. 多维度的评估数据生成方法，结合了基于GPT-4的自动生成和人工标注，生成实例的人工评估接受率达到87.47%；
3. 多阶段的评估系统，分别评估检索和生成性能，全面覆盖RAG流程；
4. 基于规则和LLM的稳健评估指标，通过人工标注和LLM评估器的有监督微调，提升了评估的可靠性。
实验表明，OmniEval涵盖了广泛的测试数据集，并揭示了RAG系统在不同主题和任务中的性能差异，为RAG模型在特定领域的能力提升提供了重要机会。我们在 https://github.com/RUC-NLPIR/OmniEval{https://github.com/RUC-NLPIR/OmniEval} 开源了基准代码。

## Compressed Chain of Thought: Efficient Reasoning Through Dense Representations
[压缩思维链：通过密集表示进行高效推理](https://arxiv.org/abs/2412.13171)

思维链 (Chain-of-thought, CoT) 解码通过增加生成延迟来提升语言模型的推理性能。近期研究提出了沉思 Token (contemplation tokens) 的变体，我们引入这一术语，指的是在推理过程中用于进行额外计算的特殊 Token。先前的工作使用从离散嵌入集合中提取的固定长度序列作为沉思 Token。在此，我们提出压缩思维链 (Compressed Chain-of-Thought, CCoT)，这是一个生成内容丰富且连续的沉思 Token 的框架，其序列长度可变。生成的沉思 Token 是显式推理链的压缩表示，并且我们的方法可以应用于现成的解码器语言模型。通过实验，我们展示了 CCoT 如何利用密集内容丰富的表示进行额外推理，从而提升准确性。此外，通过调整生成的沉思 Token 数量，推理改进可以按需自适应调整。

## Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference
[更智能、更优、更快速、更长时：一种用于高效快速、内存节省及长上下文微调与推理的现代双向编码器](https://arxiv.org/abs/2412.13663)

仅编码器的 Transformer 模型（如 BERT）在检索和分类任务中相对于较大的仅解码器模型提供了优异的性能与尺寸平衡。尽管 BERT 是众多生产流程的主力，但自其发布以来，对其的帕累托优化进展有限。本文中，我们提出了 ModernBERT，将现代模型优化技术引入仅编码器模型，并实现了对旧编码器的重要帕累托改进。在 2 万亿 Token 上训练，并支持 8192 序列长度，ModernBERT 在涵盖多样化分类任务及不同领域（包括代码）的单向和多向检索的广泛评估中展现了领先的结果。此外，ModernBERT 不仅在下游任务中表现出色，还是速度最快、内存效率最高的编码器，并专门设计用于在通用 GPU 上进行推理。

## TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks
[TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks](https://arxiv.org/abs/2412.14161)

我们每天都在与计算机交互，无论是在日常生活还是工作中，许多工作都可以通过计算机和互联网来完成。与此同时，随着大语言模型 (LLMs) 的进步，AI 智能体与环境交互并产生影响的能力也迅速发展。然而，AI 智能体在加速甚至自主完成工作任务方面的表现如何？这一问题的答案对希望将 AI 融入工作流程的行业以及理解 AI 采用对劳动力市场影响的经济政策都至关重要。为了评估这些大语言模型智能体在执行实际专业任务方面的进展，本文提出了 TheAgentCompany，这是一个可扩展的基准，用于评估通过浏览网页、编写代码、运行程序以及与同事沟通等方式与世界交互的 AI 智能体。我们构建了一个包含内部网站和数据的自包含环境，模拟了一个小型软件公司，并设计了多种可能由员工执行的任务。我们测试了基于 API 的封闭模型和开放权重语言模型 (LMs) 驱动的基线智能体，结果显示，在最具竞争力的智能体中，24% 的任务可以自主完成。这表明，在模拟真实工作环境的情况下，许多简单任务可以自主解决，但更复杂的长远任务仍超出了当前系统的处理能力。

## AniDoc: Animation Creation Made Easier
[AniDoc: 动画制作变得更简单](https://arxiv.org/abs/2412.14173)

2D 动画的制作遵循行业标准的流程，涵盖四个关键阶段：角色设计、关键帧动画、中间帧绘制和上色。我们的研究致力于通过利用日益强大的生成式 AI 来降低上述过程中的劳动力成本。基于视频扩散模型，AniDoc 作为一个视频线稿上色工具，能够根据参考角色规范自动将草图序列转化为彩色动画。我们的模型采用对应匹配作为显式引导，从而对参考角色与每个线稿帧之间的变化（如姿势）表现出很强的稳健性。此外，我们的模型还能自动化中间帧绘制过程，用户只需提供角色图像以及起始和结束草图，即可轻松创建时间上连贯的动画。我们的代码可在以下网址获取：https://yihao-meng.github.io/AniDoc_demo。

## No More Adam: Learning Rate Scaling at Initialization is All You Need
[无需 Adam：初始化时的学习率缩放已足够](https://arxiv.org/abs/2412.11768)

在本研究中，我们探讨了自适应梯度方法在深度神经网络训练中的必要性。SGD-SaI 是对带有动量的随机梯度下降（SGDM）的一种简单而有效的改进。SGD-SaI 在初始化时根据各参数组的梯度信噪比（g-SNR）对学习率进行缩放（SaI）。通过调整学习率而不依赖自适应二阶动量，SGD-SaI 能够从训练一开始就避免不平衡现象，并将优化器的内存使用量减少一半，相比 AdamW 更为高效。尽管 SGD-SaI 简单且高效，但在各种基于 Transformer 的任务中，它始终与 AdamW 持平或表现更优，成功解决了使用 SGD 训练 Transformer 的长期难题。在 ImageNet-1K 分类任务中，SGD-SaI 与 Vision Transformer（ViT）结合表现优异，同时在 GPT-2 预训练大语言模型（LLM，仅 Transformer 解码器）中也展示了其对超参数变化的鲁棒性和广泛应用的实用性。此外，在 LLM 的 LoRA 微调和扩散模型等任务中，SGD-SaI 也表现出色，始终优于当前最先进的优化器。从内存效率的角度来看，SGD-SaI 在优化器状态方面实现了显著的内存节省，在全精度训练设置下，GPT-2（1.5B 参数）减少了 5.93 GB 的内存使用，Llama2-7B 减少了 25.15 GB 的内存使用，相比 AdamW 更为高效。

## Qwen2.5 Technical Report
[Qwen2.5 技术报告](https://arxiv.org/abs/2412.15115)

在本报告中，我们介绍了 Qwen2.5，这是一系列旨在满足多样化需求的大语言模型 (LLMs)。与之前的版本相比，Qwen 2.5 在预训练和后训练阶段都取得了显著进展。在预训练阶段，我们将高质量的预训练数据集从之前的 7 万亿 tokens 扩展到 18 万亿 tokens，为模型提供了更丰富的语料库，从而增强了其常识、专家知识和推理能力。在后训练阶段，我们采用了精细的监督微调策略，使用了超过 100 万个样本，并结合多阶段的强化学习，进一步优化了模型的性能。这些后训练技术不仅增强了模型对人类偏好的适应性，还显著提升了长文本生成、结构化数据分析和指令跟随的能力。

为了应对多样化和多变的使用场景，我们推出了丰富的 Qwen2.5 LLM 系列。开放权重版本包括基础模型和指令调优模型，并提供了量化版本，以满足不同用户的需求。此外，对于托管解决方案，我们目前提供了两种混合专家 (MoE) 变体：Qwen2.5-Turbo 和 Qwen2.5-Plus，用户可以通过阿里巴巴云模型工作室获取这些模型。

Qwen2.5 在多项基准测试中表现出色，涵盖了语言理解、推理、数学、编码、人类偏好对齐等多个领域。具体而言，开放权重旗舰模型 Qwen2.5-72B-Instruct 在多项测试中超越了众多开放和专有模型，并与当前最先进的开放权重模型 Llama-3-405B-Instruct 展开了激烈竞争，尽管后者规模是其 5 倍。此外，Qwen2.5-Turbo 和 Qwen2.5-Plus 在成本效益方面表现优异，在与 GPT-4o-mini 和 GPT-4o 的对比中展现了强大的竞争力。

作为基础模型，Qwen2.5 在训练专用模型如 Qwen2.5-Math、Qwen2.5-Coder、QwQ 和多模态模型方面发挥了关键作用，进一步拓展了其应用范围。

## Progressive Multimodal Reasoning via Active Retrieval
[通过主动检索实现渐进式多模态推理](https://arxiv.org/abs/2412.14835)

多步骤多模态推理任务对多模态大语言模型 (MLLMs) 提出了重大挑战，如何有效提升其在此类场景中的性能仍是一个未解难题。本文提出了 AR-MCTS，一个通用框架，通过主动检索 (AR) 和蒙特卡洛树搜索 (MCTS) 逐步增强 MLLMs 的推理能力。

我们的方法首先开发了一个统一的检索模块，从混合模态检索语料库中提取解决复杂推理问题的关键信息。为弥补自动化多模态推理验证的不足，我们结合 MCTS 算法与主动检索机制，实现了逐步注释的自动生成。该策略动态获取每一步推理所需的关键信息，超越了传统束搜索采样，提升了推理空间的多样性和可靠性。

此外，我们引入了一个逐步对齐的过程奖励模型，用于自动验证多模态推理任务。在三个复杂多模态推理基准上的实验结果表明，AR-MCTS 框架能有效提升多种多模态模型的性能。进一步分析显示，AR-MCTS 能优化采样多样性和准确性，从而实现可靠的多模态推理。

## MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval
[MegaPairs: 大规模数据合成用于通用多模态检索](https://arxiv.org/abs/2412.14475)

尽管多模态检索的需求日益增长，但其发展仍因训练数据不足而受到严重制约。本文提出 MegaPairs，一种创新的数据合成方法，结合视觉语言模型 (Vision Language Models, VLMs) 和开放域图像，生成大规模合成数据集。实证分析显示，MegaPairs 生成的数据质量高，使得多模态检索器在现有数据集上训练的基线模型基础上，显著提升了性能，而基线模型使用的数据量是其 70 倍。此外，MegaPairs 仅依赖通用图像语料库和开源 VLMs，易于扩展，从而持续提升检索性能。目前，我们已生成超过 2600 万训练实例，并基于此数据训练了多个不同规模的模型。这些模型在 4 个流行的组合图像检索 (Composed Image Retrieval, CIR) 基准上实现了最先进的零样本性能，并在 MMEB 提供的 36 个数据集上取得了最高整体性能。此外，这些模型在下游任务微调后，性能也有显著提升。我们生成的数据集、训练好的模型及数据合成流程将公开发布，以推动该领域未来发展。

## How to Synthesize Text Data without Model Collapse?
[如何合成文本数据而不发生模型崩溃？](https://arxiv.org/abs/2412.14689)

合成数据中的模型崩溃现象表明，在自生成数据上进行迭代训练会导致模型性能逐渐下降。随着 AI 模型的广泛应用，合成数据将深刻改变网络数据生态系统。未来的 GPT-{n} 模型将不可避免地使用合成数据与人类生成数据的混合进行训练。本文重点探讨两个问题：合成数据对语言模型训练的影响，以及如何避免模型崩溃的合成数据生成方法。我们首先在不同比例的合成数据上预训练语言模型，发现合成数据比例与模型性能呈负相关。随后，我们对合成数据进行统计分析，揭示了数据分布的偏移现象以及 n-gram 特征的过度集中。基于这些发现，我们提出对人类生成数据进行 Token 编辑，以生成半合成数据。作为概念验证，我们从理论上证明了 Token 级别的编辑能够防止模型崩溃，因为测试误差被限制在一个有限的范围内。我们通过广泛的实验，包括从头开始预训练、持续预训练和有监督的微调，验证了 Token 编辑能够提升数据质量并增强模型性能。

