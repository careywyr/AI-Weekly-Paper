## Unveiling Encoder-Free Vision-Language Models  
[无编码器视觉-语言模型的探索](https://arxiv.org/abs/2406.11832)  
当前的视觉-语言模型（VLMs）主要依赖视觉编码器提取视觉特征，并结合大语言模型（LLMs）执行视觉-语言任务。然而，视觉编码器在抽象视觉表示时引入的强烈归纳偏差，如分辨率、宽高比和语义先验，可能限制了VLMs的灵活性和效率。训练不包含视觉编码器的纯VLMs，即无缝接受视觉和语言输入，仍面临挑战且研究较少。实证研究表明，不使用编码器直接训练会导致收敛缓慢和性能差距大。本研究旨在弥合基于编码器与无编码器模型之间的差距，并提出一种简单有效的训练方案，以实现纯VLMs。具体而言，我们通过深入实验揭示了高效训练无编码器VLMs的关键要素：（1）在统一解码器内融合视觉-语言表示；（2）通过额外监督提升视觉识别能力。基于这些策略，我们开发了EVE，一种高效训练和前向传播的无编码器视觉-语言模型。值得注意的是，仅利用3500万公开数据，EVE在多个视觉-语言基准上与类似容量的基于编码器VLMs表现相当。它显著超越了训练过程神秘且数据未公开的Fuyu-8B模型。我们相信，EVE为跨模态开发纯解码器架构提供了一条透明且高效的路径。相关代码和模型已在以下公开发布：https://github.com/baaivision/EVE。  
  
## FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs  
[FunAudioLLM：面向人类与大语言模型自然交互的语音理解与生成基础模型](https://arxiv.org/abs/2407.04051)  
本报告介绍了 FunAudioLLM，这是一个旨在增强人类与大语言模型（LLMs）之间自然语音交互的模型家族。其核心包括两个创新模型：SenseVoice，用于多语言语音识别、情感识别及音频事件检测；以及 CosyVoice，能够生成多语言、可控音色、说话风格和说话者身份的自然语音。  
SenseVoice-Small 为 5 种语言提供极低延迟的自动语音识别（ASR），而 SenseVoice-Large 支持超过 50 种语言的高精度 ASR。CosyVoice 在多语言语音生成、零样本上下文学习、跨语言语音克隆及指令遵循方面表现卓越。  
与 SenseVoice 和 CosyVoice 相关的模型已在 Modelscope 和 Huggingface 上开源，相应的训练、推理和微调代码也已在 GitHub 发布。通过将这些模型与 LLMs 集成，FunAudioLLM 实现了语音到语音翻译、情感语音聊天、互动播客和富有表现力的有声书叙述等应用，从而推动了语音交互技术的发展。演示可在 https://fun-audio-llm.github.io 上查看，代码可在 https://github.com/FunAudioLLM 访问。  
  
## AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents
[AriGraph：为大语言模型智能体学习知识图谱世界模型与情景记忆](https://arxiv.org/abs/2407.04363)  
生成式 AI 的发展为大语言模型 (LLMs) 在自主智能体开发中的应用开辟了广阔的前景。实现真正的自主性不仅需要积累和更新从环境交互中获得的知识，还需有效利用这些知识。当前基于大语言模型的方法通过利用完整的观察历史、总结或检索增强来利用过去的经验。然而，这些非结构化的记忆表示并不利于复杂决策所需的推理和规划。在我们的研究中，我们引入了 AriGraph，这是一种新颖的方法，其中智能体在探索环境时构建了一个整合语义和情景记忆的记忆图。这种图结构促进了与智能体当前状态和目标相关的相互关联概念的高效联想检索，从而成为一个有效的环境模型，增强了智能体的探索和规划能力。我们证明，配备这种提出的记忆架构并增强规划和决策能力的 Ariadne LLM 智能体，在 TextWorld 环境中能够基于零样本学习有效处理复杂任务。我们的方法在各种任务中显著优于全历史、总结和检索增强生成等既定方法，包括第一届 TextWorld 问题竞赛中的烹饪挑战以及诸如房屋清洁和宝藏狩猎拼图等新任务。  
  
## MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?
[MJ-Bench: 你的多模态奖励模型真的是文本到图像生成的优秀评判者吗？](https://arxiv.org/abs/2407.04842)  
尽管像 DALLE-3 和 Stable Diffusion 这样的文本到图像模型正在迅速普及，但它们经常遇到诸如幻觉、偏见以及产生不安全、低质量输出等问题。为了有效解决这些问题，根据多模态评判者的反馈来调整这些模型以符合期望行为至关重要。尽管这些评判者的重要性显而易见，但目前的多模态评判者对其能力和局限性的评估往往不足，可能导致调整不当和安全微调结果不佳。为了解决这一问题，我们引入了 MJ-Bench，这是一个创新的基准，它包含一个全面的首选项数据集，用于从四个关键角度评估多模态评判者在为图像生成模型提供反馈方面的表现：对齐、安全、图像质量和偏见。具体而言，我们在我们的首选项数据集的每个分解子类别上评估了多种多模态评判者，包括较小的基于 CLIP 的评分模型、开源 VLM（例如 LLaVA 系列）和闭源 VLM（例如 GPT-4o、Claude 3）。实验表明，闭源 VLM 通常提供更好的反馈，其中 GPT-4o 在多个方面的平均表现上优于其他评判者。与开源 VLM 相比，较小的评分模型在文本-图像对齐和图像质量方面能提供更好的反馈，而 VLM 由于其更强的推理能力，在安全性和生成偏见方面提供更准确的反馈。进一步的研究表明，VLM 评判者通常能比数值尺度提供更准确和稳定的自然语言（Likert 尺度）反馈。值得注意的是，使用这些多模态评判者的独立反馈对端到端微调模型进行的人类评估得出了类似的结论，进一步证实了 MJ-Bench 的有效性。所有数据、代码、模型均可在 https://huggingface.co/MJ-Bench 获取。  
  
## LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages
[LLaMAX: 拓展大语言模型 (LLM) 的语言边界，通过增强翻译能力覆盖超过100种语言](https://arxiv.org/abs/2407.05975)  
大语言模型 (LLMs) 在资源丰富的语言任务中展现了卓越的翻译能力，但在资源匮乏的语言中，其性能受到预训练期间多语言数据不足的限制。为此，我们投入了35,000个A100-SXM4-80GB GPU计算小时，对LLaMA系列模型进行了广泛的多语言持续预训练，实现了对超过100种语言的翻译支持。通过全面分析训练策略，如词汇扩展和数据增强，我们开发了LLaMAX。值得注意的是，在不牺牲其泛化能力的前提下，LLaMAX相较于现有的开源大语言模型 (提升了超过10个spBLEU分数) 实现了显著更高的翻译性能，并在Flores-101基准测试中与专用翻译模型 (M2M-100-12B) 表现相当。广泛的实验表明，LLaMAX可以作为一个强大的多语言基础模型。代码和模型已公开发布，详见：代码 [链接](https://github.com/CONE-MT/LLaMAX/) 和 模型 [链接](https://huggingface.co/LLaMAX/)。  
  
## Vision language models are blind
[视觉语言模型的视觉盲区](https://arxiv.org/abs/2407.06581)  
具备视觉功能的大语言模型（VLMs），如 GPT-4o 和 Gemini 1.5 Pro，正广泛应用于众多图像与文本结合的应用中，并在多项视觉理解基准测试中表现优异。然而，我们发现这些 VLMs 在 7 项对人类而言极为简单的视觉任务上却表现不佳，包括判断（a）两个圆是否重叠；（b）两条线是否相交；（c）一个单词中哪个字母被圈出；以及（d）计算类似奥运标志中圆圈的数量。四款顶尖 VLMs 的这一令人震惊的低表现，表明它们的视觉能力至多如同近视者看细节模糊，最差则如同聪明的盲人仅凭猜测。相关代码可在此网址获取：https://vlmsareblind.github.io/  
  
## AgentInstruct: Toward Generative Teaching with Agentic Flows
[AgentInstruct：通过智能体流实现生成式教学](https://arxiv.org/abs/2407.03502)  
合成数据在加速语言模型（无论大小）的开发中扮演着越来越关键的角色。尽管合成数据的应用已取得多项成功，但研究人员对其可能导致的模型崩溃和模仿缺陷表示担忧。这种担忧源于合成数据在质量和多样性上的不一致。有效利用合成数据往往需要大量人力进行精心筛选。我们专注于利用合成数据进行后训练，特别是通过强大模型生成数据来传授新技能或行为给其他模型，我们将这一过程称为生成式教学。我们提出AgentInstruct，一个可扩展的智能体框架，用于自动生成大量多样且高质量的合成数据。AgentInstruct能够仅基于原始数据源（如文本文档和代码文件）生成提示和响应。我们通过创建一个包含2500万对的后训练数据集来验证AgentInstruct的效用，该数据集旨在提升语言模型在文本编辑、创意写作、工具使用、编程、阅读理解等多方面的能力。此数据集适用于任何基础模型的指令调优。我们使用这些数据对Mistral-7b进行了后训练。对比结果模型Orca-3与基于相同基础模型的Mistral-7b-Instruct，我们在多项基准测试中观察到显著提升，例如在AGIEval上提升了40%，在MMLU上提升了19%，在GSM8K上提升了54%，在BBH上提升了38%，在AlpacaEval上提升了45%。此外，Orca-3在性能上持续超越其他模型，如LLAMA-8B-instruct和GPT-3.5-turbo。  
  
## PaliGemma: A versatile 3B VLM for transfer
[PaliGemma: 一个多功能的3B VLM用于迁移](https://arxiv.org/abs/2407.07726)  
PaliGemma是一款基于SigLIP-So400m视觉编码器和Gemma-2B语言模型的开放式视觉-语言模型(VLM)。它经过训练，成为一个多功能且知识广博的基础模型，具有高效的迁移能力。PaliGemma在广泛的开放世界任务中展现出了卓越的性能。我们对其进行了近40个多样化的任务评估，涵盖了标准VLM基准测试，以及更为专业的任务，例如遥感(remote-sensing)和分割(segmentation)。  
  
## Inference Performance Optimization for Large Language Models on CPUs
[大语言模型在CPU上的推理性能优化](https://arxiv.org/abs/2407.07304)  
大语言模型（Large Language Models, LLMs）在多种任务中展现了卓越性能和巨大潜力。然而，如何在资源有限的环境中高效部署LLMs，已成为业界关注的焦点。在GPU硬件资源受限的情况下，探索CPU上的解决方案显得尤为重要。为减轻财务负担并突破硬件限制，推理性能的优化势在必行。本文提出了一种易于部署的推理性能优化方案，旨在加速CPU上的LLMs。该方案通过有效减少KV缓存大小并保持精度，实现了性能提升。我们进一步提出了一种基于oneAPI集体通信库的分布式推理优化方法，并对常用模型进行了针对性优化。相关代码已在[https://github.com/intel/xFasterTransformer](https://github.com/intel/xFasterTransformer)开源。  
  
## LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models
[LLaVA-NeXT-Interleave：在大规模多模态模型中处理多图像、视频和3D问题](https://arxiv.org/abs/2407.07895)  
视觉指令调优在提升大规模多模态模型（LMMs）的能力方面取得了重大进展。然而，现有的开放LMM主要集中在单图像任务上，其在多图像场景中的应用探索尚不充分。此外，先前的LMM研究分别处理不同场景，导致无法通过新涌现的能力实现跨场景泛化。为此，我们引入了LLaVA-NeXT-Interleave，该模型能够同时应对LMM中的多图像、多帧（视频）、多视角（3D）和多补丁（单图像）场景。为了实现这些能力，我们将交错数据格式视为通用模板，并编译了包含1,177.6k样本的M4-Instruct数据集，涵盖4个主要领域、14项任务和41个数据集。我们还精心设计了LLaVA-Interleave Bench，以全面评估LMM在多图像性能上的表现。通过广泛的实验，LLaVA-NeXT-Interleave在多图像、视频和3D基准测试中取得了领先成果，同时保持了单图像任务的性能。此外，我们的模型还展现了几种新兴能力，例如在不同设置和模态之间转移任务。代码可在https://github.com/LLaVA-VL/LLaVA-NeXT获取。  
  
本文探讨了提升大语言模型 (LLMs) 数学推理能力的潜在因素。我们认为，现代 LLMs 的数学推理能力数据缩放定律远未饱和，强调了模型质量随数据量增加而提升的现象。为此，我们引入了 Skywork-Math 模型系列，该系列在常见的 7B LLMs 上使用我们提出的 2.5M 实例 Skywork-MathQA 数据集进行监督微调 (SFT)。Skywork-Math 7B 在仅使用 SFT 数据的竞赛级 MATH 基准测试中达到了 51.2% 的准确率，在 GSM8K 基准测试中达到了 83.9% 的准确率，超越了早期版本的 GPT-4 在 MATH 上的表现。Skywork-Math 模型的优异性能得益于我们创新的两阶段数据合成和模型 SFT 流程，包括三种不同的增强方法和一个多样化的种子问题集，确保了 Skywork-MathQA 数据集在不同难度级别上的数量和质量。最关键的是，我们提供了几个实用的要点，以增强 LLMs 在研究和工业应用中的数学推理能力。  
  
我们已在构建基础视频扩散模型方面取得显著进展。鉴于这些模型采用大规模无监督数据训练，将其适应特定下游任务变得尤为关键。传统的适应方法涉及监督微调，这需要收集目标视频数据集，过程既具挑战性又繁琐。在本研究中，我们采用基于强大视觉判别模型上学习偏好而预训练的奖励模型，来适应视频扩散模型。这些奖励模型包含关于生成RGB像素的密集梯度信息，对在复杂视频搜索空间中实现高效学习至关重要。我们证实，通过将这些奖励模型的梯度反向传播至视频扩散模型，能够实现其在计算和样本效率上的高效对齐。我们在多种奖励模型和视频扩散模型上进行了实验，结果显示，相较于以往的无梯度方法，我们的方法在奖励查询和计算效率上更为高效。相关代码、模型权重及更多可视化内容已发布于https://vader-vid.github.io。  

## Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models -- The Story Goes On  
[Skywork-Math: 大语言模型数学推理的数据扩展法则 -- 故事继续](https://arxiv.org/abs/2407.08348)
本文探讨了可能提升大语言模型 (LLM) 数学推理能力的潜在因素。我们认为，现代大语言模型的数学推理能力尚未达到数据扩展的极限，强调了模型质量会随着数据量增加而不断提升。为支持这一观点，我们推出了Skywork-Math模型系列，这些模型在常见的7B LLM基础上使用包含250万个实例的Skywork-MathQA数据集进行有监督微调 (SFT)。Skywork-Math 7B在数学竞赛级别的MATH基准测试中取得了51.2%的出色准确率，在GSM8K基准测试中达到了83.9%的准确率，仅使用SFT数据，就超越了早期版本的GPT-4在MATH上的表现。Skywork-Math模型的优异表现得益于我们新颖的两阶段数据合成和模型SFT流水线，包括三种不同的增强方法和多样化的种子问题集，确保了Skywork-MathQA数据集在不同难度级别上的数量和质量。最重要的是，我们提供了一些实用建议，以提高LLM的数学推理能力，适用于研究和工业应用。
  
## Video Diffusion Alignment via Reward Gradients  
[通过奖励梯度进行视频扩散对齐](https://arxiv.org/abs/2407.08737)  
我们在构建基础视频扩散模型方面取得了重大进展。由于这些模型是使用大规模无监督数据训练的，适应这些模型以执行特定的下游任务变得至关重要。然而，通过有监督微调来适应这些模型需要收集目标视频数据集，这既具挑战性又繁琐。在这项工作中，我们利用在强大视觉判别模型基础上通过偏好学习的预训练奖励模型来适应视频扩散模型。这些模型包含对生成的RGB像素的密集梯度信息，这对于在复杂搜索空间（如视频中）进行高效学习至关重要。我们展示了通过将这些奖励模型的梯度反向传播到视频扩散模型，可以实现计算和样本高效的视频扩散模型对齐。我们的实验结果表明，我们的方法在奖励查询和计算方面比之前的无梯度方法更为高效。我们的代码、模型权重和更多可视化内容可在https://vader-vid.github.io获取。
  
## Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model
[多模态自指令：基于语言模型的抽象图像与视觉推理合成](https://arxiv.org/abs/2407.07053)  
  
尽管当前多数大型多模态模型 (LMMs) 已能理解自然场景和肖像照片，但对于抽象图像（如图表、地图）及视觉推理能力仍显不足。例如，在读取时钟时间、理解流程图或利用地图规划路线等日常任务中，这些模型往往表现不佳。为此，我们设计了一种多模态自指令方法，借助大型语言模型及其代码能力，在日常场景中合成大量抽象图像与视觉推理指令。  
  
我们的策略成功构建了一个包含 11,193 条指令的多模态基准，涵盖图表、表格、模拟地图、仪表盘、流程图、关系图、平面图及视觉谜题等八个视觉场景。该基准采用简单线条和几何元素构建，有效揭示了先进 LMMs 如 Claude-3.5-Sonnet 和 GPT-4o 在抽象图像理解、空间关系推理及视觉元素归纳方面的局限。  
  
此外，为验证合成数据质量，我们使用 62,476 条合成指令对一 LMM 进行微调。结果表明，图表理解和地图导航性能显著提升，并显示出在其他视觉推理任务中的潜在优势。项目代码已公开于：[GitHub 链接](https://github.com/zwq2018/Multi-modal-Self-instruct)。