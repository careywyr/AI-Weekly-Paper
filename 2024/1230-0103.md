## HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs
[HuatuoGPT-o1, 面向医学复杂推理的大语言模型](https://arxiv.org/abs/2412.18925)

OpenAI 的 GPT-4 突破突显了通过增强推理能力来改进大语言模型的潜力。然而，大多数关于推理的研究都集中在数学任务上，而像医学这样的领域则研究较少。尽管医学领域与数学不同，但由于医疗保健的高要求，它同样需要强大的推理能力来提供可靠的答案。然而，与数学不同，验证医学推理更具挑战性。为了解决这个问题，我们提出了可验证的医学推理问题，并使用医学验证器来检查模型输出的正确性。这种可验证性通过两阶段方法促进了医学推理的发展：(1) 使用验证器指导搜索复杂的推理轨迹以微调大语言模型，(2) 应用基于验证器奖励的强化学习 (RL) 来进一步增强复杂推理。最后，我们介绍了 HuatuoGPT-o1，这是一种能够进行复杂推理的医学大语言模型，仅使用 40K 个可验证问题就超越了通用和医学专用基准模型。实验表明，复杂推理提高了医学问题解决能力，并且从强化学习中获得更大的提升。我们希望我们的方法能够激发医学和其他专业领域推理的进步。

## 1.58-bit FLUX
[1.58 位 FLUX](https://arxiv.org/abs/2412.18653)

我们提出了 1.58 位 FLUX，这是首次成功量化最先进的文本到图像生成模型 FLUX.1-dev 的方法，使用 1.58 位权重（即值在 {-1, 0, +1} 中），同时保持生成 1024 x 1024 图像的可比性能。值得注意的是，我们的量化方法无需访问图像数据，仅依赖于 FLUX.1-dev 模型的自监督。此外，我们开发了一个针对 1.58 位操作优化的自定义内核，实现了模型存储降低 7.7 倍，推理内存降低 5.1 倍，并减少了推理延迟。在 GenEval 和 T2I Compbench 基准测试上的广泛评估表明，1.58 位 FLUX 在保持生成质量的同时显著提高了计算效率。

## Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey
[面向多模态智能的下一个 Token 预测：全面综述](https://arxiv.org/abs/2412.18619)

在自然语言处理中，语言建模的基础推动了下一个 Token 预测 (Next Token Prediction, NTP) 的发展，使其成为适用于多种模态机器学习任务的通用训练目标，并取得了显著的成功。随着大语言模型 (Large Language Models, LLMs) 的进步，文本模态中的理解和生成任务得以统一。最近的研究表明，来自不同模态的任务也可以有效地封装在 NTP 框架中，即将多模态信息转化为 Token，并在给定上下文的情况下预测下一个 Token。本文提出了一种全面的分类法，通过 NTP 的视角统一了多模态学习中的理解和生成任务。该分类法涵盖了五个关键方面：多模态 Token 化、MMNTP 模型架构、统一任务表示、数据集与评估以及开放挑战。这一分类法旨在帮助研究人员探索多模态智能。相关的 GitHub 仓库收集了最新的论文和代码库，可在 https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction 访问。

## Explanatory Instructions: Towards Unified Vision Tasks Understanding and Zero-shot Generalization
[解释性指令：迈向统一视觉任务理解和零样本泛化](https://arxiv.org/abs/2412.18525)

尽管计算机视觉 (CV) 已经借鉴了自然语言处理 (NLP) 中的许多重要成果，例如大型 Transformer 模型、广泛的预训练以及自回归范式等，但它尚未像 NLP 那样实现零样本任务的泛化能力。在本文中，我们提出了一种观点，即 CV 领域中的离散化和术语化任务定义（例如“图像分割”）可能是阻碍零样本任务泛化的关键因素。我们假设，由于这些术语化定义的存在，深度模型无法真正理解先前见过的任务，从而导致其在面对新任务时难以泛化。为了验证这一假设，我们引入了“解释性指令”，它通过从输入图像到输出的详细语言转换，提供了一种直观的方式来定义 CV 任务目标。我们构建了一个包含 1200 万“图像输入-解释性指令-输出”三元组的大规模数据集，并训练了一个基于自回归的视觉语言模型 (AR-based VLM)，该模型能够同时处理图像和解释性指令。通过学习这些指令，基于自回归的视觉语言模型在已见过的任务上实现了指令级别的零样本能力，并在未见过的 CV 任务上展现了显著的零样本泛化能力。代码和数据集将在我们的 GitHub 仓库中公开提供。

## On the Compositional Generalization of Multimodal LLMs for Medical Imaging
[多模态大语言模型在医学影像中的组合泛化能力研究](https://arxiv.org/abs/2412.20070)

多模态大语言模型 (MLLMs) 在医学领域展现出巨大的潜力，但其能力往往受到某些医学领域数据不足的限制。因此，理解哪些类型的图像可以被 MLLMs 用于泛化显得尤为重要。当前研究表明，多任务训练优于单任务训练，因为不同任务之间可以相互促进。然而，这些研究往往忽略了任务间的内在关联，导致在选择数据集以增强特定任务时提供的指导有限。为分析这一现象，我们尝试采用组合泛化 (CG) 作为指导框架。组合泛化是指模型通过重新组合已学元素来理解新组合的能力。由于医学图像可以通过模态、解剖区域和任务来精确定义，这为探索 CG 提供了一个天然的环境。为此，我们整合了 106 个医学数据集，构建了 Med-MAT 以进行全面的实验。实验结果表明，MLLMs 能够利用 CG 来理解未见过的医学图像，并且 CG 是多任务训练中泛化能力的主要驱动力之一。此外，进一步研究表明，CG 能够有效支持数据有限的数据集，并在不同的骨干网络中表现出稳定的性能，展现了其多功能性和广泛的适用性。Med-MAT 已在 https://github.com/FreedomIntelligence/Med-MAT 公开提供。

## Bringing Objects to Life: 4D generation from 3D objects
[从 3D 物体生成 4D 内容：赋予物体动态生命](https://arxiv.org/abs/2412.20422)

生成建模领域的最新进展使得通过文本提示控制生成 4D 内容（即动态 3D 物体）成为可能。4D 生成在虚拟世界、媒体和游戏等领域具有广阔的应用前景，但现有方法对生成内容的外观和几何形状的控制较为有限。本文提出了一种基于文本提示的 3D 物体动画生成方法，能够在保持原始物体身份的同时生成自定义动画。我们首先将 3D 网格转换为静态 4D 神经辐射场（NeRF），以保留输入物体的视觉属性。随后，利用文本驱动的图像到视频扩散模型对物体进行动画化。为了提升运动的真实感，我们引入了增量视角选择策略，用于采样视角以增强运动的逼真性，并结合掩码分数蒸馏采样（SDS）损失，通过注意力图将优化集中在相关区域。我们在时间一致性、提示遵循和视觉保真度等方面对模型进行了评估，结果表明，相较于其他方法，我们的方法在身份保留（通过 LPIPS 分数衡量）方面提升了三倍，并在视觉质量与动态内容之间实现了有效平衡。

## Efficiently Serving LLM Reasoning Programs with Certaindex
[使用 Certaindex 高效服务大语言模型推理程序](https://arxiv.org/abs/2412.20993)

大语言模型 (LLMs) 的快速发展已经释放了它们在高级推理任务中的能力，如数学问题解决、代码生成和法律分析。这一进展的核心是推理时推理算法，这些算法通过探索多个解决方案路径来优化输出，但同时也增加了计算需求和响应延迟。现有的服务系统无法适应这些算法的扩展行为或查询的不同难度，导致资源使用效率低下和未达到的延迟目标。
  我们提出了 Dynasor，一个优化大语言模型推理查询的推理时计算的系统。与传统引擎不同，Dynasor 跟踪并调度推理查询中的请求，并使用 Certaindex，一个基于模型确定性测量统计推理进度的代理，来动态指导计算分配。Dynasor 将调度与推理进度动态协调：它为困难查询分配更多计算，减少简单查询的计算，并提前终止无望的查询，平衡准确性、延迟和成本。在多样化的数据集和算法上，Dynasor 在批处理中减少了高达 50% 的计算，并在在线服务中维持了 3.3 倍的查询率或 4.7 倍的延迟 SLOs。

## Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs
[不要为 2+3=? 过度思考：关于类似 o1 的大语言模型的过度思考问题](https://arxiv.org/abs/2412.21187)

OpenAI o1 等模型的卓越性能可以归因于它们在推理过程中模拟人类长时间思考的能力。这些模型采用了扩展的思维链过程，探索多种策略以增强问题解决能力。然而，一个关键问题仍然存在：如何在测试过程中智能且高效地扩展计算资源。本文首次对这些模型中普遍存在的过度思考问题进行了全面研究，其中过多的计算资源被分配给简单问题，而收益甚微。我们从结果和过程两个角度引入了新的效率指标，以评估类似 o1 的模型对计算资源的合理使用。通过自训练范式，我们提出了缓解过度思考的策略，优化推理过程而不影响准确性。实验结果表明，我们的方法成功减少了计算开销，同时在一系列难度不同的测试集（如 GSM8K、MATH500、GPQA 和 AIME）上保持了模型性能。

## OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis
[OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis](https://arxiv.org/abs/2412.19723)

基于视觉-语言模型 (Vision-Language Models, VLMs) 的图形用户界面 (GUI) 智能体已展现出接近人类的计算机控制能力。尽管这些智能体在推动数字化自动化方面具有重要价值，但一个关键瓶颈仍然存在：如何高效收集用于训练的高质量轨迹数据。目前，常见的做法是通过人工监督或执行预定义任务来生成合成数据，但这些方法要么成本高昂，要么难以保证数据质量。此外，现有方法还存在数据多样性不足以及合成数据与现实环境差异较大的问题。为解决这些问题，我们提出了 OS-Genesis，这是一种创新的 GUI 数据合成流程，它颠覆了传统的轨迹收集方式。OS-Genesis 不依赖预定义任务，而是让智能体先感知环境并进行逐步交互，随后逆向推导出高质量任务，从而实现轨迹级别的探索。为了确保生成的轨迹质量，我们还引入了轨迹奖励模型。实验表明，使用 OS-Genesis 训练的 GUI 智能体在极具挑战的在线基准测试中表现显著提升。进一步的分析验证了 OS-Genesis 的高效性，并证明其在数据质量和多样性方面优于现有合成方法。我们的代码、数据和模型检查点可在 https://qiushisun.github.io/OS-Genesis-Home/{OS-Genesis 主页} 获取。

## 2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining
[2.5 年课堂：用于视觉-语言预训练的多模态教科书](https://arxiv.org/abs/2501.00958)
与传统的图像-文本对数据相比，交错语料库能够使视觉-语言模型（VLMs）更自然地理解世界，类似于人类的学习方式。然而，现有的此类数据集大多是从网页中爬取的，存在知识密度低、图像-文本关系松散以及图像之间逻辑连贯性差等问题。另一方面，互联网上存在大量教学视频（例如在线几何课程），这些视频被广泛用于人类学习基础学科，但这些宝贵的资源在 VLM 训练中仍未得到充分利用。本文提出了一种高质量的多模态教科书语料库，为 VLM 预训练提供了更丰富的基础知识。该语料库收集了超过 2.5 年的教学视频，总计 22,000 课时。我们首先使用大语言模型（LLM）提出的分类法系统地收集教学视频，然后逐步从视频中提取并精炼视觉（关键帧）、音频（ASR）和文本知识（OCR），并根据时间顺序将其组织成图像-文本交错语料库。与现有数据集相比，我们以视频为中心的教科书提供了更连贯的上下文、更丰富的知识以及更好的图像-文本对齐。实验结果表明，该语料库在预训练中表现出色，特别是在 ScienceQA 和 MathVista 等知识和推理密集型任务中。此外，基于该教科书预训练的 VLMs 表现出卓越的交错上下文意识，能够在少样本上下文中利用视觉和文本线索进行任务解决。我们的代码可在 \url{https://github.com/DAMO-NLP-SG/multimodal_textbook} 获取。

## VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control
[VideoAnydoor: 高保真视频对象插入与精确运动控制](https://arxiv.org/abs/2501.01427)

尽管视频生成技术取得了显著进展，但将给定对象插入视频中仍然是一项具有挑战性的任务。难点在于同时保留参考对象的外观细节并准确建模其连贯运动。在本文中，我们提出了 VideoAnydoor，一个具有高保真细节保留和精确运动控制的零样本视频对象插入框架。基于文本到视频的模型，我们利用一个 ID 提取器来注入全局身份信息，并通过一个框序列来控制整体运动。为了保留详细的外观并支持细粒度的运动控制，我们设计了一个像素变形模块。该模块以带有任意关键点的参考图像和相应的关键点轨迹作为输入，根据轨迹对像素细节进行变形，并将变形后的特征与扩散 U-Net 的特征融合，从而改善细节保留并支持用户操纵运动轨迹。此外，我们提出了一种结合视频和静态图像的训练策略，通过重新加权重建损失来提升插入质量。VideoAnydoor 在现有方法中表现出显著优势，并自然支持各种下游应用（例如，头部生成、视频虚拟试穿、多区域编辑），而无需进行任务特定的微调。

## CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings
[CodeElo: 基于人类可比 Elo 评分的大语言模型竞赛级代码生成基准测试](https://arxiv.org/abs/2501.01257)

随着现有大语言模型 (LLMs) 代码推理能力的不断增强，以及 OpenAI o1 和 o3 等推理模型的突破，开发更具挑战性和全面性的基准测试以有效评估其复杂的竞赛级编码能力的需求日益增长。现有的基准测试，如 LiveCodeBench 和 USACO，由于缺乏私有测试用例、不支持特殊评判以及执行环境不匹配等问题，存在明显不足。为了弥补这一差距，我们提出了 CodeElo，这是一个标准化的竞赛级代码生成基准测试，首次有效解决了上述所有挑战。CodeElo 基准测试主要基于官方的 CodeForces 平台，并尽可能与该平台保持一致。我们收集了 CodeForces 上最近六个月的竞赛题目，包括竞赛分区、题目难度评级和题目算法标签等详细信息。我们引入了一种独特的评估方法，将题目直接提交到平台，并开发了一个可靠的 Elo 评分计算系统，该系统与平台保持一致，并且与人类参与者具有可比性，但方差较低。通过在 CodeElo 上进行测试，我们首次提供了 30 个现有流行的开源模型和 3 个专有模型的 Elo 评分。结果显示，o1-mini 和 QwQ-32B-Preview 表现突出，分别获得了 1578 和 1261 的 Elo 评分，而其他模型即使在最简单的问题上也表现不佳，位于所有人类参与者的最低 20%。此外，我们还进行了详细的分析实验，以提供跨算法性能的深入分析以及使用 C++ 和 Python 的比较，这为未来的研究提供了方向。

## VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM
[VideoRefer Suite: 通过视频大语言模型推进时空对象理解](https://arxiv.org/abs/2501.00599)

视频大语言模型 (Video Large Language Models) 最近在通用视频理解方面展示了显著的能力。然而，它们主要关注整体理解，难以有效捕捉细粒度的空间和时间细节。此外，缺乏高质量的对象级视频指令数据和全面的基准进一步阻碍了它们的进展。为了应对这些挑战，我们引入了 VideoRefer Suite，以增强视频大语言模型在更细粒度时空视频理解方面的能力，具体来说，在整个视频中对任何对象进行感知和推理。具体而言，我们在三个关键方面全面开发了 VideoRefer Suite：数据集、模型和基准。首先，我们引入了一个多智能体数据引擎，精心构建了一个大规模、高质量的对象级视频指令数据集，称为 VideoRefer-700K。接下来，我们提出了 VideoRefer 模型，该模型集成了一个多功能时空对象编码器，用于捕捉精确的区域和序列表示。最后，我们精心创建了 VideoRefer-Bench，以全面评估视频大语言模型的时空理解能力，从多个维度进行评估。大量实验和分析结果表明，我们的 VideoRefer 模型不仅在视频引用基准上取得了有希望的性能，还促进了通用视频理解能力。

## Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models
[重建 vs. 生成：在潜在扩散模型中驯服优化困境](https://arxiv.org/abs/2501.01423)

采用 Transformer 架构的潜在扩散模型在生成高保真图像方面表现优异。然而，最近的研究指出了这种两阶段设计中的优化困境：虽然在视觉 Tokenizer 中增加每个 Token 的特征维度可以提高重建质量，但它需要更大的扩散模型和更多的训练迭代才能达到相当的生成性能。因此，现有系统通常只能选择次优的解决方案，要么由于 Tokenizer 内的信息丢失而产生视觉伪影，要么由于昂贵的计算成本而无法完全收敛。我们提出，这种困境源于学习无约束高维潜在空间的固有困难。为了解决这个问题，我们提出在训练视觉 Tokenizer 时，将潜在空间与预训练的视觉基础模型对齐。我们提出的 VA-VAE（视觉基础模型对齐变分自编码器）显著提升了潜在扩散模型的重建-生成边界，使得 Diffusion Transformer (DiT) 在高维潜在空间中更快收敛。为了充分发挥 VA-VAE 的潜力，我们构建了一个增强的 DiT 基线，改进了训练策略和架构设计，称为 LightningDiT。集成系统在 ImageNet 256x256 生成上达到了最先进的 (SOTA) 性能，FID 得分为 1.35，同时在仅 64 个 epoch 内达到了 2.11 的 FID 得分，展示了显著的训练效率——与原始 DiT 相比，收敛速度提升了 21 倍以上。模型和代码可在以下网址获取：
https://github.com/hustvl/LightningDiT。

