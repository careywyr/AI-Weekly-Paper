# Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences
[直接纳什优化：教授语言模型通过通用偏好自我提升](https://arxiv.org/abs/2404.03715)

本文研究如何利用强大神谕的偏好反馈，对大语言模型 (大语言模型) 进行后训练，以帮助模型迭代地自我改进。传统的大语言模型后训练方法采用基于人类反馈的强化学习 (强化学习) ，通常将奖励学习与策略优化分开处理。然而，这种基于奖励最大化的方法受到“点对点”奖励的限制（如 Bradley-Terry 模型所示），无法表达复杂的非传递性或循环偏好关系。尽管在强化学习领域的进展表明奖励学习与策略优化可以整合为一个稳定的对比目标，但这种方法仍然局限于奖励最大化的框架内。近期的研究动向开始摒弃奖励最大化的前提，转而直接针对“成对”或通用偏好进行优化。在本文中，我们引入了直接纳什优化 (直接纳什优化) ，这是一种经过验证的可扩展算法，它将对比学习的简单性和稳定性与优化通用偏好的理论普遍性相结合。由于直接纳什优化采用基于回归的目标的批处理在线策略，使得其实施直接且高效。此外，直接纳什优化在迭代过程中实现了单调的性能提升，有助于其超越如 GPT-4 这样的强大教师。在我们的实验中，通过直接纳什优化调整的 7B 参数的 Orca-2.5 模型，在 AlpacaEval 2.0 中以 33% 的胜率对抗 GPT-4-Turbo，即使控制了响应长度后，也比起始模型高出了 26% (从 7% 提升至 33%)。它的表现超过了参数更多的模型，如 Mistral Large、自我奖励 LM (70B 参数) 以及较旧版本的 GPT-4。

# No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance
[没有“零样本”就没有指数级数据：预训练概念频率决定多模态模型性能](https://arxiv.org/abs/2404.04125)

Web 爬取的预训练数据集是多模态模型，如 CLIP 用于分类/检索和 Stable-Diffusion 用于图像生成，在“零样本”评估性能上取得印象深刻成绩的基础。然而，这些多模态模型的“零样本”泛化能力是否具有实际意义尚未明确，因为尚不清楚它们的预训练数据集在多大程度上覆盖了“零样本”评估期间针对的下游概念。在这项研究中，我们探讨了多模态模型在下游概念上的表现如何受到这些概念在预训练数据集中频率的影响。我们在 34 个模型和五个标准预训练数据集 (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics) 上进行了全面的调查，共生成了超过 300GB 的数据。我们的发现表明，远离“零样本”泛化的表现，多模态模型需要指数级的数据增长才能在“零样本”性能上实现线性的提升，遵循样本效率低下的对数线性规模趋势。即使在控制预训练和下游数据集之间样本级相似性，并在纯合成数据分布上进行测试的情况下，这一趋势仍然存在。此外，我们根据我们的分析对长尾数据进行了采样，并以此对模型进行了基准测试，发现这些多模态模型普遍表现不佳。我们将这一长尾测试集作为“Let it Wag!”基准，以推动此方向的进一步研究。总的来说，我们的研究揭示了对训练数据的指数级需求，表明在大规模训练范式下实现“零样本”泛化能力的关键还有待发现。

# AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent
[AutoWebGLM：自举并加强基于大语言模型的 Web 导航智能体](https://arxiv.org/abs/2404.03648)

大语言模型已推动了许多智能体任务，如 web 导航，但大多数现有智能体在处理真实网页时表现不尽如人意，主要是由于以下三个因素：(1) 网页操作的多样性，(2) HTML 文本超出模型处理能力，(3) 由于 web 领域的开放性，决策复杂性增加。针对这些挑战，我们开发了 AutoWebGLM，这是一个性能超越 GPT-4 的自动化 web 导航智能体，基于 ChatGLM3-6B。受人类浏览模式的启发，我们设计了一种 HTML 简化算法，以简洁方式保留网页的关键信息。我们采用人机合作方式为课程训练构建 web 浏览数据。通过强化学习和拒绝采样，我们启动了该模型，以进一步促进对网页的理解、浏览器操作和任务的有效分解。为测试目的，我们建立了一个双语基准——AutoWebBench——用于评估真实世界的 web 浏览任务。我们在多个 web 导航基准测试中评估了 AutoWebGLM，显示出其在处理真实环境时的改进，但也揭示了需要解决的挑战。相关代码、模型和数据将在以下网址发布：https://github.com/THUDM/AutoWebGLM。

# Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs
[Ferret-UI：用多模态大语言模型理解移动 UI 的实践](https://arxiv.org/abs/2404.05719)

近年来，多模态大语言模型的进步显著，但这些模型在理解和有效互动移动用户界面方面仍有不足。本文介绍了 Ferret-UI，这是一种专为增强移动 UI 屏幕理解而设计的多模态大语言模型，具备指称、定位和推理功能。考虑到 UI 屏幕通常具有更长的宽高比并且包含比自然图像更小的对象（如图标、文字），我们在 Ferret 基础上增加了“任意分辨率”功能，以放大细节并利用增强的视觉特征。我们将每个屏幕根据其原始宽高比分为两个子图像，横屏则垂直分割，竖屏则水平分割。在发送给大语言模型之前，这两个子图像分别进行编码。我们从各种基本 UI 任务中收集了大量训练样本，这些任务包括图标识别、文本查找和小部件列表，这些样本已格式化以便精确地进行指称和定位。为了增强模型的推理能力，我们还编制了一套高级任务数据集，包括详细描述、感知/交互对话和功能推断。在经过精心策划的数据集训练后，Ferret-UI 不仅能够出色地理解 UI 屏幕，还能执行开放式指令。我们为模型评估建立了一个全面的基准，涵盖了所有上述任务。在所有基本 UI 任务中，Ferret-UI 的表现不仅超过了大多数开源的多模态大语言模型，还超过了 GPT-4V。

# ByteEdit: Boost, Comply and Accelerate Generative Image Editing
[ByteEdit：增强、遵从和加速生成性图像编辑](https://arxiv.org/abs/2404.04860)

近期，基于扩散的生成图像编辑技术的进步引发了深刻的变革，重塑了图像外延绘制和内填充任务的格局。尽管取得了这些进展，该领域仍面临若干固有挑战，包括：i) 质量较低；ii) 一致性不佳；iii) 指令遵从性不足；iv) 生成效率不佳。为了克服这些难题，我们提出了 ByteEdit，一个创新的反馈学习框架，旨在增强、遵从和加速生成图像编辑任务。ByteEdit 无缝集成了专门用于提升美感和图像-文本对齐的图像奖励模型，同时引入了一个旨在增强输出一致性的密集、像素级奖励模型。此外，我们还提出了一种先进的对抗性和渐进式反馈学习策略，用以加快模型的推理速度。通过广泛的大规模用户评估，我们证明 ByteEdit 在生成质量和一致性方面均超越了 Adobe、Canva 和美图等领先的生成图像编辑产品。相比基线模型，ByteEdit 在外延绘制任务中的质量和一致性分别提高了 388% 和 135%。实验还证明了我们的加速模型在保持出色的质量和一致性方面的卓越性能。

# OmniFusion Technical Report
[OmniFusion 技术报告](https://arxiv.org/abs/2404.06212)

去年，多模态架构在 AI 领域掀起了一场革命，极大地扩展了大语言模型 (Large Language Model, LLM) 的功能。我们提出了基于预训练大语言模型和视觉适配器的 OmniFusion 模型。我们评估并比较了多种架构设计原则，以更好地整合文本和视觉数据：MLP 和 Transformer 适配器、各种基于 CLIP ViT 的编码器（如 SigLIP、InternVIT 等）及其融合方式、图像编码方式（整图或切片编码）以及两种 7B 参数的大语言模型（一种专有的和一种开源的 Mistral）。在 8 个视觉-语言基准测试中，OmniFusion 在各种视觉问答 (VQA) 任务中表现最佳，超过了开源的 LLaVA 类似解决方案：VizWiz、Pope、MM-Vet、ScienceQA、MMBench、TextVQA、VQAv2、MMMU。我们还展示了 OmniFusion 在不同领域提供详细答案的多种场景，如家政、观光、文化、医学、手写和扫描方程的识别等。基于 Mistral 的 OmniFusion 模型是开源的，相关的权重、训练和推理脚本可在 https://github.com/AIRI-Institute/OmniFusion 获取。

# LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders
[LLM2Vec：大语言模型秘密地是强大的文本编码器](https://arxiv.org/abs/2404.05961)

当前，仅解码器的大语言模型 (Large Language Model, LLM) 是大多数今日 NLP 任务和基准测试中的先进模型。尽管如此，社区在采用这些模型进行需要丰富上下文表达的文本嵌入任务时仍较为缓慢。在本研究中，我们介绍了 LLM2Vec——一种简单的非监督方法，能将任何仅解码器的大语言模型转变为强大的文本编码器。LLM2Vec 包括三个简单步骤：1) 启用双向注意力；2) 掩蔽下一个 Token 的预测；3) 非监督对比学习。我们通过将其应用于从 1.3B 至 7B 参数的三个流行的大语言模型，并在英语单词级和序列级任务上进行评估，证明了 LLM2Vec 的有效性。我们在单词级任务上大幅超过了仅编码器模型，在 Massive Text Embeddings Benchmark 上达到了新的非监督最先进水平。此外，当我们将 LLM2Vec 与监督对比学习结合使用时，在仅利用公开数据训练的模型中，我们在 Massive Text Embeddings Benchmark 上实现了最先进的表现。我们的实证结果和广泛的分析表明，大语言模型可以在无需昂贵适应性改造或合成 GPT-4 数据的情况下，以参数效率高的方式有效转变为通用文本编码器。

# InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD
[InternLM-XComposer2-4KHD：一个开创性的大视觉-语言模型，处理从 336 像素到 4K HD 的分辨率](https://arxiv.org/abs/2404.06512)

大视觉-语言模型 (Large Vision-Language Model, LVLM) 领域已取得显著进展，但其理解细粒度视觉内容的能力因分辨率限制而受阻。最近的努力旨在提高 LVLM 的高分辨率理解能力，但目前这些能力最多只能达到大约 1500 x 1500 像素，并且分辨率范围相对狭窄。本文介绍的 InternLM-XComposer2-4KHD 是一项开创性的研究，旨在将 LVLM 的分辨率能力提升至 4K HD (3840 x 1600) 甚至更高。同时，考虑到在所有场景中可能不需要超高分辨率，该模型支持从 336 像素到 4K 标准的广泛分辨率范围，显著扩展了其应用范围。具体而言，该研究通过引入动态分辨率和自动补丁配置的新颖扩展，推进了补丁划分范式。在自动变化补丁数量和配置布局的同时，保持了训练图像的宽高比，从而实现了从 336 像素到 4K 标准的动态训练分辨率。我们的研究表明，将训练分辨率提升至 4K HD 可以带来持续的性能提升，而不会达到潜在改进的上限。InternLM-XComposer2-4KHD 在 16 个基准中的 10 个上展示了卓越的能力，匹敌甚至超过了 GPT-4V 和 Gemini Pro。InternLM-XComposer2-4KHD 模型系列的 7B 参数版本已在 https://github.com/InternLM/InternLM-XComposer 公开。

# Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence
[Eagle and Finch：RWKV 采用矩阵值状态和动态递归](https://arxiv.org/abs/2404.05892)

我们介绍了 Eagle (RWKV-5) 和 Finch (RWKV-6)，这是在 RWKV (RWKV-4) 架构基础上的改进型序列模型。我们的架构设计进步包括引入多头矩阵值状态 (multi-headed matrix-valued states) 和动态递归机制 (dynamic recurrence mechanism)，不仅提升了模型的表达能力，还保留了RNN的高效推理特性。此外，我们还创建了一个新的多语言语料库，包含 1.12 万亿 Token，并开发了一个基于贪心匹配的快速分词器 (fast tokenizer based on greedy matching)，进一步增强了模型的多语言处理能力。我们训练了四个规模不一的 Eagle 模型，参数从 0.46 亿到 7.5 亿不等，以及两个 Finch 模型，参数分别为 1.6 亿和 3.1 亿，均在各种性能基准测试中表现出色。所有模型均已在 HuggingFace 上以 Apache 2.0 许可证形式发布。模型可在以下链接获取：https://huggingface.co/RWKV，相关训练、推理及时序并行训练的代码分别可在以下链接找到：https://github.com/RWKV/RWKV-LM，https://github.com/RWKV/ChatRWKV，https://github.com/RWKV/RWKV-infctx-trainer。

# Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention
[不留下任何上下文：高效的无限上下文 Transformer 与 Infini-attention](https://arxiv.org/abs/2404.07143)

本研究提出了一种高效方法，可以使基于 Transformer 的大语言模型 (LLM) 处理具有有界内存和计算的无限长输入。我们提出的关键技术是 Infini-attention，这一新型注意力技术将压缩内存融合进传统的注意力机制，并在单一的 Transformer 块中实现了遮掩的局部注意力和长期线性注意力机制。我们在处理长序列语言建模、1M 序列长度的 passkey 上下文块检索以及 500K 字符书籍摘要任务等方面验证了此方法的有效性，模型规模包括 1B 和 8B。我们的方法仅引入最少的有界内存参数，有效支持了 LLM 的快速流式推理。

# RULER: What's the Real Context Size of Your Long-Context Language Models?
[RULER: 你的长上下文语言模型的实际上下文大小是多少？](https://arxiv.org/abs/2404.06654)

针-草堆 (NIAH) 测试，主要检测从长干扰文本中检索特定信息 (即“针”) 的能力，已广泛用于评估长上下文语言模型。但这种基于简单检索的测试仅能反映出长上下文理解的表层能力。为了全面评价长上下文语言模型，我们开发了一个具有灵活配置的新合成基准 RULER，可自定义序列长度及任务复杂度。RULER 在原有 NIAH 测试基础上增加了多样化的“针”类型和数量的变体。此外，RULER 还新增了多跳跟踪和聚合等任务类别，用于检验模型超出上下文搜索的行为。我们使用 RULER 对十种长上下文语言模型进行了包含 13 个代表性任务的评估。尽管这些模型在原始 NIAH 测试中表现近乎完美，但随着上下文长度增加，所有模型的性能均显著下降。尽管这些模型声称支持 32K Token 或更多的上下文大小，但只有四种模型（GPT-4, Command-R, Yi-34B, 和 Mixtral）能在 32K Token 长度下保持较好性能。我们对支持 200K Token 上下文长度的 Yi-34B 进行的分析显示，随着输入长度和任务复杂度的增加，仍有很大的提升空间。我们已将 RULER 开源，以促进长上下文语言模型的综合评估。

# Rho-1: Not All Tokens Are What You Need
[Rho-1: 并非所有 Token 都是你需要的](https://arxiv.org/abs/2404.07965)

之前的语言模型预训练方法通常对所有训练 Token 应用下一个 Token 的预测损失。对此常规做法提出挑战，我们认为“语料库中的 Token 并非对训练都同等重要”。我们的初步分析深入了解了语言模型的 Token 级训练动态，发现不同 Token 存在不同的损失模式。借助这些洞察，我们推出了一种新型语言模型 Rho-1。与传统的 LMs 不同，Rho-1 采用选择性语言模型 (SLM) 策略，只针对与期望分布一致的有用 Token 进行训练。此方法涉及用参考模型对预训练 Token 进行评分，然后针对那些高额外损失的 Token 进行集中训练。在 15B OpenWebMath 语料库上继续预训练后，Rho-1 在 9 个数学任务中的少样本精度提高了高达 30%，经过微调，Rho-1-1B 和 7B 在 MATH 数据集上分别达到了 40.6% 和 51.8% 的顶级成绩，仅使用了 3% 的预训练 Token。此外，当在 80B 通用 Token 上预训练时，Rho-1 在 15 个多样化任务中平均提升了 6.8%，有效提高了语言模型预训练的效率和表现。

# ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback
[ControlNet++: 通过有效的一致性反馈提高条件控制性能](https://arxiv.org/abs/2404.07987)

为了提升文本至图像扩散模型的控制能力，现有的方法如 ControlNet 引入了基于图像的条件控制。我们在本文中发现，现有方法在生成与图像条件控制相符的图像方面仍面临重大挑战。针对这一问题，我们提出了 ControlNet++，这是一种新方法，通过显式优化生成图像与条件控制间的像素级循环一致性，来改善可控制的生成过程。具体来说，对于给定的条件控制，我们使用预训练的判别奖励模型提取生成图像的相应条件，并优化输入条件控制与提取条件之间的一致性损失。直接从随机噪声生成图像然后计算一致性损失的方法虽直观，但需要存储多个采样步骤的梯度，这导致了巨大的时间和内存成本。为此，我们引入了一种高效的奖励策略，该策略通过添加噪声有意地扰乱输入图像，并利用单步去噪后的图像进行奖励的微调。这样做避免了图像采样相关的高昂成本，实现了奖励微调的高效性。广泛的实验显示，ControlNet++ 在各种条件控制下显著提升了控制性能。例如，与 ControlNet 相比，在分割掩膜、线条边缘和深度条件下分别提高了 7.9% 的 mIoU、13.4% 的 SSIM 和 7.6% 的 RMSE。

# OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments
[OSWorld: 在真实计算机环境中为开放式任务基准测试多模态智能体](https://arxiv.org/abs/2404.07972)

能够在极少人工干预下执行复杂计算机任务的自主代理 (autonomous agents) 有可能彻底改变人机交互方式，极大提升可访问性和生产力。然而，现有的评估标准要么缺乏交互环境，要么局限于特定应用或领域的环境，未能真实反映计算机在现实世界中的多样化和复杂性应用，这限制了任务的广泛性和智能体的扩展性。为了解决这一问题，我们推出了 OSWorld，这是第一个专为多模态代理设计的、可扩展的真实计算机环境，支持跨 Ubuntu、Windows 和 macOS 等多种操作系统进行任务配置、基于执行的评估以及交互式学习。OSWorld 可作为一个统一、集成的计算机环境，用于评估涉及任意应用的开放式计算机任务。基于 OSWorld，我们建立了一个涵盖 369 个涉及实际网络与桌面应用、操作系统文件输入/输出及跨多应用的工作流的计算机任务基准。每个任务示例都源于真实的计算机使用案例，包括详细的初始状态设定和定制的基于执行的评估脚本，确保评估的可靠性和可重复性。在 OSWorld 上对最新的大语言模型/视觉语言模型 (LLM/VLM) 基智能体进行的广泛评估揭示了它们在充当计算机助理方面的显著不足。尽管人类能完成超过 72.36% 的任务，但最优模型的成功率仅为 12.24%，主要是在图形用户界面 (GUI) 的精确操作和操作知识方面遇到困难。通过 OSWorld 进行的深入分析为开发多模态通用代理提供了以往评估无法达到的宝贵洞察。我们的代码、环境、基线模型和数据已在 https://os-world.github.io 上公开。

# RecurrentGemma: Moving Past Transformers for Efficient Open Language Models
[RecurrentGemma: 为高效的开放语言模型超越 Transformer 架构](https://arxiv.org/abs/2404.07839)

我们推出了 RecurrentGemma，这是一款采用 Google 最新的 Griffin 架构的开放语言模型。Griffin 结合线性递归和局部注意力机制，显著提升了语言处理性能。该模型具备固定的状态大小，有效降低了内存占用，并提升了对长序列的高效推理能力。我们提供了一款预训练模型，包含 2B 非嵌入式参数，并提供了经过指令调优的变体。这两款模型在较少 token 的训练下，仍与 Gemma-2

# Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models
[Ferret-v2: 在大语言模型中提高指代和视觉定位的改进基线](https://arxiv.org/abs/2404.07973)

尽管 Ferret 成功地将区域理解融入大语言模型 (LLM) 中，从而增强了其指代和定位功能，但它在更广泛任务上的表现受限于预训练的固定视觉编码器。在本项工作中，我们推出了 Ferret-v2，对 Ferret 进行了重要升级，并加入了三项关键设计：(1) 任意分辨率的定位和指代功能，使模型能轻松处理更高分辨率的图像，进而提升对图像细节的处理和理解；(2) 多粒度视觉编码，通过融合新的 DINOv2 编码器，模型能更有效地学习全局及细节视觉信息的各种底层上下文；(3) 三阶段训练范式，在图像-字幕对齐的基础上，增加了一个针对高分辨率密集对齐的训练阶段，后续进行指令调优。实验显示，Ferret-v2 在高分辨率图像处理和细节解析能力上显著优于 Ferret 及其他先进模型。

# JetMoE: Reaching Llama2 Performance with 0.1M Dollars
[JetMoE: 仅需十万美元便可达到 Llama2 的性能水平](https://arxiv.org/abs/2404.07413)

大语言模型 (LLM) 虽然取得了显著成果，但不断增长的资源需求已成为推动高级人工智能发展的一大障碍。本报告介绍的 JetMoE-8B 是一款新型 LLM，训练成本不足十万美元，使用了 1.25T 来自精心挑选开源语料库的 token 和 30,000 H100 GPU 小时。尽管成本较低，JetMoE-8B 的性能却非常出色，超越了 Llama2-7B 模型，而 JetMoE-8B-Chat 甚至超过了 Llama2-13B-Chat 模型。这表明，LLM 训练可以更加经济高效。JetMoE-8B 采用高效的稀疏门控混合专家 (SMoE) 架构，包含注意力和前馈专家层，这两层仅在必要时激活，使得模型虽有 8B 参数，实际推理时只激活 2B 参数，相较 Llama2-7B 减少了约 70% 的计算量。此外，JetMoE-8B 极具开放性，对学术界友好，所有训练数据和代码均采用公开资源，所有训练细节在本报告中有所阐述，以便推动开放模型基础设施的发展。我们的透明策略旨在促进合作和进一步推动高效且易于获取的 LLM 技术的进步。模型权重公开于 https://github.com/myshell-ai/JetMoE。