## Baichuan-Omni Technical Report
[Baichuan-Omni 技术报告](https://arxiv.org/abs/2410.08565)

GPT-4o 在多模态能力和交互体验方面的显著表现，使其在实际应用中扮演了重要角色，但目前尚无高性能的开源替代方案。本文中，我们推出了 Baichuan-Omni，这是首个开源的 7B 多模态大语言模型 (MLLM)，能够同时处理和分析图像、视频、音频和文本，并提供卓越的多模态交互体验和强劲性能。我们设计了一种高效的多模态训练策略，从 7B 模型出发，通过多模态对齐和跨模态多任务微调两个阶段，使模型能够有效处理视觉和音频数据。在多项全模态和多模态基准测试中表现出色，我们希望这一成果能成为开源社区在推动多模态理解和实时交互领域的有力基石。

## Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis
[Meissonic: 重振掩码生成式 Transformer 实现高效高分辨率文本到图像合成](https://arxiv.org/abs/2410.08261)

扩散模型，如 Stable Diffusion，在视觉生成领域取得了显著进展，但其范式与自回归语言模型存在根本性差异，使得统一语言视觉模型的开发变得复杂。近期，如 LlamaGen 等尝试使用离散 VQVAE Token 进行自回归图像生成，但由于 Token 数量庞大，效率低下且速度缓慢。本文提出 Meissonic，将非自回归掩码图像建模 (MIM) 文本到图像生成提升至与 SDXL 等先进扩散模型相媲美的水平。通过引入一系列架构创新、先进的位置编码策略及优化的采样条件，Meissonic 显著提升了 MIM 的性能与效率。此外，我们利用高质量训练数据，结合人类偏好引导的微条件，并采用特征压缩层，进一步增强图像保真度与分辨率。实验表明，Meissonic 在生成高质量高分辨率图像方面不仅与现有模型如 SDXL 相当，且往往更胜一筹。我们发布了一个可生成 1024x1024 分辨率图像的模型检查点。

## From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning
[从通用到专用：通过任务特定视觉指令调优适应视觉语言模型](https://arxiv.org/abs/2410.06456)

大型视觉语言模型 (VLM) 结合了大语言模型与视觉编码器，在多种任务中展现出巨大潜力。然而，由于预训练与微调之间存在领域差异，这些模型在特定任务应用中的表现往往不尽如人意。为此，我们提出了 VITask，一种通过集成任务特定模型 (TSM) 来提升 VLM 任务适应性的创新框架。VITask 采用三种关键策略：示例提示 (EP)、响应分布对齐 (RDA) 和对比响应调优 (CRT)，通过调整响应分布来增强 VLM 的任务性能。EP 利用 TSM 特征引导 VLM，而 RDA 则使 VLM 在推理过程中无需 TSM 即可通过学习示例提示模型进行适应。CRT 进一步优化了正确图像-响应对的排序，从而降低生成不期望响应的风险。实验结果显示，在涵盖 9 种成像模式的 12 个医疗诊断数据集上，VITask 的表现优于普通指令调优的 VLM 和 TSM，有效整合了两种模型的互补特征。此外，VITask 还具备灵活的 TSM 集成能力及对不完整指令的鲁棒性，使其成为任务特定 VLM 调优的灵活且高效解决方案。相关代码已公开，详见 https://github.com/baiyang4/VITask。

## StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization
[StructRAG: 提升大语言模型知识密集型推理能力的推理时混合信息结构化方法](https://arxiv.org/abs/2410.08815)

检索增强生成 (RAG) 是提升大语言模型 (LLMs) 在众多知识型任务中表现的关键技术。然而，现有 RAG 方法在处理知识密集型推理任务时效果不佳，因为所需信息分散严重。这使得现有方法难以准确识别关键信息并进行全局推理。本文基于人类在处理知识密集型推理时将原始信息转化为结构化知识的认知理论，提出了一种新框架——StructRAG。该框架能识别任务的最佳结构类型，将原始文档重构为该结构化格式，并基于此结构进行推理。在多种知识密集型任务上的实验表明，StructRAG 达到了最先进水平，特别是在挑战性场景中表现尤为出色，展示了其在增强大语言模型复杂应用中的潜力。

## LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models
[LOKI: 使用大模型进行综合合成数据检测的基准测试](https://arxiv.org/abs/2410.09732)

随着生成式 AI 内容的快速发展，未来的互联网可能会充斥着合成数据，使得真实和可信的多模态数据的辨别变得越来越具有挑战性。因此，合成数据检测引起了广泛关注，大模型在这一任务中的表现也吸引了大量兴趣。大模型可以为其真实性判断提供自然语言解释，从而增强了合成内容检测的解释性。同时，区分真实和合成数据的任务有效地测试了大模型的感知、知识和推理能力。为此，我们引入了 LOKI，这是一个新颖的基准测试，旨在评估大模型在多模态中检测合成数据的能力。LOKI 涵盖了视频、图像、3D、文本和音频模式，包含 18K 个精心设计的问题，分为 26 个子类别，具有明确的难度级别。该基准测试包括粗粒度判断和多项选择题，以及细粒度异常选择和解释任务，便于对大模型进行全面分析。我们在 LOKI 上评估了 22 个开源大模型和 6 个闭源模型，突显了它们作为合成数据检测器的潜力，同时也揭示了大模型能力发展中的一些局限性。更多关于 LOKI 的信息可以在 https://opendatalab.github.io/LOKI/ 找到。

## MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models
[MMIE: 大型视觉-语言模型的大规模多模态交错理解基准测试](https://arxiv.org/abs/2410.10139)

交错多模态理解和生成，即模型能够在任意序列中生成和解释图像与文本，已成为多模态学习的关键领域。尽管取得了显著进展，但对此能力的评估仍显不足。现有基准在数据规模、范围和评估深度方面存在局限，而当前的评估指标往往成本高昂或存在偏见，缺乏实际应用中的可靠性。

为应对这些挑战，我们引入了MMIE，这是一个大规模知识密集型基准测试，用于评估大型视觉-语言模型（LVLMs）中的交错多模态理解和生成。MMIE包含20K精心策划的多模态查询，涵盖3个类别、12个领域和102个子领域，包括数学、编码、物理、文学、健康和艺术。它支持交错的输入和输出，提供多种选择和开放式问题格式的混合，以评估多样化的能力。

此外，我们提出了一种可靠的自动化评估指标，利用经过人工标注数据微调的评分模型和系统的评估标准，旨在减少偏见并提高评估准确性。广泛的实验表明，我们的基准测试和指标在全面评估交错LVLMs方面具有有效性。具体而言，我们评估了八种LVLMs，发现即使是最佳模型也显示出显著的改进空间，大多数模型仅取得了中等结果。

我们相信MMIE将推动交错LVLMs的进一步发展。我们在https://mmie-bench.github.io/公开发布了我们的基准测试和代码。

## Animate-X: Universal Character Image Animation with Enhanced Motion Representation
[Animate-X: 增强运动表示的通用角色图像动画](https://arxiv.org/abs/2410.10306)

角色图像动画技术近年来取得了显著进展，能够从参考图像和目标姿态序列生成高质量视频。然而，现有方法大多仅适用于人体图像，在游戏和娱乐等行业中常用的拟人化角色上表现不佳。我们的深入分析表明，这一局限性源于对运动建模的不足，无法准确理解驱动视频的运动模式，导致姿态序列僵硬地应用于目标角色。为此，本文提出了 Animate-X，这是一个基于 LDM 的通用动画框架，适用于各种角色类型（统称为 X），包括拟人化角色。

为了增强运动表示，我们引入了姿态指示器，通过隐式和显式方式全面捕捉驱动视频的动作模式。隐式方法利用 CLIP 视觉特征提取驱动视频的运动主旨，如整体运动模式和动作间的时间关系；显式方法则通过提前模拟推理过程中可能出现的输入，增强 LDM 的泛化能力。此外，我们引入了一个新的动画拟人化基准（A^2Bench），以评估 Animate-X 在通用且广泛适用的动画图像上的性能。大量实验表明，与最先进的方法相比，Animate-X 具有优越性和有效性。

## Toward General Instruction-Following Alignment for Retrieval-Augmented Generation
[Toward General Instruction-Following Alignment for Retrieval-Augmented Generation](https://arxiv.org/abs/2410.09584)

自然指令的遵循对检索增强生成 (RAG) 系统的有效应用至关重要。尽管大语言模型 (LLM) 近期有所进展，但 RAG 领域内指令遵循 (IF) 对齐的评估与改进研究仍显不足。为此，我们提出了 VIF-RAG，这是首个自动化、可扩展且可验证的合成管道，用于 RAG 系统中的指令对齐。首先，我们手动创建了一组少于 100 条的原子指令，并制定了组合规则以合成和验证复杂指令。随后，我们利用监督模型进行指令重写，并通过 Python 执行器自动验证指令质量。最后，我们将这些指令与大量 RAG 和通用数据样本结合，通过自动化流程扩展至高质量的 VIF-RAG-QA 数据集 (>100k)。

为弥补 RAG 系统指令自动评估的空白，我们引入了 FollowRAG 基准，包含约 3K 测试样本，涵盖 22 类一般指令约束和四个知识密集型 QA 数据集。得益于其稳健的管道设计，FollowRAG 可无缝集成于不同 RAG 基准。通过 FollowRAG 及八个广泛使用的 IF 和基础能力基准对 LLM 进行测试，我们发现 VIF-RAG 显著提升了 LLM 在多种指令约束下的性能，并有效利用了其在 RAG 场景中的能力。进一步分析为实现 RAG 系统中的指令对齐提供了实用见解。相关代码和数据集已发布于 https://FollowRAG.github.io。

## MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks
[MEGA-Bench: 将多模态评估扩展至 500 多个真实世界任务](https://arxiv.org/abs/2410.10563)

我们推出了 MEGA-Bench，这是一个旨在将多模态评估扩展至 500 多个真实世界任务的评估套件。此举旨在应对终端用户日常使用中高度异构的需求。我们的目标是通过一组高质量的数据样本，全面覆盖多样且丰富的多模态任务，同时确保评估过程既经济又准确。具体而言，我们收集了 505 个现实任务，涵盖超过 8,000 个样本，这些任务由 16 位专家注释者提供，以确保多模态任务空间的广泛覆盖。

与现有方法不同，我们并未将这些任务统一为标准的多选题形式（如 MMMU、MMBench 和 MMT-Bench），而是采用了多种输出格式，包括数字、短语、代码、\LaTeX、坐标、JSON 以及自由文本等。为适应这些多样化的输出格式，我们开发了超过 40 种评估指标。

MEGA-Bench 的独特之处在于，它提供了跨多个维度（如应用领域、输入类型、输出格式、技能要求）的细粒度能力报告，使用户能够深入了解并可视化模型的各项能力。我们在 MEGA-Bench 上对多种前沿视觉语言模型进行了评估，以全面了解它们在这些维度上的表现。

## LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content
[LiveXiv -- 基于 Arxiv 论文内容的多模态实时基准](https://arxiv.org/abs/2410.10783)

在从网络抓取的数据上对多模态模型进行大规模训练，已显示出在这些模型中注入所需的世界知识，从而有效执行多个下游任务的卓越效用。然而，从网络抓取数据的缺点之一可能是牺牲用于评估这些模型能力的基准。为了防止测试数据污染并真正测试这些基础模型的能力，我们提出了 LiveXiv：一个基于科学 ArXiv 论文的可扩展演化实时基准。LiveXiv 在任何给定时间戳访问特定领域的稿件，并提出自动生成视觉问答对 (VQA)。这一过程无需人工干预，利用稿件中的多模态内容，如图形、图表和表格。此外，我们引入了一种高效的评估方法，通过仅评估模型子集来估计所有模型在演化基准上的性能，从而显著降低整体评估成本。我们在基准的第一个版本上对多个开放和专有的多模态大模型 (LMM) 进行了基准测试，展示了其挑战性并揭示了模型的真实能力，避免了污染。最后，在我们对高质量的承诺中，我们收集并评估了一个手动验证的子集。通过比较其总体结果与自动注释，我们发现性能差异确实很小 (<2.5%)。我们的数据集可在 HuggingFace 在线获取，我们的代码将在此处提供。

## Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models
[Omni-MATH: 面向大语言模型的通用奥林匹克级别数学基准](https://arxiv.org/abs/2410.07985)

大语言模型 (LLMs) 的最新进展显著提升了数学推理能力。然而，现有基准如 GSM8K 或 MATH 已被高精度解决（例如，OpenAI o1 在 MATH 数据集上达到 94.8%），显示其已不足以真正挑战这些模型。为此，我们提出了一项全面且具有挑战性的基准，专门用于评估 LLMs 在奥林匹克级别的数学推理能力。与现有奥林匹克相关基准不同，我们的数据集专注于数学，包含 4428 道经过严格人工标注的竞赛级别问题。这些问题被细分为超过 33 个子领域，并涵盖 10 多个难度级别，从而全面评估模型在奥林匹克数学推理中的表现。此外，我们基于此基准进行了深入分析。实验结果显示，即使是最高级的模型，OpenAI o1-mini 和 OpenAI o1-preview，在高度挑战性的奥林匹克级别问题上准确率分别为 60.54% 和 52.55%，突显了奥林匹克级别数学推理的重大挑战。

## Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free
[Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free](https://arxiv.org/abs/2410.10814)

尽管大语言模型 (LLMs) 在生成任务上表现出色，但其仅解码器架构若不进行进一步表示微调，通常会限制其作为嵌入模型的潜力。这是否与它们自称的通用性相矛盾？为了回答这个问题，我们深入研究了专家混合 (MoE) 大语言模型。我们的研究表明，MoE 大语言模型中的专家路由器可以作为现成的嵌入模型，在多种嵌入任务上表现出有前景的性能，而无需任何微调。此外，我们广泛的分析表明，MoE 路由权重 (RW) 与大语言模型广泛使用的隐藏状态 (HS) 是互补的。与 HS 相比，我们发现 RW 对提示选择的鲁棒性更强，并且更关注高级语义。受此分析启发，我们提出了 MoEE，它结合了 RW 和 HS，其性能优于单独使用其中任何一种。我们对它们组合和提示策略的探索揭示了几个新颖的见解，例如，RW 和 HS 相似性的加权和优于它们的串联相似性。我们的实验在 Massive Text Embedding Benchmark (MTEB) 的 6 个嵌入任务和 20 个数据集上进行。结果表明，MoEE 在不进行进一步微调的情况下，显著提升了基于大语言模型的嵌入性能。

## LLM$\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models
[LLM$\times$MapReduce: 使用大语言模型简化长序列处理](https://arxiv.org/abs/2410.09342)

扩展大语言模型 (LLMs) 的上下文窗口已成为一个关键的研究领域，特别是在涉及极长文本的应用中。在这项工作中，我们提出了一种新颖的无训练框架，用于处理长文本，利用分治策略来实现全面的文档理解。所提出的 LLMtimesMapReduce 框架将整个文档分割成多个部分供 LLMs 阅读，然后聚合中间答案以生成最终输出。分治长文本处理框架的主要挑战在于，在分割文档时存在丢失重要长程依赖的风险，这可能导致模型基于分割后的文本生成不完整或错误的答案。中断的长程依赖可以分为两类：块间依赖和块间冲突。我们设计了一种结构化信息协议来更好地处理块间依赖，并设计了一种上下文内置信度校准机制来解决块间冲突。实验结果表明，LLMtimesMapReduce 可以优于代表性的开源和商业长上下文 LLMs，并且适用于几种不同的模型。

## Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts
[通过语言家族专家混合高效普及50种语言的医疗大语言模型](https://arxiv.org/abs/2410.10626)

将医疗大语言模型适配到本地语言中可以降低获取医疗服务的障碍，但数据稀缺仍然是一个重大挑战，特别是对于低资源语言。为了解决这个问题，我们首先构建了一个高质量的医疗数据集，并进行分析以确保数据集的质量。为了利用多语言大语言模型的泛化能力，高效地扩展到更多资源受限的语言中，我们探索了从多语言角度使用专家混合（MoE）模块化的LLM内部信息流。技术上，我们提出了一种新的MoE路由方法，该方法采用语言特定的专家和跨语言路由机制。受电路理论启发，我们的路由分析揭示了一种“末端扩散”的信息流模式：虽然早期层集中了跨语言信息流，但后期层表现出语言特定的发散。这一见解直接导致了Post-MoE架构的发展，该架构仅在后期层应用稀疏路由，同时保持其他层的密集性。实验结果表明，这种方法增强了多语言模型对其他语言的泛化能力，同时保持了模型的解释性。最后，为了高效地将模型扩展到50种语言，我们引入了语言家族专家的概念，借鉴了语言学先验知识，这使得在不增加额外参数的情况下扩展语言数量成为可能。

## VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI
[VidEgoThink: 评估具身AI的自我中心视频理解能力](https://arxiv.org/abs/2410.11623)

多模态大语言模型 (MLLMs) 的最新进展为具身AI应用带来了新机遇。基于先前的工作 EgoThink，我们推出了 VidEgoThink，这是一个全面评估自我中心视频理解能力的基准。为弥合 MLLMs 与具身AI低级控制间的差距，我们设计了四个关键任务：视频问答、层次规划、视觉定位和奖励建模。为降低手动标注成本，我们基于 Ego4D 数据集开发了自动数据生成管道，利用 GPT-4o 的先验知识和多模态能力。三位人类标注者随后对生成数据进行筛选，确保多样性和质量，最终形成 VidEgoThink 基准。我们采用三种模型进行了广泛实验：基于API的 MLLMs、开源的基于图像的 MLLMs 和开源的基于视频的 MLLMs。实验结果显示，包括 GPT-4o 在内的所有 MLLMs 在自我中心视频理解相关任务中表现欠佳。这表明，基础模型在应用于具身AI的第一人称场景前，仍需大幅改进。总之，VidEgoThink 反映了利用 MLLMs 进行自我中心视觉研究的趋势，旨在实现类似人类的主动观察和互动能力，以应对复杂现实环境。

## HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks
[HumanEval-V: 通过编码任务评估大型多模态模型的视觉理解和推理能力](https://arxiv.org/abs/2410.12381)

编码任务对于评估大语言模型 (Large Language Models, LLMs) 具有重要价值，因为它们要求对高级指令的理解、复杂推理以及功能程序的实现——这些都是推进通用人工智能 (Artificial General Intelligence) 的核心能力。尽管大型多模态模型 (Large Multimodal Models, LMMs) 在视觉感知和理解能力方面取得了进展，但在强调视觉推理的任务中，仍然缺乏严格的编码基准来评估这些模型。为了填补这一空白，我们推出了 HumanEval-V，这是一个新颖且轻量级的基准，专门设计用于通过代码生成来评估 LMMs 的视觉理解和推理能力。HumanEval-V 包括 108 个精心设计的入门级 Python 编码任务，源自 CodeForces 和 Stack Overflow 等平台。每个任务通过修改原始问题的上下文和算法模式来改编，视觉元素被重新绘制以确保与源头的区别，防止潜在的数据泄露。LMMs 需要根据提供的视觉上下文和预定义的 Python 函数签名完成代码解决方案，以明确任务要求。每个任务都配备了精心手工制作的测试用例，以确保对模型生成的解决方案进行全面且可靠的评估。我们使用 HumanEval-V 评估了 19 个最先进的 LMMs，发现了显著的挑战。如 GPT-4o 这样的专有模型在 pass@1 上仅达到 13%，在 pass@10 上达到 36.4%，而具有 70B 参数的开源模型在 pass@1 上的得分低于 4%。消融研究进一步揭示了当前 LMMs 在视觉推理和编码能力方面的局限性。这些结果强调了未来研究中需要增强 LMMs 能力的关键领域。我们在 https://github.com/HumanEval-V/HumanEval-V-Benchmark 上开源了我们的代码和基准。

## The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio
[多模态挑战：评估大语言模型在语言、视觉和音频中的幻觉现象](https://arxiv.org/abs/2410.12787)

大语言模型 (LMM) 的最新进展在多种任务中显著提升了性能，并持续努力整合视频和音频等额外模态。然而，大多数现有 LMM 仍易受幻觉现象的影响，即事实多模态输入与生成文本输出之间的差异，这限制了其在各种现实场景中的应用。本文首次系统地研究了涉及语言、视觉和音频三种最常见模态的 LMM 中的幻觉现象。我们的研究发现，幻觉现象的两个主要原因是过度依赖单模态先验知识和虚假的模态间关联。为应对这些挑战，我们引入了基准测试“多模态挑战 (CMM)”，该基准全面评估了 LMM 中的幻觉现象，并详细分析了其根本问题。我们的研究结果突显了关键的脆弱性，包括模态整合的不平衡和训练数据中的偏见，强调了平衡跨模态学习和增强幻觉缓解策略的必要性。基于我们的观察和发现，我们提出了可能增强 LMM 可靠性的潜在研究方向。

## Movie Gen: A Cast of Media Foundation Models
[Movie Gen: 媒体基础模型的阵容](https://arxiv.org/abs/2410.13720)

我们提出了 Movie Gen，这是一组能够生成高质量 1080p HD 视频的基础模型，支持不同的宽高比和同步音频。此外，我们还展示了其他功能，如基于精确指令的视频编辑和根据用户图像生成个性化视频。我们的模型在多个任务上设定了新的最先进水平：文本到视频合成、视频个性化、视频编辑、视频到音频生成以及文本到音频生成。我们最大的视频生成模型是一个 30B 参数的 Transformer，训练时最大上下文长度为 73K 视频 Token，对应于以 16 帧每秒生成的 16 秒视频。我们在架构、潜在空间、训练目标和配方、数据整理、评估协议、并行化技术和推理优化方面展示了多项技术创新和简化，使我们能够从扩展预训练数据、模型大小和训练计算中获益，以训练大规模媒体生成模型。我们希望这篇论文能帮助研究社区加速媒体生成模型的进展和创新。本文中的所有视频均可在 https://go.fb.me/MovieGenResearchVideos 获取。

## MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures
[MixEval-X: 从真实世界数据混合中进行任意到任意评估](https://arxiv.org/abs/2410.13754)

感知和生成多样化的模态对于 AI 模型有效学习和与真实世界信号互动至关重要，这需要对其发展进行可靠的评估。我们发现当前评估中存在两个主要问题：(1) 标准不一致，受不同社区的协议和成熟度水平影响；(2) 明显的查询、评分和泛化偏差。为解决这些问题，我们引入了 MixEval-X，这是首个旨在优化和标准化输入和输出模态评估的任意到任意真实世界基准。我们提出了多模态基准混合和适应矫正管道，以重建真实世界的任务分布，确保评估能有效泛化到真实世界的用例。广泛的元评估显示，我们的方法有效地将基准样本与真实世界的任务分布对齐，模型排名与众包的真实世界评估高度相关（高达 0.98）。我们提供全面的排行榜，以重新排名现有模型和组织，并提供见解以增强对多模态评估的理解，并指导未来研究。

## MobA: A Two-Level Agent System for Efficient Mobile Task Automation
[MobA: 一种用于高效移动任务自动化的两级智能体系统](https://arxiv.org/abs/2410.13757)

当前的移动助手受限于对系统 API 的依赖，或在处理复杂用户指令和多样界面时，由于理解和决策能力的限制而面临挑战。为应对这些挑战，我们提出了 MobA，一种由多模态大语言模型驱动的移动电话智能体，通过两级智能体架构增强了理解和规划能力。全局智能体 (GA) 负责理解用户命令、跟踪历史记忆和规划任务。本地智能体 (LA) 在子任务和 GA 记忆的指导下，以函数调用的形式预测详细动作。引入反思模块，使得任务完成更加高效，并使系统能够处理以前未见过的复杂任务。MobA 在实际评估中展示了任务执行效率和完成率的显著提升，突显了多模态大语言模型赋能的移动助手的潜力。

