# VisionLLaMA: A Unified LLaMA Interface for Vision Tasks
[VisionLLaMA: 视觉任务的统一 LLaMA 接口](https://arxiv.org/abs/2403.00522)

大语言模型基于 Transformer 架构构建，主要处理文本输入。其中，LLaMA 是众多开源实现中的佼佼者。那么，相同的 Transformer 能否用于处理 2D 图像呢？本文通过提出一种专为此目的设计的 LLaMA 式视觉 Transformer —— VisionLLaMA（包括平面和金字塔两种形式），来回答这一问题。VisionLLaMA 是一个统一且通用的模型框架，旨在解决大部分视觉任务。我们通过在图像感知及尤其是图像生成的下游任务中使用典型预训练范式对其有效性进行了广泛评估。在许多情况下，VisionLLaMA 在性能上都有显著提升，超越了之前的最先进视觉 Transformer。我们认为 VisionLLaMA 可作为视觉生成和理解的强大新基准模型。我们的代码将发布在 https://github.com/Meituan-AutoML/VisionLLaMA。
# Learning and Leveraging World Models in Visual Representation Learning
[学习与利用视觉表示学习中的世界模型](https://arxiv.org/abs/2403.00504)

联合嵌入预测架构 (JEPA) 成为一种前景广阔的自监督学习方法，该方法通过利用世界模型进行学习。过去仅限于预测输入的缺失部分，本研究将 JEPA 预测任务推广到更广泛的损坏情况。我们引入了图像世界模型 (IWM)，该模型超越了掩码图像建模，学习预测潜在空间中全局光度变换的效果。我们研究了学习有效 IWM 的关键因素，包括条件化、预测难度和模型容量。此外，我们证明通过微调，IWM 世界模型能够适应解决多样化的任务，并且在性能上匹配或超越之前的自监督方法。最后，我们展示了如何通过 IWM 学习，控制所学表示的抽象级别，既可以学习像对比方法那样的不变表示，也可以学习像掩码图像建模那样的等变表示。

# Resonance RoPE: Improving Context Length Generalization of Large Language Models
[Resonance RoPE: 改善大型语言模型的上下文长度泛化能力](https://arxiv.org/abs/2403.00071)

本文针对训练短测试长 (TSTL) 情境下的挑战，尤其是那些采用旋转位置嵌入 (RoPE) 的大型语言模型 (LLMs)，这些模型在预训练时使用较短的序列，随后在处理较长序列时面临分布外 (OOD) Token位置的难题。我们提出了 Resonance RoPE，一种新颖方法，通过改善 OOD 位置的 RoPE 特征插值，缩小 TSTL 场景中的泛化差距，显著提升了模型性能，且不增加额外的在线计算成本。我们还引入了 PosGen，一个专为 TSTL 场景下细致行为分析而设计的新合成基准，旨在区分长上下文中 Token 生成难度的持续增长和识别新 Token 位置的挑战。我们的实验显示，应用 Resonance RoPE 后，Transformers 能更好且更稳定地识别 OOD 位置。在广泛的 LLM 实验中，采用 Resonance RoPE 的性能超越了现有最先进的 RoPE 缩放方法 YaRN，在上游语言建模任务和多种下游长文本应用中表现出色。

# MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies
[MovieLLM: 通过 AI 生成的电影提升长视频理解](https://arxiv.org/abs/2403.01422)

多模态模型的发展在机器理解视频方面取得了重要进展，尤其是在分析短视频片段方面。然而，对于电影这类长格式视频，它们往往表现不佳，主要障碍是高质量、多样化视频数据的缺乏，以及收集或标注此类数据的高成本。面对这些挑战，我们提出了 MovieLLM，一个旨在为长视频创建合成、高质量数据的创新框架。该框架充分利用了 GPT-4 和文本到图像模型的能力，生成详细的剧本和相应的视觉效果。我们的方法以其灵活性和扩展性而突出，是传统数据收集方法的有效替代。我们的广泛实验验证了，MovieLLM 生成的数据显著提升了多模态模型在理解复杂视频叙事方面的性能，克服了现有数据集的稀缺性和偏见问题。

# OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on
[OOTDiffusion: 基于融合的潜在扩散技术，用于可控虚拟试穿](https://arxiv.org/abs/2403.01779)

基于图像的虚拟试穿（VTON）任务，即生成目标人物穿上店内服装的图片，是一项具有挑战性的图像合成任务。它要求不仅人物的造型高度逼真，还要完整保留服装的所有细节。为应对此挑战，我们推出了 Outfitting over Try-on Diffusion (OOTDiffusion) 技术，通过预训练的潜在扩散模型的强大能力，并设计了全新的网络架构，实现了真实且可控制的虚拟试穿体验。我们设计了一种服装 UNet，无需传统的变形过程，即可学习服装细节特征，并在扩散模型的去噪过程中，通过我们提出的服装融合技术，将其与目标人体结合。为了增强服装 UNet 的可控性，我们在训练过程中加入了服装 dropout 技术，这使我们可以通过无分类器引导技术调节服装特征的强度。我们在 VITON-HD 和 Dress Code 数据集上的综合实验结果表明，OOTDiffusion 能够高效地生成任意人物和服装图像的高质量装扮图片，其在保真度和可控性方面均优于其他 VTON 方法，标志着在虚拟试穿领域的一大突破。我们的源代码已发布在 https://github.com/levihsu/OOTDiffusion。

# AtomoVideo: High Fidelity Image-to-Video Generation
[AtomoVideo: 高保真图像到视频生成](https://arxiv.org/abs/2403.01800)

随着优秀的文本到图像生成技术的发展，视频生成领域近期也取得了显著进展。在本项工作中，我们提出了一个名为 AtomoVideo 的高保真图像到视频生成框架。通过采用多粒度的图像融合策略，我们实现了对给定图像生成更高保真度的视频。另外，得益于高质量的数据集和训练策略，我们在保持时间连贯性和稳定性的同时，实现了更加生动的运动效果。我们的架构灵活地扩展到视频帧预测任务，通过迭代生成，实现了长序列预测。此外，得益于适配器训练的设计，我们的方法可以与现有的个性化模型和可控模块良好地融合。经过定量和定性的评估，AtomoVideo 在与流行方法的比较中展现了卓越的性能，更多示例可以在我们的项目网站 https://atomo-video.github.io/ 查看。

# InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding
[InfiMM-HD: 高分辨率多模态理解的进步](https://arxiv.org/abs/2403.01487)

多模态大语言模型 (MLLMs) 最近取得了重大进展。然而，高分辨率图像中复杂细节的准确识别和理解仍面临挑战。尽管这对于发展强大的 MLLMs 非常关键，但这一领域的研究仍然不足。为此，我们推出了 InfiMM-HD，这是一种专为处理不同分辨率图像而设计的新架构，旨在以低计算成本扩展 MLLMs 到更高分辨率。InfiMM-HD 通过引入交叉注意模块和视觉窗口来降低计算成本。通过结合这种架构设计和四阶段训练流程，我们的模型以高效且经济的方式达到了改进视觉感知的目标。实证研究突显了 InfiMM-HD 的鲁棒性和有效性，为高分辨率多模态理解领域的进一步探索提供了新的可能。代码和模型已发布在 https://huggingface.co/Infi-MM/infimm-hd。

# Design2Code: How Far Are We From Automating Front-End Engineering?
[Design2Code: 自动化前端工程还有多远？](https://arxiv.org/abs/2403.03163)

近年来，生成式 AI 技术飞速进展，特别在多模态理解和代码生成方面展现出了空前的能力。这为前端开发带来了一种全新的范式，即直接将视觉设计转化为代码的可能性。在这篇论文中，我们把这一过程定义为 Design2Code 任务，并对其进行了全面的基准测试。我们精心挑选了 484 个各具特色的真实网页案例进行测试，并开发了一系列自动评估指标，以评定当前多模态大语言模型在将截图作为输入的情况下，生成的代码质量如何，即这些代码是否能准确还原为给定的参考网页。此外，我们还引入了详尽的人工评估来辅助自动化指标。我们还展示了多种多模态提示技术，并证明了它们在 GPT-4V 和 Gemini Pro Vision 上的有效性。我们进一步微调了一个开源的 Design2Code-18B 模型，并成功达到了与 Gemini Pro Vision 相当的表现。无论是人工评估还是自动化指标，GPT-4V 在此任务上的表现均优于其他模型。更值得一提的是，审阅人员认为，在视觉外观和内容方面，有 49% 的 GPT-4V 生成的网页可以完全替代原始参考网页；令人惊喜的是，有 64% 的情况下，GPT-4V 生成的网页甚至被认为优于原始参考网页。我们的详细评估指标显示，开源模型主要在回放输入网页的视觉元素和产生准确布局设计方面存在不足，而通过适当的微调，文本内容和配色等方面的表现大幅提升。

# Scaling Rectified Flow Transformers for High-Resolution Image Synthesis
[放大校正流 Transformer 以实现高分辨率图像合成](https://arxiv.org/abs/2403.03206)

扩散模型通过反向模拟从数据到噪声的过程来从噪声中生成数据，已经成为处理高维感知数据（如图像和视频）的一种强有力的生成模型技术。校正流是一种新兴的生成模型，它通过一条直线连接数据和噪声。虽然理论性质更优且概念上更简单，但它还没有成为标准的实践方法。在这项研究中，我们通过倾向于感知相关尺度的方式改进了训练校正流模型的噪声采样技术。通过一项大规模研究，我们展示了这种方法在高分辨率文本到图像合成方面，相比传统扩散模型有更好的性能。此外，我们引入了一种基于 Transformer 的新型文本到图像生成架构，该架构为两种模态分配了独立的权重，并实现了图像和文本 Token 之间的双向信息流，从而提高了文本理解、排版质量和人们的偏好。我们证实了这种架构的可预测扩展趋势，并发现验证损失的降低与通过各种评估指标和人类评价所衡量的文本到图像合成质量的提高呈正相关。我们的大型模型在性能上超越了现有最先进的模型，我们计划公开实验数据、代码和模型权重。

# NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models
[NaturalSpeech 3: 基于因子化编解码器和扩散模型的零样本语音合成](https://arxiv.org/abs/2403.03100)

尽管最近的大规模文本到语音（TTS）模型取得了重大进展，但在语音质量、相似性和韵律方面仍有待提高。鉴于语音包含了多个复杂属性（如内容、韵律、音色和声学细节），这些属性的生成面临显著挑战，因此一个自然的思路是将语音分解成表示不同属性的子空间，并分别对其进行生成。基于这一理念，我们提出了 NaturalSpeech 3，这是一个采用新型因子化扩散模型的 TTS 系统，能够以零样本方式生成自然语音。具体来说，1) 我们设计了一个带有因子化向量量化（FVQ）的神经编解码器，用于将语音波形解构为内容、韵律、音色和声学细节等子空间；2) 我们提出一个因子化扩散模型，在每个子空间中根据对应的提示生成属性。这种因子化设计使 NaturalSpeech 3 能以分而治之的方式高效准确地模拟复杂的语音及其解构的子空间。实验结果表明，NaturalSpeech 3 在质量、相似性、韵律和可理解性方面超越了当前最先进的 TTS 系统。通过将模型扩展至 10 亿参数和 20 万小时的训练数据，我们获得了更佳的性能。

# Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters
[微调后的多模态语言模型作为高质量图文数据过滤器](https://arxiv.org/abs/2403.02677)

我们提出了一种新颖的框架，用于利用微调后的多模态语言模型（MLMs）过滤图文数据。通过整合最新的 MLM 进展，我们的方法优于现有主流过滤方法（如 CLIPScore）。我们设计了四种不同但相辅相成的指标，全面评估图文数据的质量。我们建立了一套新的流程，为微调 MLM 作为数据过滤器构建高质量的指导数据。与 CLIPScore 相比，我们的 MLM 过滤器产生了更精确和全面的评分，直接提升了数据过滤的质量，并增强了预训练模型的性能。我们在流行的基础模型（例如 CLIP 和 BLIP2）和多种下游任务上取得了显著进步。我们的 MLM 过滤器能够适用于不同的模型和任务，并可以作为 CLIPScore 的有效替代。此外，我们还提供了一项额外的消融研究，以验证我们的 MLM 过滤器设计选择的有效性。

# GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection
[GaLore: 通过梯度低秩投影提高大语言模型训练的内存效率](https://arxiv.org/abs/2403.03507)

大语言模型的训练面临着显著的内存使用挑战，主要是因为权重和优化器状态大小的不断膨胀。尽管诸如低秩适配（LoRA）之类的内存减少技术通过在每一层添加一个可训练的低秩矩阵到冻结的预训练权重中，减少了可训练参数和优化器状态，但这些方法在预训练和微调阶段的性能通常不如使用全秩权重的训练，因为它们将参数搜索限制在了一个低秩子空间内，并改变了训练动态，有时还需进行全秩的热启动。在本项工作中，我们提出了梯度低秩投影（GaLore）策略，它在允许全参数学习的同时，比传统低秩适配方法如 LoRA 更节省内存。我们的方法在优化器状态的内存使用上实现了高达 65.5% 的降低，同时在对 LLaMA 1B 和 7B 架构使用 C4 数据集进行预训练以及在 GLUE 任务上微调 RoBERTa 时，保持了高效性和性能。我们的 8 位 GaLore 进一步将优化器内存降低了高达 82.5%，与 BF16 基线相比，总训练内存降低了 63.3%。特别地，我们首次证明了在消费级 GPU（例如 NVIDIA RTX 4090，24GB 内存）上，无需模型并行、检查点或卸载策略，就能预训练一个 7B 模型的可能性。

# ShortGPT: Layers in Large Language Models are More Redundant Than You Expect
[ShortGPT: 大语言模型层的冗余度超乎你的预期](https://arxiv.org/abs/2403.03853)

随着大语言模型性能的持续提升，它们的规模也在显著扩大，当前的模型包含了数十亿乃至数万亿的参数。然而，本研究发现，很多大语言模型的层之间存在高度的相似性，有些层对网络的功能几乎没有影响。基于此，我们定义了“块影响力（BI）”这一指标来衡量 LLM 中每个层的重要性。接着，我们提出了一种简单的剪枝方法——层移除，即根据 BI 分数直接去除冗余层。实验证明，我们称之为 ShortGPT 的方法在模型剪枝方面显著优于现有的最先进方法。更重要的是，ShortGPT 与量化等方法相互独立，可以进一步减少参数和计算量。通过简单的层移除而不是复杂的剪枝技巧获得更好的结果，揭示了模型架构中存在的高度冗余。

# SaulLM-7B: A pioneering Large Language Model for Law
[SaulLM-7B: 法律领域的先锋大语言模型](https://arxiv.org/abs/2403.03883)

本文介绍了 SaulLM-7B，这是一款专门针对法律领域设计的大型语言模型，拥有 70 亿参数，它是首个明确针对法律文本理解和生成而设计的大模型。SaulLM-7B 基于 Mistral 7B 架构，并在包含超过 300 亿 Token 的英文法律语料库上进行训练，展现了在理解和处理法律文档方面的最先进水平。此外，我们还介绍了一种新颖的、基于法律数据集的指导性微调方法，以进一步提升 SaulLM-7B 在法律任务上的表现。SaulLM-7B 以 CC-BY-SA-4.0 许可证发布。

# Yi: Open Foundation Models by 01.AI
[Yi: 由 01.AI 提供的开放式基础模型](https://arxiv.org/abs/2403.04652)

我们推出了 Yi 模型系列，这是一系列展现出强大多维能力的语言和多模态模型。Yi 模型系列基于 6B 和 34B 的预训练语言模型，并进一步扩展到聊天模型、200K 长上下文模型、深度增强模型和视觉语言模型。我们的基础模型在如 MMLU 等广泛的基准测试上表现出色，我们微调后的聊天模型在 AlpacaEval 和 Chatbot Arena 等主要评估平台上获得了高人类偏好率。基于我们可扩展的超级计算基础设施和经典的 Transformer 架构，Yi 模型的卓越性能主要归功于我们在数据工程上的努力，结果是高质量的数据。在预训练阶段，我们利用级联数据去重和质量过滤流程，构建了 3.1 万亿 Token 的英中语料库。在微调阶段，我们经过多次迭代精细打磨少于 1 万的指令数据集，确保每个实例都经过我们机器学习工程师的直接验证。在视觉语言领域，我们将聊天语言模型与视觉 Transformer 编码器相结合，训练模型以将视觉表征与语言模型的语义空间对齐。通过轻量级的连续预训练，我们进一步将上下文长度扩展到 200K，并展示出强大的“大海捞针”式检索性能。我们发现，通过连续预训练加深预训练检查点的深度，可以进一步提高性能。鉴于当前的成果，我们相信，通过对数据进行深度优化并持续扩大模型参数，将能够开发出更强大的前沿模型。

# Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference
[Chatbot Arena: 评估大语言模型 (LLMs) 与人类偏好一致性的开放平台](https://arxiv.org/abs/2403.04132)

大语言模型 (LLMs) 为我们开辟了新的能力和应用前景。然而，评估它们是否与人类偏好相符仍然充满挑战。为此，我们推出 Chatbot Arena，这是一个基于人类偏好来评估 LLMs 的开放式平台。本平台采用成对比较的方式，并通过众包方式，汇聚了来自各方用户的意见。该平台已经运行了数月，收集了超过 240K 的投票。本文将介绍这一平台，分析我们迄今为止收集的数据，并说明我们采用的一些经过验证的统计方法，这些方法能够高效且准确地对模型进行评估和排名。我们验证了，通过众包得到的问题覆盖了广泛的范围且具有辨识度，同时，众包得到的人类投票与专家评价高度一致。这些分析为 Chatbot Arena 的可靠性提供了强有力的证据。得益于其独特的价值和开放性，Chatbot Arena 已经成为被引用最多的 LLM 排行榜之一，受到许多领先的 LLM 开发者和公司的广泛关注。我们的演示现已公开，欢迎访问 https://chat.lmsys.org。

# PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation
[PixArt-Σ: 4K文本到图像生成的弱到强训练扩散 Transformer (DiT) 模型](https://arxiv.org/abs/2403.04692)

本文介绍 PixArt-Σ，一种能够直接生成 4K 分辨率图像的扩散 Transformer (DiT) 模型。相比于其前作 PixArt-α，PixArt-Σ 在图像的高保真度和文本提示对齐方面取得了显著进步。其训练效率的提升是 PixArt-Σ 的一大亮点。通过在 PixArt-α 的基础预训练上进一步加强，它采用了“从弱到强”的训练方法，通过引入更高质量的数据，实现了模型的质的飞跃。PixArt-Σ 的优化在两个方面：一是引入了更高质量的训练数据和更精确的图像描述；二是提出了一种新颖的注意力模块，有效压缩了关键信息和值，极大地提升了生成超高分辨率图像的效率。得益于这些改进，PixArt-Σ 在显著减小模型大小（0.6B 参数）的同时，实现了比现有文本到图像扩散模型，如 SDXL (2.6B 参数) 和 SD Cascade (5.1B 参数)，更优的图像质量和对用户提示的遵循能力。此外，PixArt-Σ 生成 4K 图像的能力，为电影和游戏等行业生产高分辨率的海报和壁纸提供了有效支持。

# Teaching Large Language Models to Reason with Reinforcement Learning
[用强化学习让大语言模型 (LLMs) 学会推理](https://arxiv.org/abs/2403.04642)

基于人类反馈的强化学习 (RLHF) 已经成为让 LLM 输出与人类偏好相匹配的主流方法。受 RLHF 成功的启发，我们研究了多种算法（包括专家迭代、近端策略优化 (PPO)、条件回报 RL）在提升 LLM 推理能力方面的效果。这些算法处理的奖励既包括稀疏奖励也包括密集奖励，既有通过启发式方法获得的，也有通过学习获得的奖励模型。研究还涉及了多种模型尺寸和初始化状态，考察了使用和不使用监督微调 (SFT) 数据的情形。结果显示，所有算法的表现相当，其中专家迭代在大多数情况下效果最佳。令人意外的是，专家迭代的样本复杂度与 PPO 相似，从一个预训练的检查点开始，最多需要约10^6个样本就能收敛。我们分析了这一现象的原因，认为在 RL 训练过程中，模型很难探索出超越 SFT 模型已经找到的解决方案。此外，我们还讨论了 SFT 训练中的 maj@1 和 pass@96 指标之间的权衡，以及 RL 训练如何同时改善这两个指标。最后，我们讨论了这些发现对 RLHF 以及强化学习在未来 LLM 微调中角色的影响。