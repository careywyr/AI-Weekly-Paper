## VITA: Towards Open-Source Interactive Omni Multimodal LLM
[VITA：迈向开源交互式全模态大语言模型](https://arxiv.org/abs/2408.05211)

GPT-4o 的卓越多模态能力和交互体验，凸显了其在实际应用中的重要性，但开源模型在这两个领域的表现往往不尽人意。本文介绍的 VITA，是首个开源的多模态大语言模型（MLLM），能够同时处理和分析视频、图像、文本和音频，并提供先进的多模态交互体验。我们从 Mixtral 8x7B 这一语言模型基础出发，扩展其中文词汇，并进行了双语指令调整。通过多模态对齐和指令调整的两阶段多任务学习，我们进一步赋予了该模型视觉和听觉能力。VITA 在多种单模态和多模态基准测试中表现出色，显示出其强大的多语言、视觉和听觉理解能力。此外，我们在提升自然多模态人机交互体验方面取得了显著进展，首次在 MLLM 中实现了非唤醒交互和音频中断功能。VITA 标志着开源社区在探索多模态理解和交互无缝集成方面迈出了第一步。尽管 VITA 仍需大量工作以接近闭源模型的水平，但我们相信其作为先驱的角色将为后续研究奠定坚实基础。项目页面：https://vita-home.github.io。

## Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2
[Gemma Scope: 在Gemma 2上全面开放稀疏自编码器](https://arxiv.org/abs/2408.05147)

稀疏自编码器（SAEs）是一种无监督学习方法，旨在将神经网络的潜在表示分解为稀疏且看似可解释的特征。尽管近期对其潜力充满期待，但行业外的研究应用因训练一套全面SAE的高成本而受限。在本研究中，我们推出了Gemma Scope，这是一个包含JumpReLU SAE的开放套件，针对Gemma 2 2B和9B的所有层及子层，以及Gemma 2 27B基础模型的选定层进行训练。我们主要在Gemma 2预训练模型上训练SAEs，并额外发布了在指令调优的Gemma 2 9B上训练的SAEs，以便进行比较。我们依据标准指标对每个SAE的质量进行了评估，并公开了这些结果。我们期望通过发布这些SAE权重，能够助力社区更便捷地开展更大胆的安全性和可解释性研究。权重和教程可访问https://huggingface.co/google/gemma-scope，互动演示则可在https://www.neuronpedia.org/gemma-scope找到。

## mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models
[mPLUG-Owl3: 面向多模态大语言模型中的长图像序列理解](https://arxiv.org/abs/2408.04840)

多模态大语言模型 (MLLMs) 在执行单图像任务指令方面已显示出显著的能力。然而，在建模长图像序列方面仍面临重大挑战。本研究中，我们引入了多功能的多模态大语言模型 mPLUG-Owl3，该模型在结合检索图像文本知识、交错图像文本和长视频场景中，增强了长图像序列的理解能力。具体来说，我们提出了新颖的超注意力块，有效地将视觉和语言整合到共同的语言引导语义空间中，从而促进了扩展多图像场景的处理。广泛的实验结果显示，mPLUG-Owl3 在单图像、多图像和视频基准测试中，在类似规模的模型中达到了最先进的性能。此外，我们提出了一项名为干扰抵抗的挑战性长视觉序列评估，以评估模型在干扰中保持聚焦的能力。最后，通过提出的架构，mPLUG-Owl3 在超长视觉序列输入上展现了卓越的性能。我们希望 mPLUG-Owl3 能够为开发更高效和强大的多模态大语言模型做出贡献。

## The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery
[AI 科学家：迈向全自动开放式科学发现](https://arxiv.org/abs/2408.06292)

通用人工智能的重大挑战之一是开发能够进行科学研究和发现新知识的智能体。尽管前沿模型已被用作人类科学家的辅助工具，例如用于激发创意、编写代码或执行预测任务，但它们仅执行科学过程的一小部分。本文提出了首个全面的全自动科学发现框架，使前沿大语言模型能够独立开展研究并交流其发现。我们介绍了 AI 科学家，它能够生成新颖的研究想法、编写代码、执行实验、可视化结果、通过撰写完整的科学论文描述其发现，并随后运行模拟评审过程进行评估。原则上，这一过程可以重复进行，以开放式方式迭代开发想法，类似于人类科学界的行为。我们通过将其应用于机器学习的三个不同子领域——扩散模型、基于 Transformer 的语言建模和学习动力学——来展示其广泛适用性。每个想法的实施和开发成完整的论文成本不到 15 美元。为了评估生成的论文，我们设计并验证了一个自动化评审器，该评审器在评估论文分数方面达到了接近人类的表现。AI 科学家能够产出超过顶级机器学习会议接受阈值的论文，这是由我们的自动化评审器评判的。这种方法标志着机器学习领域科学发现的新时代的开始：将 AI 智能体的变革性益处带给 AI 自身的整个研究过程，并使我们更接近于一个能够释放无限可负担创造力和创新的世界，以解决世界上最具挑战性的问题。我们的代码已在 [https://github.com/SakanaAI/AI-Scientist](https://github.com/SakanaAI/AI-Scientist) 开源。

## Med42-v2: A Suite of Clinical LLMs
[Med42-v2: 一套临床大语言模型](https://arxiv.org/abs/2408.06142)
Med42-v2 推出了一套专门针对医疗保健环境中通用模型局限性的临床大语言模型 (LLMs)。这些模型采用 Llama3 架构，并利用专业临床数据进行精细调整。通过多阶段的偏好对齐训练，它们能够有效地回应自然提示。与通用模型不同，通用模型通常为了避免回答临床查询而进行偏好对齐，Med42-v2 则专门设计来克服这一限制，使其在临床环境中得以应用。与原始的 Llama3 模型在 8B 和 70B 参数配置以及 GPT-4 相比，Med42-v2 模型在各种医学基准测试中展现出更优异的性能。这些 LLMs 旨在深入理解临床查询、执行复杂的推理任务，并在临床环境中提供宝贵的辅助。这些模型现已公开发布在 https://huggingface.co/m42-health{https://huggingface.co/m42-health}。

## Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers
[互推理增强小型大语言模型的问题解决能力](https://arxiv.org/abs/2408.06195)

本文介绍了一种名为 rStar 的自博弈互推理方法，该方法显著提升了小型语言模型 (SLM) 的推理能力，无需进行模型微调或依赖更高级的模型。rStar 将推理过程分解为自博弈的互生成与判别过程。首先，目标 SLM 通过引入一系列类似人类的推理策略，增强了蒙特卡洛树搜索 (MCTS)，从而构建出更高质量的推理路径。随后，另一个与目标 SLM 能力相当的 SLM 作为判别器，对目标 SLM 生成的每条推理路径进行验证。经过双方确认的推理路径被认为是相互一致的，因此更可能正确。在五个不同的小型语言模型上进行的广泛实验显示，rStar 能有效解决包括 GSM8K、GSM-Hard、MATH、SVAMP 和 StrategyQA 在内的多样化推理问题。特别地，rStar 将 LLaMA2-7B 在 GSM8K 上的准确率从 12.51% 提升至 63.91%，Mistral-7B 从 36.46% 提升至 81.88%，LLaMA3-8B-Instruct 从 74.53% 提升至 91.13%。相关代码将在 [https://github.com/zhentingqi/rStar](https://github.com/zhentingqi/rStar) 上提供。

## ControlNeXt: Powerful and Efficient Control for Image and Video Generation
[ControlNeXt: 图像与视频生成的强大与高效控制](https://arxiv.org/abs/2408.06070)

扩散模型在图像和视频生成领域展现出显著且稳定的性能。为了提升生成结果的可控性，研究者们引入了诸如ControlNet、Adapters和ReferenceNet等额外架构以加强条件控制。然而，现有的可控生成技术通常需要大量额外计算资源，尤其是在视频生成场景中，且在训练过程中存在诸多挑战或控制力度不足。本文提出ControlNeXt：一种既强大又高效的可控图像与视频生成技术。我们首先构建了一种更为简洁高效的新架构，该架构在保持与基础模型相近成本的同时，有效替代了原先繁重的额外模块。这种精简设计还使得我们的方法能与LoRA权重无缝结合，从而在不增加训练负担的前提下实现风格转换。在训练效率方面，我们成功将可学习参数数量削减至其他方案的10%以下。此外，我们还创新性地提出了交叉归一化（CN）技术，用以取代传统的“零卷积”方法，从而加速并稳定训练过程。通过在不同基础模型上对图像和视频进行广泛实验，我们验证了ControlNeXt方法的稳健性能。

## CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer
[CogVideoX: 基于专家 Transformer 的文本到视频扩散模型](https://arxiv.org/abs/2408.06072)

我们介绍 CogVideoX，这是一个专为基于文本提示生成视频设计的大规模扩散 Transformer 模型。为高效建模视频数据，我们采用 3D 变分自编码器 (VAE) 压缩视频的空间和时间维度。为提升文本与视频的对齐，我们设计了一个配备专家自适应 LayerNorm 的专家 Transformer，以深化两种模态的融合。借助渐进式训练技术，CogVideoX 能生成连贯且具有显著运动特征的长时视频。此外，我们构建了一套包括多种预处理策略和视频字幕方法的文本到视频数据处理流程，大幅提升了 CogVideoX 的性能，不仅提高了生成质量，还增强了语义对齐。结果表明，CogVideoX 在多个机器指标和人类评估中均达到最先进水平。3D 因果 VAE 和 CogVideoX 的模型权重已在 https://github.com/THUDM/CogVideo 公开发布。

## LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs
[LongWriter: 释放长上下文大语言模型中的超长文本生成能力](https://arxiv.org/abs/2408.07055)

当前的长上下文大语言模型 (LLMs) 虽能处理高达100,000个 Token 的输入，但在生成超过2,000字的输出时仍显不足。通过精心设计的实验，我们发现模型的生成能力受限于其在监督微调 (SFT) 阶段所接触的样本长度。简言之，现有 SFT 数据集中缺乏长文本输出示例是导致这一限制的主因。为突破此局限，我们创新性地提出了 AgentWrite 智能体流水线，该流水线能将超长文本生成任务细化为多个子任务，从而使标准 LLMs 能够生成超过20,000字的连贯文本。基于 AgentWrite，我们构建了 LongWriter-6k 数据集，内含6,000个 SFT 数据，输出长度涵盖2k至32k字。通过将此数据集融入模型训练，我们成功将现有模型的输出能力提升至超过10,000字，且保持了输出质量。此外，我们还推出了 LongBench-Write 基准，用于全面评估超长文本生成性能。经 DPO 优化的9B参数模型，在该基准测试中表现卓越，超越了诸多规模更大的专有模型。总体而言，我们的研究表明，现有长上下文 LLMs 已具备显著提升输出长度的潜力，关键在于在模型对齐过程中利用更丰富的长文本输出数据进行训练。相关代码与模型已公开于：https://github.com/THUDM/LongWriter。

## Imagen 3
[Imagen 3](https://arxiv.org/abs/2408.07009)

本文介绍 Imagen 3，这是一种能够根据文本提示生成高质量图像的潜在扩散模型。我们详细阐述了该模型的质量和责任评估结果。在当前的评估中，Imagen 3 的表现超越了其他所有最先进 (SOTA) 模型。此外，我们还探讨了与模型安全性和表示相关的议题，并介绍了我们采取的措施以尽可能减少模型可能带来的潜在风险。

## Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents
[多样性赋能智能：集成软件工程智能体的专业知识](https://arxiv.org/abs/2408.07060)

大语言模型（LLM）智能体在解决现实世界软件工程（SWE）问题方面展现出巨大潜力。最先进的开源SWE智能体在SWE-Bench Lite中能够解决超过27%的真实GitHub问题。然而，这些复杂的智能体框架在不同任务中表现出不同的优势，擅长某些任务而在其他任务中表现不佳。为了充分利用这些智能体的多样性，我们提出了DEI（Diversity Empowered Intelligence），这是一个利用它们独特专业知识的框架。DEI作为现有SWE智能体框架之上的元模块（meta-module），管理智能体集体以增强问题解决能力。实验结果显示，DEI引导的智能体委员会能够大幅超越最佳个体智能体的性能。例如，一组开源SWE智能体，在SWE-Bench Lite上的最高个体解决率为27.3%，通过DEI可以实现34.3%的解决率，提升了25%，并超越了大多数闭源解决方案。我们表现最佳的群体以55%的解决率脱颖而出，在SWE-Bench Lite上获得最高排名。我们的研究成果为日益增长的关于协作AI系统（collaborative AI systems）及其解决复杂软件工程挑战潜力的研究做出了贡献。

## Layerwise Recurrent Router for Mixture-of-Experts
[层级循环路由器用于专家混合模型](https://arxiv.org/abs/2408.06793)

大语言模型 (LLMs) 的规模扩展已经彻底提升了它们在多种任务中的性能，然而这种增长必须与高效的计算策略相匹配。专家混合 (MoE) 架构因其能够在不显著增加训练成本的情况下扩展模型大小而脱颖而出。尽管有这些优势，当前的 MoE 模型往往表现出参数效率低下。例如，一个具有 520 亿参数的基于 MoE 的预训练 LLM 可能与一个具有 67 亿参数的标准模型表现相当。作为 MoE 的关键部分，当前不同层的路由器独立分配 Token，没有利用历史路由信息，可能导致 Token 与专家的非最优组合和参数效率问题。为了缓解这一问题，我们引入了层级循环路由器用于专家混合模型 (RMoE)。RMoE 利用门控循环单元 (GRU) 在连续层之间的路由决策中建立依赖关系。这种层级循环可以高效地并行计算输入 Token，并引入可协商的成本。我们广泛的实证评估表明，基于 RMoE 的语言模型始终优于多种基准模型。此外，RMoE 集成了一种与现有方法正交的新计算阶段，允许与其它 MoE 架构完美兼容。我们的分析将 RMoE 的收益归因于其有效的跨层信息共享，这也改善了专家选择和多样性。我们的代码位于 https://github.com/qiuzh20/RMoE

## DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search
[DeepSeek-Prover-V1.5：利用证明助手反馈进行强化学习和蒙特卡洛树搜索](https://arxiv.org/abs/2408.08152)

我们推出 DeepSeek-Prover-V1.5，一款专为 Lean 4 定理证明设计的开源语言模型，通过优化训练与推理流程，提升了 DeepSeek-Prover-V1 的性能。该模型在 DeepSeekMath-Base 上预训练，专注于形式化数学语言，并利用从 DeepSeek-Prover-V1 衍生出的增强型形式化定理证明数据集进行监督微调。此外，通过从证明助手反馈中进行强化学习（RLPAF），进一步提升了模型的精细度。不同于 DeepSeek-Prover-V1 的单次全证明生成方式，我们引入了 RMaxTS，这是一种基于内在奖励驱动探索策略的蒙特卡洛树搜索变体，旨在生成多样化的证明路径。DeepSeek-Prover-V1.5 在 DeepSeek-Prover-V1 的基础上取得了显著进步，在高中级别的 miniF2F 基准测试集（63.5%）和本科级别的 ProofNet 基准测试集（25.3%）上刷新了最先进记录。

## Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents
[Agent Q: 自主 AI 智能体的高级推理与学习](https://arxiv.org/abs/2408.07199)

大语言模型 (LLM) 在涉及复杂推理的自然语言任务中展现出卓越的能力，但在交互环境中进行智能体的多步推理仍然面临巨大挑战。传统的静态数据集上的监督预训练不足以赋予自主智能体在动态环境（如网页导航）中进行复杂决策的能力。以往通过在精心挑选的专家示范数据上进行监督微调来弥合这一差距的尝试，常常因累积误差和有限的探索数据而失败，导致策略效果不佳。为了解决这些问题，我们提出了一个框架，该框架将引导式蒙特卡洛树搜索 (MCTS) 与自我批评机制结合，并通过一种离线策略的直接偏好优化 (DPO) 算法对智能体交互数据进行迭代微调。我们的方法使得 LLM 智能体能够从成功和失败的轨迹中有效学习，从而提升其在复杂多步推理任务中的泛化能力。我们在 WebShop 环境（一个模拟电子商务平台）中验证了我们的方法，该方法的表现持续优于行为克隆和强化微调的基线模型，并且在具备在线搜索功能时，表现超越了平均人类水平。在实际预订场景中，我们的方法将 Llama-3 70B 模型的零样本任务表现从 18.6% 提升至 81.7% 的成功率（相对提升 340%），并通过在线搜索进一步提升至 95.4%。我们相信，这代表了自主智能体能力的重大突破，为在现实世界中进行更复杂和可靠的决策铺平了道路。
