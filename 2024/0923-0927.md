## Imagine yourself: Tuning-Free Personalized Image Generation
[想象你自己：无调优个性化图像生成](https://arxiv.org/abs/2409.13346)

扩散模型在多种图像生成任务中表现出色。本研究中，我们推出了“想象你自己”，一种无需调优的个性化图像生成模型。与依赖调优的传统方法不同，“想象你自己”允许所有用户共享同一框架，无需个体调整。此前，模型在身份保持、复杂提示遵循和视觉质量之间难以平衡，常导致对参考图像的过度依赖，难以生成需要大幅改变参考图像的图像，如改变表情、姿态等，且生成图像的多样性有限。为此，我们提出了1) 新的合成配对数据生成机制以增强多样性，2) 包含三个文本编码器和可训练视觉编码器的全并行注意力架构，以提升文本忠实度，以及3) 从粗到细的多阶段微调策略，逐步提升视觉质量。实验表明，“想象你自己”在身份保持、视觉质量和文本对齐方面均优于现有最先进模型，为个性化应用奠定了坚实基础。人类评估结果显示，该模型在身份保持、文本忠实度和视觉吸引力方面均优于以往模型。

## YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models
[YesBut: 高质量多模态数据集用于评估视觉-语言模型的讽刺理解能力](https://arxiv.org/abs/2409.13592)

理解讽刺和幽默对视觉-语言模型来说是一项艰巨任务。本文中，我们提出了三项挑战性任务：讽刺图像检测（判断图像是否具有讽刺意味）、讽刺理解（解释图像为何具有讽刺意味）和讽刺图像完成（给定一半图像，从两个选项中选择另一半，使完整图像具有讽刺意味）。我们发布了一个高质量数据集 YesBut，包含 2547 张图像，其中 1084 张为讽刺图像，1463 张为非讽刺图像，涵盖多种艺术风格，用于评估这些任务。每张讽刺图像描绘了一个正常场景，同时包含一个有趣或讽刺的冲突场景。尽管视觉-语言模型在多模态任务（如视觉问答和图像描述）上表现出色，但在 YesBut 数据集的提议任务中，零样本设置下，无论是自动化还是人工评估，这些模型的表现均不佳。此外，我们还发布了一个包含 119 张真实讽刺照片的数据集，供进一步研究使用。数据集和代码可在 https://github.com/abhi1nandy2/yesbut_dataset 获取。

## Prithvi WxC: Foundation Model for Weather and Climate
[Prithvi WxC: 天气和气候的基础模型](https://arxiv.org/abs/2409.13598)

随着 AI 模拟器性能的提升，其已能与传统数值天气预报模型相媲美，尤其是在 HPC 系统上运行的模型。因此，越来越多的 AI 大模型开始应用于预测、降尺度或临近预报等场景。尽管 AI 领域的研究正朝着基础模型（这些模型能够通过调整解决多种不同用例）的方向发展，但天气和气候领域的模型仍主要针对单一用例，尤其是中期预报。为此，我们推出了 Prithvi WxC，这是一个基于现代时代研究与应用回顾分析第 2 版 (MERRA-2) 中 160 个变量开发的 23 亿参数基础模型。Prithvi WxC 采用编码器-解码器架构，融合了多种 Transformer 模型的设计理念，能够有效捕捉输入数据中的区域和全球依赖关系。该模型设计支持大量 Token，以便在不同拓扑结构中以高分辨率模拟天气现象。此外，它通过结合掩码重建和预测的范式进行混合目标训练。我们在一系列具有挑战性的下游任务上测试了该模型，包括自回归滚动预测、降尺度、重力波通量参数化和极端事件估计。目前，具有 23 亿参数的预训练模型及其微调工作流程已通过 Hugging Face 作为开源项目公开发布。

## RACER: Rich Language-Guided Failure Recovery Policies for Imitation Learning
[RACER: 丰富的语言引导故障恢复策略](https://arxiv.org/abs/2409.14674)

由于缺乏自我恢复机制和简单语言指令在指导机器人动作方面的局限性，开发稳健且可纠正的视觉运动策略用于机器人操作是一项挑战。为解决这些问题，我们提出了一种可扩展的数据生成管道，自动将专家演示与故障恢复轨迹及细粒度语言注释结合，用于训练。

我们引入了丰富的语言引导故障恢复 (RACER)，这是一个监督者-执行者框架，结合故障恢复数据和丰富语言描述以增强机器人控制。RACER 包含一个视觉语言模型 (VLM)，作为在线监督者提供详细语言指导以进行错误纠正和任务执行，以及一个语言条件化的视觉运动策略作为执行者来预测下一步动作。

实验结果显示，RACER 在多种评估设置下，包括标准长时任务、动态目标变化任务和零样本未见任务，均优于最先进的 Robotic View Transformer (RVT)，在模拟和真实环境中均表现卓越。视频和代码可在以下网址获取：https://rich-language-failure-recovery.github.io。

## A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?
[o1医学应用初探：迈向AI医生？](https://arxiv.org/abs/2409.15277)

大语言模型 (LLMs) 在多领域展示了卓越能力，拓展了学习和认知的边界。OpenAI的o1作为首个采用强化学习策略的内部化链式思维技术的大语言模型，表现突出。尽管在通用语言任务上表现出色，其在医学等专业领域的性能仍待验证。为此，本报告全面探索了o1在不同医疗场景中的表现，重点考察了理解、推理和多语言能力三个方面。

具体而言，我们评估了6项任务，使用了37个医疗数据集，包括基于《新英格兰医学杂志》(NEJM) 和《柳叶刀》专业测验的两个新建且更具挑战性的问答 (QA) 任务。与标准医疗QA基准（如MedQA）相比，这些数据集更具临床相关性，更能有效转化为实际临床效用。o1分析显示，LLMs增强的推理能力可能显著提升其理解医疗指令和推理复杂临床场景的能力。值得注意的是，o1在19个数据集和两个新建复杂QA场景中的平均准确率分别比GPT-4高出6.2%和6.6%。

但同时，我们发现模型能力和现有评估协议存在幻觉、多语言能力不一致及评估指标不一致等弱点。我们将在https://ucsc-vlaa.github.io/o1_medicine/发布原始数据和模型输出，供未来研究使用。

## Phantom of Latent for Large Language and Vision Models
[大型语言与视觉模型的潜在幻影](https://arxiv.org/abs/2409.14713)

视觉指令调优的成功极大地推动了大型语言与视觉模型 (LLVM) 的发展。根据指令调优大型语言模型 (LLM) 的扩展规律，LLVM 的规模进一步扩大，达到了 26B、34B 甚至 80B 参数。虽然这种规模的增加显著提升了模型性能，但也对训练和推理所需的硬件资源提出了更高要求。因此，高效 LLVM 的需求日益迫切，这些模型需要在保持较小规模的同时，达到更大模型的性能。为此，我们提出了一种新的高效 LLVM 系列，模型规模为 0.5B、1.8B、3.8B 和 7B 参数，名为 Phantom，它在有限结构内显著提升了学习能力。通过在多头自注意力机制 (MHSA) 期间暂时增加潜在隐藏维度，Phantom 能够在不显著增加物理模型规模的情况下，准备处理和理解更多的视觉语言知识。为了充分发挥其优势，我们引入了 Phantom 优化 (PO)，结合自回归监督微调 (SFT) 和类似直接偏好优化 (DPO) 的概念，从而在消除错误和模糊答案的同时，有效遵循正确答案。Phantom 在众多开源和闭源的更大规模 LLVM 中表现优异，成为高效 LLVM 领域的领先解决方案。

## HelloBench: Evaluating Long Text Generation Capabilities of Large Language Models
[HelloBench: 评估大语言模型的长文本生成能力](https://arxiv.org/abs/2409.16191)

近年来，大语言模型 (LLMs) 在长上下文理解等任务中展示了显著能力，并提出了众多基准测试。然而，我们发现长文本生成能力研究不足。为此，我们引入了分层长文本生成基准测试 (HelloBench)，这是一个综合、真实且开放式的基准测试，旨在评估大语言模型在生成长文本方面的性能。借鉴布鲁姆的分类法，HelloBench 将长文本生成任务细分为五个子任务：开放式问答、摘要、聊天、文本补全和启发式文本生成。此外，我们提出了分层长文本评估 (HelloEval)，这是一种与人类评估高度一致的方法，显著减少了人类评估所需的时间和精力，同时保持了与人类评估的高度相关性。我们在约 30 个主流大语言模型上进行了广泛实验，发现当前大语言模型在长文本生成方面存在不足。具体来说，首先，无论指令是否包含显式或隐式的长度限制，大多数大语言模型无法生成超过 4000 字的文本。其次，虽然一些大语言模型能生成更长文本，但存在诸多问题，如严重重复和质量下降。第三，为展示 HelloEval 的有效性，我们将其与传统指标 (如 ROUGE、BLEU 等) 和大语言模型作为评判方法进行了比较，结果表明 HelloEval 与人类评估的相关性最高。我们在 https://github.com/Quehry/HelloBench 发布了我们的代码。

## MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling
[MIMO: 可控角色视频合成与空间分解建模](https://arxiv.org/abs/2409.16160)

角色视频合成旨在生成动画角色在真实场景中的逼真视频。作为计算机视觉和图形学领域的一个基础问题，3D 作品通常需要多视角捕捉来进行逐个案例的训练，这严重限制了其在短时间内对任意角色建模的适用性。最近的 2D 方法通过预训练的扩散模型打破了这一限制，但它们在姿态通用性和场景交互方面存在局限。为此，我们提出了 MIMO，一种新颖的框架，它不仅能够根据用户提供的简单输入（即角色、动作和场景）合成具有可控属性的角色视频，而且还能在一个统一框架内同时实现对任意角色的高级可扩展性、对新颖 3D 动作的通用性以及对交互式真实世界场景的适用性。核心思想是将 2D 视频编码为紧凑的空间代码，考虑到视频内容的固有 3D 特性。具体来说，我们使用单目深度估计器将 2D 帧像素提升到 3D，并根据 3D 深度将视频片段分解为三个空间组件（即主要人物、底层场景和浮动遮挡）在层次结构中进行分解。这些组件进一步编码为规范身份代码、结构化运动代码和完整场景代码，这些代码作为合成过程的控制信号。空间分解建模的设计使得用户控制灵活、复杂动作表达以及场景交互的 3D 感知的合成成为可能。实验结果证明了所提出方法的有效性和鲁棒性。

## Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models
[Molmo 和 PixMo：为最先进的跨模态模型提供开放权重和开放数据](https://arxiv.org/abs/2409.17146)

目前，最先进的跨模态模型仍为私有。最强大的开放权重模型严重依赖于从私有视觉语言模型 (VLM) 中获取的合成数据，以实现良好性能，实质上将这些封闭模型转化为开放模型。因此，社区仍缺乏关于如何从头构建高性能 VLM 的基础知识。我们提出了 Molmo，这是一个新的 VLM 家族，在开放性方面处于领先地位。我们的核心创新是一个全新、高度详细的图像描述数据集，该数据集完全由人类标注者使用基于语音的描述收集。为支持多样化的用户交互，我们还引入了一个包含自然环境中问答和创新二维指向数据的多样化微调数据集。我们方法的成功依赖于对模型架构细节的精心选择、精心调优的训练流程，以及最关键的，我们新收集数据集的高质量，这些都将被公开发布。Molmo 家族中最好的 72B 模型不仅在开放权重和数据模型中表现优异，而且在学术基准和人类评估中与 GPT-4o、Claude 3.5 和 Gemini 1.5 等私有系统相比也毫不逊色。
我们将在不久的将来发布所有模型权重、描述和微调数据以及源代码。部分模型权重、推理代码和演示可在 https://molmo.allenai.org 获取。

## Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale
[编程每个示例：大规模提升预训练数据质量如专家般](https://arxiv.org/abs/2409.17115)

大语言模型预训练传统上依赖人类专家制定启发式方法以提高语料库质量，至今已积累了大量规则。然而，这些规则缺乏灵活性，难以有效应对每个示例的独特特征。同时，为每个示例应用定制规则对人类专家来说是不切实际的。

本文中，我们证明即使是参数数量仅0.3B的小型语言模型，也能展现出与人类专家相媲美的显著数据提炼能力。我们引入了编程每个示例（ProX），这是一种新颖的框架，将数据提炼视为编程任务，使模型能够通过生成和执行细粒度操作（如字符串归一化）来大规模提炼语料库中的每个个体示例。

实验结果表明，使用ProX精选数据预训练的模型在各种下游基准测试中均优于原始数据或其他筛选方法的数据，提升幅度超过2%。其有效性涵盖了各种模型规模和预训练语料库，包括C4、RedPajama-V2和FineWeb。

此外，ProX在特定领域的持续预训练中展现出巨大潜力：无需特定领域设计，使用ProX提炼的OpenWebMath训练的模型在平均准确率上超越了基于人类手工规则的方法，Mistral-7B提升了7.6%，Llama-2-7B提升了14.6%，CodeLlama-7B提升了20.3%，所有这些都在10B Token内达到了与在200B Token上训练的Llemma-7B相当的水平。

进一步分析表明，ProX显著节省了训练FLOPs，为高效的LLM预训练提供了有前景的路径。我们正在开源ProX，包含超过100B的语料库、模型，并分享所有训练和实现细节，以促进可重复研究和未来创新。代码：https://github.com/GAIR-NLP/ProX

## MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models
[MaskLLM: 大语言模型的可学习半结构化稀疏性](https://arxiv.org/abs/2409.17481)

大语言模型 (LLMs) 因其庞大的参数数量而闻名，这些参数往往带来显著的冗余。本研究提出了 MaskLLM，一种可学习的剪枝方法，通过在 LLMs 中引入半结构化 (或“N:M”) 稀疏性，旨在减少推理过程中的计算开销。MaskLLM 不采用新的重要性标准，而是通过 Gumbel Softmax 采样将 N:M 模式建模为可学习的分布。这种方法便于在大规模数据集上进行端到端训练，并具有两大优势：1) 高质量掩码 - 我们的方法能有效扩展至大数据集，并学习到准确的掩码；2) 可迁移性 - 掩码分布的概率建模使得稀疏性能够在不同领域或任务间进行迁移学习。我们使用 2:4 稀疏性对 MaskLLM 在多种 LLMs 上进行了评估，包括 LLaMA-2、Nemotron-4 和 GPT-3，参数规模从 843M 到 15B 不等，实证结果表明，相较于最先进的方法，MaskLLM 有显著提升。例如，现有最优方法在 Wikitext 上的困惑度 (PPL) 达到 10 或更高，而密集模型的 PPL 为 5.12，但 MaskLLM 仅通过学习掩码和冻结权重就实现了显著更低的 6.72 PPL。此外，MaskLLM 的可学习特性使得能够为下游任务或领域定制无损应用的 2:4 稀疏性掩码。代码可在 https://github.com/NVlabs/MaskLLM 获取。

