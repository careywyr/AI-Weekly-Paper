## How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data
[你的代码大语言模型表现如何？利用高质量数据赋能代码指令微调](https://arxiv.org/abs/2409.03810)

最近，研究如何构建更好的代码指令微调数据集的兴趣逐渐增加。然而，我们观察到使用这些数据集训练的代码模型在HumanEval上表现优异，但在其他基准测试如LiveCodeBench上表现不佳。进一步调查显示，许多数据集存在严重的数据泄露问题。在清理大部分泄露数据后，一些知名的高质量数据集表现不理想。这一发现揭示了一个新挑战：识别哪些数据集真正符合高质量代码指令数据的资格。为此，我们提出了一种高效的代码数据修剪策略，用于选择优质样本。我们的方法从三个维度出发：指令复杂性、响应质量和指令多样性。基于我们选择的数据，我们推出了XCoder，一系列从LLaMA3微调而来的模型。我们的实验表明，XCoder在使用较少训练数据的情况下达到了新的最佳性能，验证了我们数据策略的有效性。此外，我们对数据组成进行了深入分析，发现现有代码数据集根据其构建方法具有不同的特征，为未来的代码大语言模型提供了新的见解。我们的模型和数据集已在https://github.com/banksy23/XCoder发布。

## Configurable Foundation Models: Building LLMs from a Modular Perspective
[可配置基础模型：从模块化视角构建大语言模型](https://arxiv.org/abs/2409.02877)

大语言模型的进步带来了计算效率和持续扩展性的挑战，因其需要大量参数，使得在资源受限和多能力需求的场景中应用和演进变得复杂。受人类大脑模块化结构的启发，将大语言模型分解为多个功能模块的趋势日益明显，允许部分模块推理并动态组装以应对复杂任务，如专家混合。为强调模块化方法的效率和可组合性，我们提出“砖块”概念代表每个功能模块，并将这种模块化结构称为可配置基础模型。本文全面探讨了可配置基础模型的构建、应用及其局限性。首先，我们将模块形式化为预训练阶段出现的功能神经分区（涌现砖块）和通过额外训练构建的定制砖块，以增强大语言模型的能力和知识。基于多种功能砖块，我们提出了四种面向砖块的操作：检索与路由、合并、更新和增长，这些操作允许根据指令动态配置大语言模型以处理复杂任务。为验证这一观点，我们对广泛使用的大语言模型进行了实证分析，发现前馈神经网络层具有模块化模式，神经元具备功能特化和分区。最后，我们指出了几个开放问题和未来研究方向。本文旨在为现有大语言模型研究提供新的模块化视角，并激发未来创建更高效和可扩展的基础模型。

## Towards a Unified View of Preference Learning for Large Language Models: A Survey
[大语言模型偏好学习统一视图：综述](https://arxiv.org/abs/2409.02795)

大语言模型 (LLM) 展现出极其强大的能力。其中，使 LLM 输出与人类偏好对齐是成功的关键因素之一。这一对齐过程通常仅需少量数据，即可显著提升 LLM 性能。尽管效果显著，但该领域的研究涉及多个复杂领域，不同方法之间的关系尚未充分探索，限制了偏好对齐的发展。为此，我们将现有对齐策略分解为模型、数据、反馈和算法四个组成部分，并构建统一框架，以揭示它们之间的联系。这一统一视图不仅深化了对现有对齐算法的理解，还为整合不同策略优势提供了可能。此外，我们通过详细示例，帮助读者全面理解现有流行算法。最后，基于这一统一视角，我们探讨了将大语言模型与人类偏好对齐的挑战及未来研究方向。

## MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct
[MMEvol: 通过 Evol-Instruct 赋能多模态大语言模型](https://arxiv.org/abs/2409.05840)

多模态大语言模型 (MLLMs) 的发展取得了显著成就。然而，多模态指令数据的数量和质量已成为其发展中的关键障碍。手动创建多模态指令数据既耗时又低效，难以生成高复杂度的指令。此外，从黑箱商业模型（如 GPT-4o, GPT-4V）中提炼指令数据往往导致简单的指令数据，这限制了性能至这些模型的水平。策划多样化和复杂指令数据的挑战依然巨大。我们提出了 MMEvol，一种创新的多模态指令数据进化框架，结合了细粒度感知进化、认知推理进化和交互进化。这种迭代方法突破了数据质量瓶颈，生成复杂且多样化的图文指令数据集，从而增强了 MLLMs 的能力。从初始指令集 SEED-163K 开始，我们利用 MMEvol 系统扩展指令类型的多样性，整合推理步骤以增强认知能力，并从图像中提取详细信息以提高视觉理解和鲁棒性。为了全面评估数据的有效性，我们使用进化数据训练 LLaVA-NeXT，并在 13 个视觉语言任务上进行实验。与使用种子数据训练的基线相比，我们的方法平均准确率提高了 3.1 分，并在其中 9 项任务上达到了最先进 (SOTA) 的性能。

## OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs
[OneGen: 大语言模型的高效一次通过统一生成与检索](https://arxiv.org/abs/2409.05152)

尽管大语言模型 (LLMs) 在最近的进展中显著增强了各种自然语言处理任务的生成能力，但在直接处理检索任务方面仍存在局限。然而，许多实际应用要求检索和生成无缝集成。本文介绍了一种新颖且高效的一次通过生成与检索框架 (OneGen)，旨在提升 LLMs 在需要生成和检索的任务中的性能。所提出的框架通过结合自回归生成的检索 tokens，弥合了生成和检索的传统分离训练方法。这使得单个 LLM 能够在统一的前向传递中同时处理这两项任务。我们在两种不同类型的复合任务（RAG 和实体链接）上进行了实验，以验证 OneGen 在训练和推理中的可插拔性、有效性和效率。此外，我们的结果表明，在同一上下文中集成生成和检索，既保留了 LLMs 的生成能力，又提高了检索性能。据我们所知，OneGen 是首个使 LLMs 在生成过程中进行向量检索的框架。

## LLaMA-Omni: Seamless Speech Interaction with Large Language Models
[LLaMA-Omni: 与大语言模型无缝语音交互](https://arxiv.org/abs/2409.06666)

GPT-4o 等模型通过语音实现与大语言模型 (LLMs) 的实时交互，显著提升了用户体验，相较于传统的文本交互方式。然而，基于开源 LLMs 构建语音交互模型的研究仍显不足。为此，我们提出了 LLaMA-Omni，这是一种专为低延迟和高品质语音交互设计的新型模型架构。LLaMA-Omni 集成了预训练的语音编码器、语音适配器、大语言模型 (LLM) 和流式语音解码器。它无需语音转录，能够以极低延迟直接从语音指令生成文本和语音响应。我们的模型基于最新的 Llama-3.1-8B-Instruct 模型。为适应语音交互场景，我们构建了包含 200K 条语音指令及其对应语音响应的数据集 InstructS2S-200K。实验结果显示，与之前的语音-语言模型相比，LLaMA-Omni 在内容和风格上提供了更优的响应，响应延迟低至 226ms。此外，在仅 4 个 GPU 上训练 LLaMA-Omni 仅需不到 3 天时间，为未来高效开发语音-语言模型奠定了基础。

## GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering
[GroUSE: 评估基于事实问答中评估器的基准](https://arxiv.org/abs/2409.06595)

检索增强生成 (RAG) 已成为一种常见方法，用于将大语言模型 (LLM) 与私有且最新的知识库结合使用。在这项工作中，我们解决了在使用 LLM 作为评判者评估由 RAG 系统生成的基于事实答案时所面临的挑战。为了评估评判模型的校准和辨别能力，我们识别了 7 种生成器失败模式，并引入了 GroUSE（基于事实问答评估器的单元评分），这是一个包含 144 个单元测试的综合评估基准。该基准揭示了现有的自动化 RAG 评估框架往往忽视了重要的失败模式，即使在使用 GPT-4 作为评判者的情况下也是如此。

为了改进当前自动化 RAG 评估框架的设计，我们提出了一种新的流程，并发现尽管闭源模型在 GroUSE 上表现良好，但最先进的开源评判者并未能适应我们提出的标准，尽管它们与 GPT-4 的评判有很强的相关性。我们的研究结果表明，与 GPT-4 的相关性是评判模型实际性能的不完整指标，应通过单元测试的评估来补充，以实现精确的失败模式检测。

我们进一步展示了，对 Llama-3 在 GPT-4 的推理轨迹上进行微调显著提升了其评估能力，在 GPT-4 的评估相关性和参考情境的校准方面都有所改进。

## PingPong: A Benchmark for Role-Playing Language Models with User Emulation and Multi-Model Evaluation
[PingPong: 一个带有用户模拟和多模型评估的角色扮演语言模型基准](https://arxiv.org/abs/2409.06820)

我们引入了一个新的基准，用于评估语言模型的角色扮演能力。我们的方法利用语言模型来模拟动态多轮对话中的用户，并评估生成的对话。该框架由三个主要组件组成：一个扮演特定角色的玩家模型，一个模拟用户行为的审问者模型，以及一个评估对话质量的裁判模型。我们进行了实验，比较了自动化评估与人工注释，以验证方法，展示了在多个标准上的强相关性。这项工作为交互场景中模型能力的稳健和动态评估奠定了基础。

## MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications
[MEDIC: 面向临床应用中大语言模型综合评估的框架](https://arxiv.org/abs/2409.07314)

随着大语言模型 (LLMs) 在医疗应用领域的迅速发展，业界呼吁超越 USMLE 等常用基准，进行更全面的评估，以更准确地反映实际性能。尽管现实世界的评估具有重要价值，但它们往往跟不上 LLM 的演进步伐，可能导致部署时的评估结果已过时。这种时间上的滞后性，要求我们在模型选择前进行全面评估。为此，我们推出了 MEDIC 框架，该框架从医学推理、伦理与偏见、数据与语言理解、上下文学习以及临床安全五个关键维度评估 LLM。MEDIC 引入了一种创新的交叉检验方法，无需参考输出即可量化 LLM 在覆盖率和幻觉检测等方面的表现。我们利用 MEDIC 对 LLM 在医学问答、安全性、摘要生成、笔记生成等任务上进行了评估。结果显示，不同规模、基线与医学微调模型之间存在显著性能差异，这对需要特定模型强度的应用（如低幻觉或低推理成本）的模型选择具有重要指导意义。MEDIC 的多维度评估不仅揭示了这些性能权衡，还弥合了理论能力与实际应用之间的差距，确保能够识别并适应最具潜力的模型，以满足多样化的医疗应用需求。

## DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?
[DSBench: 数据科学智能体距离成为数据科学专家还有多远？](https://arxiv.org/abs/2409.07703)

大语言模型 (LLMs) 和大规模视觉语言模型 (LVLMs) 展示了卓越的语言/视觉推理能力，推动了近期构建购物助手或 AI 软件工程师等特定应用智能体的热潮。近期，众多数据科学基准被提出，以评估这些模型在数据科学领域的表现。然而，现有基准在实际数据科学应用面前，因其简化设置而显得不足。为此，我们推出了 DSBench，一个旨在通过真实任务评估数据科学智能体的综合性基准。DSBench 包含 466 个数据分析任务和 74 个数据建模任务，均来自 Eloquence 和 Kaggle 竞赛。通过涵盖长上下文、多模态任务背景、处理大数据文件和多表结构推理以及执行端到端数据建模任务，DSBench 提供了一个真实的评估环境。我们对最先进的 LLMs、LVLMs 和智能体的评估结果显示，它们在多数任务上表现不佳，最佳智能体仅能解决 34.12% 的数据分析任务，相对性能差距 (RPG) 为 34.74%。这些结果凸显了进一步开发更实用、智能和自主的数据科学智能体的迫切需求。

## Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale
[Windows Agent Arena: 大规模评估多模态操作系统智能体](https://arxiv.org/abs/2409.08264)

大语言模型 (LLMs) 展现出作为计算机智能体的显著潜力，能够显著提升人类生产力和软件在多模态任务中的易用性，这些任务需要规划和推理。然而，在现实环境中衡量智能体性能仍然是一个挑战，原因在于：(i) 大多数基准测试局限于特定模态或领域（例如仅限于文本、网页导航、问答、编码），以及 (ii) 由于任务的多步骤顺序性质，完整的基准测试评估速度缓慢（通常需要数天时间）。为了应对这些挑战，我们引入了 Windows Agent Arena：一个可重复、通用的环境，专注于 Windows 操作系统 (OS)，智能体可以在此环境中自由操作，并使用与人类用户解决任务时相同的广泛应用程序、工具和网络浏览器。我们基于 OSWorld 框架（Xie et al., 2024），创建了 150 多个跨多个代表性领域的多样化 Windows 任务，这些任务需要智能体在规划、屏幕理解和工具使用方面的能力。我们的基准测试是可扩展的，并且可以在 Azure 中无缝并行化，从而在短短 20 分钟内完成完整的基准测试评估。为了展示 Windows Agent Arena 的能力，我们还引入了一个新的多模态智能体，Navi。我们的智能体在 Windows 领域实现了 19.5% 的成功率，而未辅助的人类表现则为 74.5%。Navi 在另一个流行的基于网络的基准测试 Mind2Web 上也展示了强劲的表现。我们提供了 Navi 性能的广泛定量和定性分析，并提供了关于未来在 Windows Agent Arena 中进行智能体开发和数据生成研究的机会的见解。

网页: https://microsoft.github.io/WindowsAgentArena
代码: https://github.com/microsoft/WindowsAgentArena

## Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers
[大语言模型能否孕育创新研究思路？一项涉及百余名 NLP 研究者的大规模研究](https://arxiv.org/abs/2409.04109)

大语言模型的最新进展引发了对其加速科学发现潜力的乐观期待，越来越多的研究开始探索自主生成和验证新思路的研究智能体。然而，迄今为止，尚无评估证实 LLM 系统能够迈出第一步，即产出新颖且达到专家水准的思路，更遑论完成整个研究流程。为此，我们设计了一项实验，旨在控制混杂因素的同时，评估研究思路的生成，并首次将专家 NLP 研究者与 LLM 构思智能体进行直接对比。通过招募超过 100 名 NLP 研究者撰写新颖思路并进行盲评，我们首次获得了关于当前 LLM 研究构思能力的统计显著性结论：LLM 生成的思路在创新性上显著优于人类专家思路 (p < 0.05)，但在可行性上稍显逊色。深入分析我们的智能体基线后，我们发现了构建和评估研究智能体中的若干关键问题，包括 LLM 自我评估的失效及其生成内容的缺乏多样性。最后，我们意识到，即便是专家，对新颖性的判断也可能存在挑战，因此提出了一种端到端的研究设计，邀请研究者将这些思路转化为完整项目，从而探究新颖性与可行性判断是否会对研究成果产生实质性影响。

