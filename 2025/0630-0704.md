## GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning
[GLM-4.1V-Thinking：基于可扩展强化学习的通用多模态推理研究](https://arxiv.org/abs/2507.01006)

本文提出GLM-4.1V-Thinking视觉语言模型(Vision-Language Model, VLM)，旨在提升通用多模态理解与推理能力。我们重点介绍了以推理为核心的新型训练框架的关键研究成果：首先通过大规模预训练构建了具有显著潜力的视觉基础模型，其性能理论上限决定了最终表现；随后提出课程采样强化学习(Reinforcement Learning with Curriculum Sampling, RLCS)方法，充分释放模型潜力，在STEM问题求解、视频理解、内容识别、编程、基础任务、GUI智能体及长文档理解等多样化任务上实现全面能力提升。我们开源了GLM-4.1V-9B-Thinking模型，在同等规模模型中达到最先进水平。在28个公开基准测试中，该模型几乎在所有任务上超越Qwen2.5-VL-7B，并在18个基准测试中与参数量级更大的Qwen2.5-VL-72B表现相当或更优。特别值得注意的是，在长文档理解和STEM推理等挑战性任务上，GLM-4.1V-9B-Thinking相较GPT-4o等闭源模型展现出竞争优势，充分验证了其强大性能。相关代码、模型及详细信息发布于https://github.com/THUDM/GLM-4.1V-Thinking。

## Kwai Keye-VL Technical Report  
[Kwai Keye-VL 技术报告](https://arxiv.org/abs/2507.01949)  

尽管多模态大语言模型 (MLLMs) 在静态图像处理上展现出卓越能力，但其对动态且信息密集的短视频——当今数字生态中的主流媒介形式——的理解仍存在不足。为此，我们推出 \textbf{Kwai Keye-VL}，这是一个拥有 80 亿参数的多模态基础模型，旨在实现短视频理解领域的尖端性能，同时保持强大的通用视觉-语言能力。Keye-VL 的研发基于两大核心支柱：一是规模超过 6000 亿 token（文本单元）的高质量数据集（其中视频数据占重要比重），二是创新的训练方案。该方案包含四阶段预训练以实现稳健的视觉-语言对齐，以及精心设计的双阶段后训练流程：第一阶段强化指令跟随等基础能力，第二阶段重点激活高级推理能力。在后者的关键创新中，我们采用包含“思考”、“非思考”、“auto-think（自动推理模式）”、“带图像思考”及高质量视频数据的五模式“冷启动”数据混合策略，训练模型自主判断推理时机与方式。后续通过强化学习 (RL) 和对齐步骤进一步优化推理能力，并修正重复输出等异常行为。经广泛评估，Keye-VL 在公开视频基准测试中达到业界最优水平，在通用图像任务中同样表现优异（图 1）。此外，我们开发并开源了专为现实短视频场景定制的评测基准 \textbf{KC-MMBench}，Keye-VL 在该基准中展现出显著优势。

## LongAnimation: Long Animation Generation with Dynamic Global-Local Memory
[LongAnimation：基于动态全局-局部记忆的长动画生成](https://arxiv.org/abs/2507.01945)

动画上色是动画产业实际生产中的关键环节。长动画上色面临人力成本高昂的问题。因此，基于视频生成模型的自动化长动画上色具有重要研究价值。现有视频上色研究主要局限于短时处理，这些方法采用局部范式，通过融合重叠特征实现片段间的平滑过渡。然而，局部范式会忽略全局信息，难以维持长期色彩一致性。本研究提出通过动态全局-局部范式实现理想的长期色彩一致性，即动态提取与当前生成过程相关的全局色彩一致特征。具体而言，我们提出LongAnimation框架，其核心包含三个组件：SketchDiT模块、动态全局-局部记忆（DGLM）模块和色彩一致性奖励机制。SketchDiT负责提取混合参考特征以支持DGLM模块；DGLM采用长视频理解模型动态压缩全局历史特征，并自适应地将其与当前生成特征融合；色彩一致性奖励机制用于提升色彩一致性。在推理阶段，我们提出色彩一致性融合方法以平滑视频片段过渡。通过在短时（14帧）和长时（平均500帧）动画上的广泛实验验证，LongAnimation在开放域动画上色任务中能有效保持短期和长期的色彩一致性。代码详见https://cn-makers.github.io/long_animation_web/。

## WebSailor: Navigating Super-human Reasoning for Web Agent
[WebSailor：实现网页智能体的超人类推理导航](https://arxiv.org/abs/2507.02592)

突破人类认知极限是大语言模型（LLM）训练的重要方向。DeepResearch等专有智能体系统已在BrowseComp等超高难度信息检索基准上展现出超越人类的表现，这是此前未能实现的技术突破。我们研究发现，其成功核心在于开源模型所不具备的高级推理模式：在海量信息空间中系统化降低极端不确定性的能力。基于该发现，我们提出WebSailor——整套训练后优化方案，通过以下方法实现该核心能力：结构化采样与信息混淆生成的高不确定性新任务、RFT冷启动技术，以及高效智能体强化学习算法DUPO（复制采样策略优化）。该完整方案使WebSailor在复杂信息检索任务中全面超越开源智能体，性能媲美专有系统，成功弥合了能力鸿沟。

## BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing
[BlenderFusion：基于3D基准的视觉编辑与生成式合成](https://arxiv.org/abs/2506.17450)

我们提出BlenderFusion，一个生成式视觉合成框架，通过重构物体、相机和背景的空间关系来合成新场景。该系统采用分层(layering)-编辑(editing)-合成(compositing)的三阶段流程：(i) 将视觉输入分割并转换为可编辑的3D实体（分层阶段）；(ii) 在Blender中运用3D基准化控制进行编辑（编辑阶段）；(iii) 使用生成式合成器将元素融合为连贯场景（合成阶段）。我们的生成式合成器基于预训练扩散模型扩展，可并行处理原始（源）场景与编辑后（目标）场景。该模型通过两种核心训练策略在视频帧数据上微调：(i) 源掩码(source masking)，支持背景替换等灵活修改；(ii) 模拟物体抖动(simulated object jittering)，实现物体与相机的独立控制。实验表明，BlenderFusion在复杂组合场景编辑任务中的性能显著超越现有方法。

## Ovis-U1 Technical Report
[Ovis-U1 技术报告](https://arxiv.org/abs/2506.23044)  

本报告介绍了 Ovis-U1——一个参数规模达 30 亿的统一模型，其整合了多模态理解、文生图（text-to-image）生成及图像编辑能力。基于 Ovis 系列架构，该模型通过整合基于扩散（diffusion）的视觉解码器与双向 token 精炼器，实现了与 GPT-4o 等领先模型相当的图像生成性能。不同于早期采用固定参数 MLLM（多模态大语言模型）处理生成任务的方案，Ovis-U1 创新性地采用以语言模型为基础的统一训练范式。相比单一理解或生成任务的独立训练，统一训练显著提升了模型性能，证明了整合这两类任务带来的协同增益。在 OpenCompass 多模态学术基准测试中，Ovis-U1 以 69.6 分的成绩超越 Ristretto-3B 和 SAIL-VL-1.5-2B 等前沿模型；在文生图任务中，其于 DPG-Bench 和 GenEval 基准分别取得 83.72 和 0.89 的优异表现；图像编辑任务中，在 ImgEdit-Bench 和 GEdit-Bench-EN 上分别达到 4.00 和 6.42 的评分。作为 Ovis 统一模型系列的首个版本，Ovis-U1 突破了多模态理解、生成与编辑的能力边界。

## Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning
[数学推理能提升大语言模型的通用能力吗？理解大语言模型推理的可迁移性](https://arxiv.org/abs/2507.00432)

数学推理已成为衡量大语言模型（LLMs）进展的重要指标，新模型在 MATH 和 AIME 等基准测试中快速超越了人类水平。但随着数学排行榜每周不断刷新，一个重要问题随之产生：这些提升反映的是更广泛的问题解决能力，还是仅仅局限于特定领域的过拟合？为探究这个问题，我们评估了超过 20 个开源权重的推理优化模型，测试范围涵盖数学、科学问答、智能体规划、编程和标准指令遵循等多个领域。我们意外发现，大多数在数学任务上表现优异的模型无法将其优势迁移到其他领域。为深入研究这一现象，我们使用纯数学数据但采用不同优化方法对 Qwen3-14B 模型进行了对照实验。结果表明，经过强化学习（RL）优化的模型展现出良好的跨领域泛化能力，而监督微调（SFT）优化的模型往往会丧失通用能力。通过潜在空间表示和 Token space 分布偏移分析发现，SFT 会导致显著的表示和输出偏移，而 RL 则能保持通用领域结构。这些发现表明，我们需要重新审视标准的后训练方案，特别是依赖 SFT 蒸馏数据来提升推理模型性能的常规做法。

## LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion
[LangScene-X：基于 TriMap 视频扩散的可泛化 3D 语言嵌入场景重建](https://arxiv.org/abs/2507.02813)

从二维图像重建具有开放词汇(open-vocabulary)场景理解能力的三维结构是一项基础性但极具挑战性的任务。最新研究进展通过采用嵌入语言信息的场景级优化方法实现了这一目标。然而，这些方法严重依赖于标定密集视图的重建范式，在输入视图数量有限时会产生明显的渲染伪影和失真的语义合成结果。本文提出了一种新型生成框架 LangScene-X，能够统一生成具有三维一致性的多模态信息，同时支持场景重建与理解。得益于生成模型创建一致性新观测的能力，我们仅需稀疏视图即可构建具有泛化能力的三维语言嵌入场景。具体而言，我们首先训练了一个 TriMap 视频扩散模型，该模型通过渐进式知识融合，能够从稀疏输入生成外观(RGB)、几何(法线)和语义(分割图)信息。此外，我们提出了一种在大规模图像数据集上训练的语言量化压缩器(LQC)，可高效编码语言嵌入特征，实现跨场景泛化而无需进行场景级重训练。最后，我们通过将语言信息映射到三维场景表面，构建了语言表面场，从而支持开放式语言查询。大量真实场景实验表明，LangScene-X 在重建质量和泛化性能方面均优于当前最先进方法。项目主页：https://liuff19.github.io/LangScene-X。

