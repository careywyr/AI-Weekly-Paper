## Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems
[基础智能体的进展与挑战：从类脑智能到进化、协作与安全系统](https://arxiv.org/abs/2504.01990)  

大语言模型 (Large Language Models, LLMs) 的出现引发了人工智能领域的变革，推动了一系列具备复杂推理能力、强健感知能力和跨领域多任务执行能力的先进智能体的发展。随着这类智能体在 AI 研究和实际应用中的作用日益凸显，其设计、评估与持续改进过程也面临着多维度、高复杂度的挑战。本综述采用模块化的类脑架构框架，融合认知科学、神经科学与计算研究的核心原理，对智能体技术进行全面梳理。研究内容分为四个相互关联的部分：首先解析智能体的模块化基础架构，通过系统化映射其认知、感知与执行模块与人脑功能的对应关系，深入阐释记忆系统、世界模型、奖励机制及类情绪系统等核心组件；其次探讨智能体的自我增强与自适应进化机制，重点分析其通过自动化优化范式（包括新兴的 AutoML 和大语言模型驱动的优化策略）实现能力自主提升、动态环境适应与持续学习的方法；第三部分研究协作型进化多智能体系统，揭示智能体通过交互协作与社会化组织产生的群体智能，及其与人类社交行为的相似性；最后针对 AI 系统的安全性、可靠性及社会效益这一关键命题，系统分析内生与外源安全威胁、伦理对齐要求、系统鲁棒性保障等核心问题，提出确保实际部署可信度的有效缓解策略。

## MoCha: Towards Movie-Grade Talking Character Synthesis  
[MoCha：迈向电影级说话角色合成](https://arxiv.org/abs/2503.23307)  

视频生成领域的最新进展已实现高度逼真的运动效果，但这些方法通常忽视了角色驱动的叙事需求——这是自动化电影与动画生成的关键任务。我们提出说话角色（Talking Characters）生成任务，旨在直接从语音和文本生成拟真角色动画。与传统的说话头部（talking head）技术不同，说话角色生成要求呈现一个或多个角色的完整肖像（包含非面部区域）。本文提出MoCha框架，这是首个实现说话角色生成的解决方案。  

为确保视频与语音的精确同步，我们设计了语音-视频窗口注意力（speech-video window attention）机制，可有效对齐语音Token和视频Token。针对带语音标注的大规模视频数据集稀缺问题，我们采用联合训练策略，同时利用语音标注和文本标注视频数据，显著提升了模型对不同角色动作的泛化能力。我们还开发了带角色标签的结构化提示模板，首次实现基于回合制对话的多角色交互——使AI生成的角色能够进行符合电影叙事逻辑的上下文感知对话。  

通过包括人类偏好研究、基准测试在内的多维度评估表明，MoCha在AI生成电影叙事领域树立了新标杆，在真实感、表现力、可控性及泛化能力等方面均达到业界领先水平。

## TextCrafter: Accurately Rendering Multiple Texts in Complex Visual Scenes
[TextCrafter：复杂视觉场景中的多文本精确渲染](https://arxiv.org/abs/2503.23461)

本文研究了复杂视觉文本生成 (Complex Visual Text Generation, CVTG) 任务，其核心目标是在视觉图像的不同区域生成分布复杂的文本内容。现有图像生成模型在 CVTG 任务中常出现文本扭曲、模糊或部分缺失的问题。为此，我们提出 TextCrafter——一种创新的多视觉文本渲染方法。该方法通过渐进策略将复杂视觉文本分解为独立组件，同时确保文本内容与视觉载体之间保持鲁棒对齐，并引入 token 聚焦增强机制以提升生成过程中视觉文本的显著性。TextCrafter 有效解决了 CVTG 任务中的文本混淆、缺失和模糊等关键问题。此外，我们构建了专用基准数据集 CVTG-2K，用于系统评估生成模型在 CVTG 任务上的性能。大量实验表明，本方法性能优于当前最优方案。

## MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization
[MergeVQ：基于解耦 Token 合并与量化的视觉生成及表征统一框架](https://arxiv.org/abs/2504.00999)

采用向量量化 (VQ) 的掩码图像建模 (MIM) 已在自监督预训练和图像生成领域取得显著成果。然而，现有方法普遍难以在共享潜在空间中权衡生成质量与表征学习效率。为此，我们提出 MergeVQ 框架，通过将 Token 合并技术融入基于 VQ 的生成模型，在统一架构中实现图像生成与视觉表征学习的协同优化。预训练阶段，MergeVQ 利用编码器自注意力块后的 Token 合并模块从潜在空间分离 top-k 语义特征，用于无查找量化 (LFQ) 和全局对齐，并通过解码器的交叉注意力机制重构细粒度细节。针对生成任务，我们提出 MergeAR 方法，通过 KV 缓存压缩实现高效的光栅扫描顺序预测。ImageNet 实验表明，MergeVQ 作为自回归 (AR) 生成模型在视觉表征学习和图像生成任务中均达到领先水平，同时保持优异的 Token 效率和推理速度。代码与模型详见 https://apexgen-x.github.io/MergeVQ。

## AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation  
[AdaptiVocab：通过轻量级词汇适配提升大语言模型在特定领域的效率](https://arxiv.org/abs/2503.19693)  

大语言模型 (LLMs) 作为通用模型表现出强大的泛化能力，但其广泛适用性带来高昂计算开销，特别是在自回归解码过程中需要逐次执行前向传播。在特定领域场景中，通用能力并非必需，可通过领域适配换取效率提升。本研究提出一种新颖的领域适配方法，通过将词汇表适配至目标领域来降低延迟和计算成本。我们设计了 AdaptiVocab——一种端到端的词汇适配框架，可提升大语言模型在低资源领域的效率。该方法兼容任意分词器和模型架构，通过用领域特定 n-gram token 替换原始 token 来优化词汇表，从而显著减少输入处理和输出生成所需的 token 数量。AdaptiVocab 采用现有嵌入的指数加权组合初始化新 n-token 嵌入，并实现可在单 GPU 上高效执行的轻量级微调。我们在三个细分领域对两个 7B 参数的大语言模型进行评估，涵盖效率、生成质量和下游任务性能指标。实验结果表明，该方法在不影响模型性能的前提下，使 token 使用量减少超过 25%。

## Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation
[Any2Caption：任意条件到文本描述的转换框架及其在可控视频生成中的应用](https://arxiv.org/abs/2503.24379)

针对当前视频生成领域在用户意图精确理解方面的瓶颈问题，我们提出Any2Caption框架，实现了任意条件下的可控视频生成。该框架的核心创新在于将条件理解过程与视频生成过程解耦。通过利用多模态大语言模型（MLLMs），Any2Caption能够将文本、图像、视频以及区域标注、运动轨迹和相机位姿等专业控制信号，转换为结构化的密集文本描述，为主干视频生成模型提供更精确的生成指导。同时，我们发布了Any2CapIns大规模数据集，包含33.7万样本实例和40.7万种条件，专门用于任意条件到文本描述的指令微调任务。综合实验表明，我们的系统显著提升了现有视频生成模型在多个维度的可控性和生成质量。项目主页：https://sqwu.top/Any2Cap/

## ZClip: Adaptive Spike Mitigation for LLM Pre-Training  
[ZClip：面向大语言模型预训练的自适应梯度峰值抑制方法](https://arxiv.org/abs/2504.02507)  

训练大语言模型 (LLMs) 存在诸多挑战，包括梯度不稳定性和损失值突变。这些现象可能引发灾难性发散，导致需要代价高昂的检查点恢复和训练批次丢弃。传统梯度裁剪技术（如固定阈值或基于范数的方法）由于依赖静态阈值或启发式方法，无法有效解决上述问题，不仅会降低学习效率，还需频繁人工干预。本文提出 ZClip——一种基于梯度范数时序统计特性的自适应裁剪算法。与现有被动策略不同，ZClip 无需预设梯度范数的量级或时序演变规律，即可主动适应训练动态。其核心机制是通过 z 分数异常检测来识别并抑制异常梯度峰值，既能预防恶性损失突变，又不会干扰正常收敛过程。代码已开源：https://github.com/bluorion-com/ZClip。

## Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing
[超越像素的构想：推理驱动的视觉编辑基准测试](https://arxiv.org/abs/2504.02826)

大规模多模态模型 (LMMs) 在视觉理解和生成领域取得了显著进展，但在通用图像编辑任务中仍面临挑战，尤其是在复杂指令遵循、外观一致性保持和灵活输入格式支持方面。为填补这一空白，我们提出了 RISEBench，这是首个面向推理驱动的视觉编辑 (Reasoning-Informed viSual Editing, RISE) 的评估基准。RISEBench 聚焦四种核心推理能力：时序、因果、空间和逻辑推理。我们为每类推理任务精心构建了高质量测试案例，并提出一个结合人工评估与 LMM 评估方法的综合框架，用于评估指令推理、外观一致性和视觉合理性。实验结果表明，尽管 GPT-4o-Native 显著优于其他开源和商业模型，但即使这一顶尖系统在逻辑推理任务上仍表现欠佳，凸显了该领域的研究不足。作为初步探索，RISEBench 旨在为推理感知的视觉编辑提供基础性见解，并推动相关研究发展。虽然当前仍处于早期阶段，我们将持续扩展和完善该基准，以支持对新一代多模态系统进行更全面、可靠和可扩展的评估。相关代码和数据将在 https://github.com/PhoenixZ810/RISEBench 开源。

## Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model
[Open-Reasoner-Zero：基于基础模型扩展强化学习的开源方案](https://arxiv.org/abs/2503.24290)

我们提出Open-Reasoner-Zero，这是首个面向推理任务的大规模强化学习（Reinforcement Learning）开源实现，重点关注可扩展性、简洁性和易用性。通过大量实验验证，我们采用极简方案——标准PPO算法配合GAE（$\lambda=1$，$\gamma=1$）和简单的规则化奖励机制（无需KL正则化）——即可同时提升生成响应长度和基准测试性能，这与DeepSeek-R1-Zero观察到的现象一致。基于与DeepSeek-R1-Zero-Qwen-32B相同的基础模型，我们的方案在AIME2024、MATH500和GPQA Diamond基准测试中表现出更优性能，同时训练效率显著提升——仅需DeepSeek-R1-Zero训练流程十分之一的迭代步数。遵循开源原则，我们公开了不同规模模型的完整代码、超参数配置、训练数据集及权重参数。

## Improved Visual-Spatial Reasoning via R1-Zero-Like Training
[基于 R1-Zero 式训练的视觉空间推理能力提升](https://arxiv.org/abs/2504.00883)

多模态大语言模型 (MLLMs) 的推理能力改进日益受到学界重视。作为物理环境 AI 智能体的核心能力，基于视频的视觉空间智能 (VSI) 已成为 MLLMs 最关键的基础推理能力之一。本文首次系统研究了通过 R1-Zero 式训练提升 MLLMs 视觉空间推理能力的方法。技术层面，我们首先发现中小型 Qwen2-VL 模型的视觉空间推理能力无法通过思维链 (CoT) 提示有效激发。随后我们引入 GRPO 训练框架，基于精心构建的 VSI-100k 数据集并参考 DeepSeek-R1-Zero 方案，显著提升了视觉空间推理性能。研究发现，在 GRPO 中保留 KL 惩罚项（即使取值较小）是必要的。仅需 120 GPU 小时训练，基于 Qwen2-VL-2B 微调的 vsGRPO-2B 模型性能即超越基线模型 12.1%，并优于 GPT-4o。此外，基于 Qwen2-VL-7B 微调的 vsGRPO-7B 模型达到了与当前最佳开源模型 LLaVA-NeXT-Video-72B 相当的水平。对比实验表明，vsGRPO 在监督微调和直接偏好优化等基线方法上均展现出显著优势。相关代码和数据集即将开源。

## DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance
[DreamActor-M1：基于混合引导的整体化、高表现力与鲁棒性人体图像动画](https://arxiv.org/abs/2504.01724)

当前基于图像的人体动画方法虽能合成逼真的身体与面部运动，但在细粒度整体控制、多尺度适应及长时序连贯性方面仍存在显著不足，制约了其表现力与鲁棒性。为此，我们提出基于扩散Transformer (DiT) 的框架DreamActor-M1，通过混合引导机制解决上述问题。运动引导方面，融合隐式面部表征、3D头部球体与3D身体骨骼的混合控制信号，在确保身份特征一致性的同时，实现了面部表情与肢体运动的高鲁棒控制。尺度适应方面，采用多分辨率数据的渐进训练策略，有效覆盖从肖像到全身视图的多样化姿态与尺度。外观引导方面，通过结合序列帧运动模式与互补视觉参考，保障复杂运动中未观测区域的长时序连贯性。实验表明，本方法在肖像、上半身及全身生成任务中均超越现有最优技术，呈现更具表现力且保持长期一致性的动画效果。项目主页：https://grisoon.github.io/DreamActor-M1/。

## AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction 
[AnimeGamer: 基于下一游戏状态预测的无限动漫人生模拟](https://arxiv.org/abs/2504.01014)  

图像与视频合成技术的最新突破为生成式游戏 (Generative Game) 带来了全新可能。其中尤为引人注目的是将动漫角色转化为可交互的游戏角色。通过语言指令，玩家可以沉浸于动态的动漫世界，以喜爱的角色体验人生模拟。我们将其定义为无限游戏，这类游戏没有预设边界和固定规则，玩家通过开放式语言交互，体验持续演进的剧情与环境。近期一项创新方法利用大语言模型 (LLM) 将多轮文本对话转化为图像生成指令，但该方法存在两个局限：一是忽略历史视觉上下文导致游戏体验不一致；二是仅能生成静态图像，缺乏游戏所需的动态表现。本文提出AnimeGamer系统，基于多模态大语言模型 (MLLM) 生成包含角色动作与状态更新的动态动画片段 (如图1所示)。我们创新性地采用动作感知多模态表示 (action-aware multimodal representations) 来编码动画片段，这些表示可通过视频扩散模型解码为高质量视频。通过以历史动画片段表示为上下文预测后续内容，AnimeGamer能生成具有上下文连续性且动态表现优异的游戏。定量指标与人工评估表明，AnimeGamer在游戏体验各维度均优于现有方法。代码与模型已开源：https://github.com/TencentARC/AnimeGamer。

## What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models  
[大语言模型测试阶段扩展技术综述：内容、方法、场景与效果评估](https://arxiv.org/abs/2503.24235)  

随着预训练阶段计算规模（数据与参数）扩展的热潮逐渐消退，测试阶段扩展（Test-Time Scaling, TTS）技术（亦称"测试时计算"）已成为新兴研究热点。最新研究表明，TTS能有效释放大语言模型（LLMs）的问题解决潜力，不仅在数学推理、代码生成等专业领域取得显著性能提升，在开放域问答等通用任务中也展现出卓越效果。尽管该领域近期涌现大量研究成果，学界仍缺乏系统性的综述研究。为此，本文提出基于四大核心维度的统一分析框架：扩展内容（What）、扩展方法（How）、应用场景（Where）和效果评估（How Well）。基于该分类体系，我们系统梳理了相关技术方法、应用场景和评估指标，通过结构化分类方法揭示了各项技术在TTS体系中的功能定位。基于此分析，我们总结了TTS技术的发展脉络，并提供了实际应用指导。此外，本文还识别了若干开放性问题，并展望了未来研究方向，包括扩展技术深化、方法机理阐释、任务泛化能力提升以及归因分析等方面。

## JudgeLRM: Large Reasoning Models as a Judge
[JudgeLRM: 大推理模型作为评估者](https://arxiv.org/abs/2504.00050)

随着大语言模型 (LLMs) 逐渐成为评估工具，它们为人工标注提供了可扩展的替代方案。然而，现有基于监督微调 (SFT) 的评判方法在需要复杂推理的领域往往表现欠佳。本研究探讨了增强推理能力是否确实能提升 LLM 评判效果。通过对评估任务中推理需求的系统分析，我们发现 SFT 的性能提升与高推理需求样本占比呈负相关，这揭示了 SFT 方法在此类场景中的固有局限。为此，我们提出了 JudgeLRM 系列模型——采用基于评判结果驱动的强化学习 (RL) 训练方案。实验表明，JudgeLRM 在性能上全面超越 SFT 微调模型和当前最先进的推理模型。其中，JudgeLRM-3B 优于 GPT-4，而 JudgeLRM-7B 在 F1 分数上以 2.79% 的优势超过 DeepSeek-R1，在需要深度推理的评判任务中表现尤为突出。

