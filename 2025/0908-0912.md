## Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing 
[分享即关怀：通过集体强化学习经验共享实现高效语言模型后训练](https://arxiv.org/abs/2509.08721)  

基于强化学习 (RL) 的语言模型 (LMs) 后训练可在无需监督微调的情况下增强复杂推理能力，DeepSeek-R1-Zero 已验证此特性。然而，有效运用 RL 训练语言模型需要大规模并行化以扩展推理，这会带来显著的技术挑战（如延迟、内存和可靠性）并持续增加财务成本。我们提出群体采样策略优化 (SAPO)——一种完全去中心化且异步的 RL 后训练算法。SAPO 专为异构计算节点的去中心化网络设计，各节点自主管理策略模型，同时与网络中其他节点"共享"轨迹数据 (rollouts)；无需对延迟、模型同构性或硬件作显式假设，节点也可按需独立运行。该算法由此规避了扩展 RL 后训练的常见瓶颈，同时支持（甚至鼓励）创新可能性。通过采样网络中"共享"的轨迹数据，促使关键突破持续传播，从而自举提升学习效率。本文实验表明，SAPO 在受控环境中实现了高达 94% 的累积奖励提升幅度。我们还分享了开源演示期间在 Gensyn 社区成员提供的数千节点网络（运行于多样化硬件与模型）中的测试洞见。

## VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model
[VLA-Adapter: 一种面向微型视觉-语言-动作模型的高效范式](https://arxiv.org/abs/2509.09372)

视觉-语言-动作 (Vision-Language-Action, VLA) 模型通常通过在机器人数据上预训练大规模视觉-语言模型 (Vision-Language Model, VLM) 来连接感知空间与动作空间。尽管这种方法显著提升了性能，但也产生了高昂的训练成本。本文研究了如何高效地将视觉-语言 (Vision-Language, VL) 表示与动作 (Action, A) 空间进行桥接。我们提出了 VLA-Adapter——一种新型范式，旨在降低 VLA 模型对大规模 VLM 和大量预训练的依赖。为此，我们首先系统分析了不同 VL 条件的有效性，并总结了哪些条件对桥接感知与动作空间至关重要。基于这些发现，我们设计了一个集成桥接注意力 (Bridge Attention) 的轻量级策略模块，能够自主地将最优条件融入动作空间。该方法仅需 0.5B 参数的主干网络，且无需任何机器人数据预训练，即可实现高性能。在模拟与真实机器人基准上的大量实验表明，VLA-Adapter 不仅达到了最先进的性能水平，还实现了迄今最快的推理速度。此外，凭借所提出的先进桥接范式，VLA-Adapter 仅需在单个消费级 GPU 上训练 8 小时即可得到强大的 VLA 模型，极大降低了部署 VLA 模型的技术障碍。项目页面: https://vla-adapter.github.io/。

## Why Language Models Hallucinate  
[为什么语言模型会产生幻觉](https://arxiv.org/abs/2509.04664)  

就像面对难题的学生一样，大语言模型在不确定时有时会猜测，生成看似合理实则错误的陈述，而非承认不确定性。这种"幻觉"现象即使在最先进的系统中依然存在，并会削弱信任。我们认为，语言模型产生幻觉是因为训练和评估过程鼓励猜测而非承认不确定性，同时我们分析了现代训练流程中幻觉产生的统计原因。幻觉并非神秘现象——它们本质上源于二元分类中的错误。如果错误陈述无法与事实区分，那么预训练语言模型中的幻觉就会通过自然的统计机制产生。我们进一步指出，幻觉持续存在是因为大多数评估的评分方式：语言模型被优化成擅长应试的系统，而在不确定时猜测能够提升测试表现。这种惩罚不确定性回答的"流行病"只能通过社会技术手段缓解：需要修改那些与真实需求不符却主导排行榜的现有基准评分方式，而非引入额外的幻觉评估指标。这一转变或许能推动领域向更可信的AI系统发展。

## Reverse-Engineered Reasoning for Open-Ended Generation  
[面向开放式生成的反向工程推理](https://arxiv.org/abs/2509.06160)  

尽管"深度推理"范式在数学等可验证领域取得了显著进展，但其在开放式创造性生成中的应用仍面临关键挑战。当前两种主流的推理能力培养方法——强化学习 (RL) 和指令蒸馏——在此领域均存在局限性：强化学习因缺乏清晰奖励信号和高质量奖励模型而难以实施，而蒸馏方法则成本极高且受限于教师模型的能力上限。为突破这些限制，我们提出了反向工程推理 (REER) ，这是一种根本性转变的新范式。REER 并非通过试错或模仿"正向"构建推理过程，而是从已知良好解决方案出发，"反向"通过计算方式发现可能生成这些解决方案的潜在逐步深度推理过程。通过这种可扩展的无梯度方法，我们构建并开源了 DeepWriting-20K——一个包含 20,000 条开放式任务深度推理轨迹的大规模数据集。基于该数据训练的 DeepWriter-8B 模型不仅超越了强开源基线，更在性能上与 GPT-4o 和 Claude 3.5 等领先专有模型持平甚至更优。

## A Survey of Reinforcement Learning for Large Reasoning Models  
[面向大推理模型的强化学习综述](https://arxiv.org/abs/2509.08827)  

本文系统综述了强化学习 (Reinforcement Learning, RL) 在大语言模型 (Large Language Models, LLMs) 推理任务中的最新进展。RL 在拓展 LLM 能力边界方面取得显著成果，尤其在数学与代码生成等复杂逻辑任务中表现突出。因此，RL 已成为将 LLM 转化为大推理模型 (Large Reasoning Models, LRMs) 的核心方法。随着该领域的快速发展，针对 LRM 的 RL 技术进一步扩展当前面临根本性挑战，这些挑战不仅存在于计算资源层面，还涉及算法设计、训练数据与基础设施。为此，有必要重新审视该领域的发展历程、评估其技术路径，并探索提升 RL 可扩展性以通向人工超智能 (Artificial SuperIntelligence, ASI) 的策略。具体而言，我们重点分析了自 DeepSeek-R1 发布后 RL 应用于 LLM 及 LRM 推理能力的研究，涵盖基础组件、核心问题、训练资源与下游应用，以明确这一快速演进领域的未来机遇与方向。本综述旨在推动面向更广泛推理模型的 RL 研究。Github:  
https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs

## HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning
[HuMo: 通过协同多模态条件控制的以人为中心视频生成](https://arxiv.org/abs/2509.08519)

以人为中心的视频生成 (HCVG) 方法旨在通过多模态输入 (包括文本、图像和音频) 合成人类视频。现有方法面临两个挑战而难以有效协调这些异构模态：配对三元组条件训练数据的稀缺性，以及多模态输入下主体保持与视听同步子任务的协同困难。本研究提出 HuMo——一个面向协同多模态控制的统一 HCVG 框架。针对第一个挑战，我们构建了包含多样化配对文本、参考图像和音频的高质量数据集。针对第二个挑战，我们提出了具有任务特定策略的两阶段渐进式多模态训练范式。对于主体保持任务，为保持基础模型的提示跟随能力和视觉生成能力，采用最小侵入式图像注入策略。对于视听同步任务，除常用的音频交叉注意力层外，提出了预测式聚焦策略，隐式引导模型建立音频与面部区域的关联。基于已获得的能力，通过逐步融入视听同步任务实现多模态输入可控性的联合学习。在推理阶段，为实现灵活细粒度的多模态控制，设计了时间自适应无分类器引导 (Classifier-Free Guidance) 策略，动态调整去噪过程中的引导权重。大量实验结果表明，HuMo 在子任务性能上超越各类专用先进方法，建立了协同多模态条件控制 HCVG 的统一框架。项目页面：  
https://phantom-video.github.io/HuMo

## Parallel-R1: Towards Parallel Thinking via Reinforcement Learning
[Parallel-R1: 通过强化学习实现并行思维](https://arxiv.org/abs/2509.07980)

并行思维是一种通过并发探索多个推理路径来增强大语言模型 (LLM) 推理能力的新方法。然而，通过训练激活这种能力仍面临挑战，因为现有方法主要依赖于对合成数据进行监督微调 (SFT)，这会鼓励教师强制学习而非自主探索与泛化。与此不同，我们提出了 **Parallel-R1**——首个能够为复杂现实世界推理任务实现并行思维行为的强化学习 (RL) 框架。该框架采用渐进式课程学习策略，显式解决了并行思维在强化学习训练中的冷启动问题。我们首先在较简单任务中基于提示生成的轨迹进行监督微调，以初步建立并行思维能力，随后转入强化学习阶段，在更复杂问题上探索和泛化该能力。在 MATH、AMC23 和 AIME 等数学基准测试上的实验表明，Parallel-R1 成功培养了并行思维能力，相较于直接通过强化学习训练序列思维模型解决挑战性任务的方法，准确率提升 8.4%。进一步分析揭示了模型思维模式的显著转变：早期阶段将并行思维作为探索策略使用，而后期阶段则将其用于多视角验证。最重要的是，我们验证了并行思维可作为**训练中期的探索脚手架**——这一临时探索阶段能为后续强化学习解锁更高性能上限，在 AIME25 数据集上相比基线模型实现 42.9% 的性能提升。我们的模型、数据与代码将在 https://github.com/zhengkid/Parallel-R1 开源。

## WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents
[WebExplorer: 通过探索与进化训练长程网络智能体](https://arxiv.org/abs/2509.06501)

大语言模型 (LLMs) 的发展范式正日益转向智能体应用，其中网页浏览能力对于从多样化在线资源中检索信息至关重要。然而，现有的开源网络智能体要么在复杂任务上表现出有限的信息获取能力，要么缺乏透明的实现方案。本研究指出，关键挑战在于缺乏适用于信息获取任务的复杂数据。为解决这一局限，我们提出了 WebExplorer：一种基于模型探索和查询语句由长到短迭代进化的系统化数据生成方法。该方法能够创建需要多步推理和复杂网页导航的挑战性查询-答案对。通过利用我们精心构建的高质量数据集，我们成功通过监督微调结合强化学习，开发出先进的网络智能体 WebExplorer-8B。该模型支持 128K 上下文长度和最多 100 次工具调用，能够实现长程问题求解。在多种信息获取基准测试中，WebExplorer-8B 在同等规模模型中取得了最先进的性能。值得注意的是，这个 80 亿参数的模型在经过强化学习训练后，平均能有效进行超过 16 轮次的搜索，在 BrowseComp-en/zh 上的准确度超越 WebSailor-720 亿参数模型，并在 WebWalkerQA 和 FRAMES 基准上取得了参数量达 1000 亿以内模型的最佳性能。除了信息获取任务，我们的模型在 HLE 基准测试中也展现出出色的泛化性能，尽管其训练仅使用了知识密集型问答数据。这些结果表明我们的方法为实现长程网络智能体提供了一条实用路径。

## Visual Representation Alignment for Multimodal Large Language Models
[多模态大语言模型的视觉表示对齐](https://arxiv.org/abs/2509.07979)

经过视觉指令调优训练的多模态大语言模型 (Multimodal Large Language Models, MLLMs) 虽然在多样化任务中展现出优异性能，但在物体计数或空间推理等以视觉为中心的任务中仍存在明显局限。我们将此性能差距归因于当前主流的纯文本监督范式：该范式仅对视觉通路 (visual pathway) 提供间接指导，往往导致 MLLMs 在训练过程中丢失细粒度视觉细节。本文提出视觉表示对齐 (VIsual Representation ALignment, VIRAL)——一种简单有效的正则化策略，通过将 MLLMs 的内部视觉表示与预训练视觉基础模型 (Vision Foundation Models, VFMs) 的表示进行显式对齐，使模型既能保留输入视觉编码器的关键视觉细节，又能补充 VFMs 提供的额外视觉知识，从而显著提升对复杂视觉输入的推理能力。实验结果表明，该方法在广泛采用的多模态基准测试的所有任务上均取得一致提升。此外，我们通过全面消融研究验证了框架的核心设计选择。我们相信这一简单发现为 MLLMs 训练中视觉信息的有效整合开辟了重要研究方向。

## SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning
[SimpleVLA-RL: 通过强化学习扩展 VLA 训练](https://arxiv.org/abs/2509.09674)

视觉-语言-动作模型 (Vision-Language-Action, VLA) 最近已成为机器人操作领域的一个强大范式。尽管通过大规模预训练和监督微调 (Supervised Fine-Tuning, SFT) 取得了实质性进展，这些模型仍面临两个基本挑战：(i) SFT 扩展所需的大规模人类操作机器人轨迹数据稀缺且成本高昂，以及 (ii) 对存在分布偏移的任务泛化能力有限。大型推理模型 (Large Reasoning Models, LRMs) 的最新突破表明，强化学习 (Reinforcement Learning, RL) 能够显著增强逐步推理能力，这引出了一个自然问题：RL 能否同样提升 VLA 的长视野 (long-horizon) 逐步动作规划性能？在本工作中，我们提出了 SimpleVLA-RL，一个专为 VLA 模型设计的高效 RL 框架。基于 veRL，我们引入了 VLA 专用的轨迹采样、可扩展的并行化处理、多环境渲染技术以及经过优化的损失计算。将 SimpleVLA-RL 应用于 OpenVLA-OFT 后，该框架在 LIBERO 基准上达到了当前最先进 (SoTA) 性能，并且凭借我们引入的探索增强策略，在 RoboTwin 1.0&2.0 上的表现甚至超过了基线策略 $\pi_0$。SimpleVLA-RL 不仅降低了对大规模数据的依赖，并实现了更强的泛化能力，更在真实世界任务中显著超越了 SFT 方法。此外，我们在 RL 训练过程中发现了一种新颖的“pushcut”现象，即策略能够发现训练历史中从未出现过的新模式，超越了既往所学的模式。Github: https://github.com/PRIME-RL/SimpleVLA-RL

## RewardDance: Reward Scaling in Visual Generation
[RewardDance：视觉生成中的奖励缩放](https://arxiv.org/abs/2509.08826)

奖励模型 (Reward Models, RMs) 对通过强化学习 (Reinforcement Learning, RL) 改进生成模型至关重要，然而视觉生成领域的 RM 缩放范式仍很大程度上未被探索。这主要源于现有方法的基本局限性：基于 CLIP 的 RMs 受到架构和输入模态的约束，而广泛采用的 Bradley-Terry 损失函数与视觉语言模型 (Vision-Language Models, VLMs) 的下一 token 预测机制存在根本性错位，阻碍了有效的规模化扩展。更关键的是，RLHF 优化过程长期受到奖励黑客 (Reward Hacking) 问题的困扰，即模型利用奖励信号中的缺陷提升分数而非真实生成质量。

为解决这些挑战，我们提出 RewardDance——一个通过新型生成式奖励范式突破这些障碍的可扩展奖励建模框架。通过将奖励分数重构为模型预测"yes" token 的概率（该概率表示生成图像在特定标准下优于参考图像），RewardDance 实现了奖励目标与 VLM 架构的内在对齐。这种对齐机制解锁了两个维度的扩展能力：(1) 模型缩放：将 RMs 系统性地扩展至 260 亿参数规模；(2) 上下文缩放：集成任务特定指令、参考示例和思维链 (Chain-of-Thought, CoT) 推理。

大量实验表明，RewardDance 在文本到图像、文本到视频和图像到视频生成任务中显著超越现有最先进方法。最关键的是，该方法攻克了长期存在的"奖励黑客"难题：我们的大规模 RMs 在 RL 微调过程中始终展现并维持高奖励方差，证实其具备抗奖励黑客攻击的能力，并能产生多样化的高质量输出。这极大地缓解了困扰较小模型的模式崩溃 (Mode Collapse) 问题。

## EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs
[EchoX: 通过回声训练缓解语音到语音大语言模型中的声学-语义鸿沟](https://arxiv.org/abs/2509.09174)

语音到语音大语言模型 (SLLMs) 正受到越来越多的关注。这类模型衍生自基于文本的大语言模型 (LLMs) , 但通常会出现知识与推理能力的退化。我们假设, 该局限性的根源在于当前 SLLMs 的训练范式未能弥补特征空间中的声学-语义鸿沟。为解决该问题, 我们提出 EchoX, 其利用语义表示动态生成语音训练目标。该方法融合了声学与语义学习, 使得 EchoX 作为语音大语言模型仍可保持强大的推理能力。实验结果表明, 在使用约六千小时训练数据的情况下, EchoX 在多个基于知识的问答基准上取得了优异性能。项目已开源于: https://github.com/FreedomIntelligence/EchoX。

## Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search
[Mini-o3: 扩展视觉搜索中的推理模式与交互轮次  ](https://arxiv.org/abs/2509.07969)  

大规模多模态模型的最新进展通过融合基于图像的工具与强化学习来处理视觉问题。然而，现有开源方法通常表现出单调的推理模式，且仅支持有限数量的交互轮次，导致其难以应对需要试错探索的复杂任务。本研究通过扩展基于工具的交互来解决这一局限，推出 Mini-o3 系统：该系统能够执行深度多轮推理（跨越数十个步骤），并在挑战性视觉搜索任务中达到最先进的性能。我们复现 OpenAI O3 风格行为的技术方案包含三个关键组成部分：首先，构建了 Visual Probe Dataset（视觉探测数据集），这是一个包含数千个专为探索性推理设计的挑战性视觉搜索问题的集合；其次，开发了迭代式数据收集流程，用于获取展现多样化推理模式（包括深度优先搜索、试错和目标维持）的冷启动轨迹 (cold-start trajectories)；第三，提出了超轮次掩码 (over-turn masking) 策略，在强化学习期间避免对超轮次响应（即达到最大交互轮次的响应）进行惩罚，从而在训练效率与测试可扩展性之间实现平衡。尽管训练时仅设置六轮交互上限，我们的模型在推理时能自然生成扩展至数十轮的轨迹，且准确率随轮次增加而提升。广泛实验证明，Mini-o3 可产生丰富的推理模式和深度思考路径，有效解决复杂视觉搜索问题。

## MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML
[MachineLearningLM: 在数百万合成表格预测任务上持续预训练语言模型以规模化上下文机器学习能力](https://arxiv.org/abs/2509.06806)

大语言模型 (LLMs) 虽拥有广泛的世界知识和强大的通用推理能力，但在标准机器学习 (ML) 任务上难以有效利用大量上下文示例进行学习，即无法在不使用梯度下降的情况下，纯粹通过上下文学习 (ICL) 来驾驭多样本 (many-shot) 演示。我们提出了 MachineLearningLM，一个轻量级的持续预训练框架，旨在为通用大语言模型赋予强大的上下文机器学习能力，同时保留其通用知识与推理能力，以支持更广泛的聊天工作流程。
  我们的预训练过程基于数百万个结构因果模型 (SCMs) 合成机器学习任务，覆盖的演示样本数 (shot count) 最高达 1,024。我们首先采用随机森林作为教师模型，将基于树的决策策略蒸馏 (distill) 到大语言模型中，以增强其在数值建模中的鲁棒性。所有任务均通过一种 Token 高效 (token-efficient) 的提示方式进行序列化，这使得每个上下文窗口可容纳的示例数量增加了 3 至 6 倍，并通过批量推理 (batch inference) 实现了高达 50 倍的摊销吞吐量 (amortized throughput)。
  尽管配置相对精简 (使用 Qwen-2.5-7B-Instruct 模型和 LoRA rank 8)，MachineLearningLM 在金融、物理、生物和医疗领域的分布外 (out-of-distribution) 表格分类任务中，平均性能仍显著超越强大的大语言模型基线 (例如 GPT-5-mini) 约 15%。它展现出了显著的多样本缩放定律 (many-shot scaling law)：当上下文演示样本数量从 8 增至 1,024 时，其准确率呈现单调上升趋势。在未进行任何任务特定训练的情况下，该模型在数百个样本上即可达到与随机森林相当的准确率水平。同时，其通用聊天能力 (包括知识问答与推理能力) 得到了完好保持：在 MMLU 基准测试中取得了 75.4% 的得分。

## Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models
[面向扩散大语言模型的强化学习框架革新](https://arxiv.org/abs/2509.06949)

我们提出TraceRL——一种面向扩散语言模型（DLMs）的轨迹感知强化学习框架，该框架将偏好推理轨迹整合到训练后阶段，并适用于不同架构。通过配备基于扩散的价值模型来增强训练稳定性，我们在复杂数学和编程任务上证明了推理性能的提升。此外，该框架还能将块专用模型适配到更大模块，从而提升采样灵活性。基于TraceRL，我们开发出系列最先进的扩散语言模型TraDo。尽管TraDo-4B-Instruct的参数量小于70亿规模的自回归模型，但在复杂数学推理任务中持续超越后者。TraDo-8B-Instruct在数学推理基准测试中相比Qwen2.5-7B-Instruct和Llama3.1-8B-Instruct分别实现6.1%和51.3%的相对准确率提升。通过课程学习，我们还训练出首个长思维链（long-CoT）扩散语言模型，在MATH500数据集上以18.1%的相对准确率优势超越Qwen2.5-7B-Instruct。为促进可复现研究和实际应用，我们开源了完整的框架，支持跨架构构建、训练和部署扩散大语言模型。该框架集成加速KV缓存技术与推理引擎，同时支持推理和强化学习，并包含针对数学、编程和通用任务的各种监督微调及强化学习方法实现。代码与模型：https://github.com/Gen-Verse/dLLM-RL

## 3D and 4D World Modeling: A Survey
[3D 和 4D 世界建模：综述](https://arxiv.org/abs/2509.07996)  

世界建模已成为 AI 研究的核心基础，使智能体能够理解、表示并预测其所在的动态环境。尽管先前的研究主要侧重于 2D 图像和视频数据的生成方法，但它们忽略了日益增多的采用原生 3D 和 4D 表示（如 RGB-D 图像、占据栅格和 LiDAR 点云）进行大规模场景建模的工作。同时，由于缺乏“世界模型”的标准化定义和分类体系，文献中的观点往往零散且存在不一致之处。本综述首次专门针对 3D 和 4D 世界建模与生成进行了全面回顾，以填补这些空白。我们明确了相关定义，提出了一个结构化分类体系，涵盖基于视频的方法 (VideoGen)、基于占据的方法 (OccGen) 和基于 LiDAR 的方法 (LiDARGen)，并系统总结了面向 3D/4D 场景定制的数据集和评估指标。此外，我们还讨论了实际应用，指出了当前面临的挑战，并展望了有潜力的研究方向，旨在为该领域的发展提供连贯且基础性的参考。现有文献的系统性总结可在 https://github.com/worldbench/survey 查看。

## Set Block Decoding is a Language Model Inference Accelerator
[集合块解码是一种语言模型推理加速器](https://arxiv.org/abs/2509.04185)

自回归的下一个词元预测语言模型具备强大的能力，但由于推理阶段高昂的计算和内存成本，尤其是在解码过程中，其实际部署面临重大挑战。我们提出集合块解码(SBD)，这是一种简单而灵活的范式，通过在单一架构中集成标准的下一个词元预测(NTP)和掩码词元预测(MATP)来加速生成过程。SBD使模型能够并行采样多个非连续的未来词元，这与以往的加速方法有显著区别。这种灵活性使得我们可以利用离散扩散领域中的高级求解器，在保持准确性的同时实现大幅加速。SBD无需改动模型架构或引入额外的训练超参数，保持了与精确KV缓存机制的兼容性，并可通过微调现有的下一个词元预测模型来实现。通过对Llama-3.1 8B和Qwen-3 8B进行微调，我们证明SBD能够将生成所需的前向传播次数减少至1/3到1/5，同时达到与等效NTP训练方法相同的性能表现。

