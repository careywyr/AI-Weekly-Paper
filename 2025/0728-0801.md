## Agentic Reinforced Policy Optimization
[智能体强化策略优化](https://arxiv.org/abs/2507.19849)

具备可验证奖励机制的大规模强化学习(RLVR)已证实其在挖掘大语言模型(LLM)单次推理任务潜力方面的有效性。在实际推理场景中，大语言模型常借助外部工具辅助任务求解。然而现有强化学习算法难以平衡模型固有的长跨度推理能力与多步工具交互能力。为此，我们提出智能体强化策略优化(ARPO)，这是一种专为训练基于大语言模型的多步智能体而设计的新型强化学习算法。初步实验表明，大语言模型在与外部工具交互后会立即表现出高度不确定性行为，其特征是生成token的熵分布显著增加。基于这一发现，ARPO引入了基于熵的自适应轨迹生成机制，动态平衡全局轨迹采样与单步采样，从而在工具调用后的高不确定性步骤中增强探索能力。通过整合优势归因估计，ARPO使大语言模型能够内化逐步工具交互中的优势差异。我们在计算推理、知识推理和深度搜索领域的13个挑战性基准测试表明，ARPO显著优于轨迹层面的强化学习算法。值得注意的是，ARPO仅需现有方法50%的工具调用次数即可实现性能提升，为基于大语言模型的智能体与实时动态环境对齐提供了可扩展方案。代码和数据集已开源：https://github.com/dongguanting/ARPO

## HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels  
[HunyuanWorld 1.0: 基于文字或像素生成沉浸式、可探索与交互的3D世界](https://arxiv.org/abs/2507.21809)  

基于文本或图像生成沉浸式、可交互的3D世界始终是计算机视觉与图形学领域的核心挑战。现有世界生成方法主要分为两类：基于视频的方法虽能提供丰富多样性，但存在3D一致性与渲染效率不足的问题；而基于3D的方法虽具备几何一致性优势，却面临训练数据有限和内存占用高的表示瓶颈。为解决这些局限性，我们提出HunyuanWorld 1.0框架，通过融合两类方法的优势，实现基于文本/图像输入生成沉浸式、可探索且交互性强的3D场景。该框架具有三大核心特性：1) 通过全景世界代理实现360度沉浸式体验；2) 支持网格导出，可与现有计算机图形管线无缝对接；3) 采用分离式对象表示以增强交互能力。其技术核心是语义分层的3D网格表示，利用全景图像作为360度世界代理进行语义感知的世界分解与重建，从而生成多样化3D场景。实验结果表明，本方法在生成连贯、可探索且交互性强的3D世界方面达到业界最优水平，同时可广泛应用于虚拟现实、物理仿真、游戏开发及交互式内容创作等领域。

## Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving
[Seed-Prover: 自动定理证明的深度与广度推理](https://arxiv.org/abs/2507.23726)

大语言模型通过结合长思维链的强化学习展示出了强大的数学推理能力，但在仅依赖自然语言时，由于缺乏明确的监督信号，其定理证明能力仍存在局限。专用领域特定语言（如Lean）通过形式化验证提供了清晰的监督信号，从而能够基于强化学习进行高效训练。本研究提出\textbf{Seed-Prover}——一种基于引理的全证明推理模型。该模型可基于Lean的验证反馈、已证明引理及自我总结实现证明的迭代优化。针对IMO级竞赛问题，我们设计了三种测试阶段推理策略，实现了深度与广度的协同推理。Seed-Prover在形式化历史IMO问题上取得了78.1%的证明成功率，在MiniF2F基准上达到饱和性能，并在PutnamBench上突破50%准确率，显著超越了现有最先进技术。为弥补Lean在几何支持方面的不足，我们开发了几何形式化推理引擎\textbf{Seed-Geometry}，其性能优于现有几何形式化引擎。基于这两个系统，我们在IMO 2025中成功证明了6道赛题中的5道。这项工作标志着自动数学推理领域的重大突破，验证了形式化方法与长思维链推理相结合的有效性。

## ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents
[ScreenCoder：通过模块化多模态智能体实现视觉到代码生成的前端自动化](https://arxiv.org/abs/2507.22827)

将用户界面(UI)设计自动转换为前端代码，对于提升软件开发效率和降低设计工作门槛具有重要价值。尽管当前的大语言模型(LLMs)在文本到代码生成方面已取得进展，但多数现有方法仅依赖自然语言提示，难以有效捕捉空间布局和视觉设计意图。而实际UI开发本质上是多模态的，通常始于视觉草图或原型设计。为此，我们提出了一种模块化多智能体框架，通过三个可解释阶段实现UI到代码的转换：定位(grounding)、规划(planning)和生成(generation)。定位智能体采用视觉语言模型(vision-language model)识别并标注UI组件，规划智能体基于前端工程经验(front-end engineering priors)构建层次化布局，生成智能体通过自适应提示合成技术输出HTML/CSS代码。该设计在鲁棒性、可解释性和保真度方面优于端到端黑盒方法(end-to-end black-box methods)。此外，我们将该框架扩展为可扩展的数据引擎(data engine)，可自动生成大规模图像-代码对。利用这些合成数据对开源视觉语言模型(VLM)进行微调强化后，在UI理解和代码质量方面取得了显著改进。大量实验表明，我们的方法在布局精度、结构连贯性和代码正确性方面均达到了SOTA(state-of-the-art)性能。代码已开源：https://github.com/leigest519/ScreenCoder。

## A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence
[自进化智能体综述：迈向超级人工智能之路](https://arxiv.org/abs/2507.21046)  

大语言模型（LLMs）虽展现出强大能力，但其本质仍属静态系统，无法通过调整内部参数来适应新任务、动态演化的知识领域或实时交互场景。随着LLMs在开放式交互环境中的广泛应用，这种静态特性已成为关键发展瓶颈，亟需具备实时自适应推理、行动与进化能力的智能体。这一从扩展静态模型到开发自进化智能体的重大范式转变，正推动学界对持续学习架构与适应方法的深入研究。本综述首次从三大核心维度（进化对象、进化时机与进化机制）出发，对自进化智能体进行系统化梳理：通过分析智能体组件（模型/记忆/工具/架构等）的进化机制，划分基于阶段的适应方法（如测试时/测试间适应），并剖析驱动进化的算法架构设计（标量奖励/文本反馈/单多智能体系统等）。此外，我们系统评估了专用评测体系与基准测试，重点阐释在编程、教育、医疗等领域的应用实践，同时指明安全性、可扩展性及协同进化动力学等关键挑战。本研究构建的自进化智能体设计框架，不仅为学术研究与实际部署提供技术路线图，更为实现超级人工智能（ASI）奠定理论基础——未来智能体将通过自主进化，在多元任务中达到或超越人类智能水平。

## Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance
[Falcon-H1：重新定义效率与性能的混合架构语言模型系列](https://arxiv.org/abs/2507.22448)

本报告介绍 Falcon-H1 系列大语言模型 (LLMs)，该系列采用混合架构设计，针对多样化应用场景优化了高性能与效率。与早期仅基于 Transformer 或 Mamba 架构的 Falcon 模型不同，Falcon-H1 采用并行混合架构，将基于 Transformer 的注意力机制与具有卓越长上下文记忆和计算效率优势的状态空间模型 (SSMs) 相结合。我们系统性地重构了模型设计、数据策略和训练动态，突破了该领域的传统范式。Falcon-H1 提供多种参数配置，包括基础版和指令调优版，参数规模涵盖 0.5B、1.5B、1.5B-deep、3B、7B 和 34B，同时提供量化版指令调优模型，在 Hugging Face Hub 平台共计发布超过 30 个模型检查点。Falcon-H1 系列展现出业界领先的性能表现及卓越的参数与训练效率：旗舰模型 Falcon-H1-34B 在性能上达到或超越 Qwen3-32B、Qwen2.5-72B 和 Llama3.3-70B 等 70B 规模模型，同时所需参数量和训练数据更少；较小规模模型同样表现优异，Falcon-H1-1.5B-Deep 与当前主流 7B-10B 模型性能相当，Falcon-H1-0.5B 则可媲美 2024 年典型的 7B 模型。该系列在逻辑推理、数学运算、多语言任务、指令遵循和科学知识等维度均表现突出，支持处理长达 256K 的上下文 Token 并涵盖 18 种语言，具有广泛的应用潜力。所有模型均采用宽松开源协议发布，体现了我们对可访问性及具影响力 AI 研究的一贯承诺。

## ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts
[ARC-Hunyuan-Video-7B：现实世界短视频的结构化理解](https://arxiv.org/abs/2507.20939)

由用户生成的现实世界短视频（特别是微信视频号和 TikTok 等平台上的内容）已成为移动互联网的主流内容形式。然而，当前主流的大规模多模态模型普遍缺乏关键的时间结构化、细粒度和深层次的视频理解能力——这些能力正是实现高效视频搜索推荐及新兴视频应用的基础。理解这类短视频存在显著挑战：其视觉元素复杂、视听信息密度高、节奏快且注重情感表达与观点传递，这要求模型具备高级推理能力以有效整合视觉、音频和文本等多模态信息。本文提出的 ARC-Hunyuan-Video 多模态模型，能够端到端处理原始视频的视觉、音频和文本信号，实现结构化理解。该模型支持多粒度带时间戳的视频描述与摘要、开放式视频问答、时序视频定位以及视频推理等任务。通过自动化标注流水线生成的高质量数据，我们采用预训练-指令微调-冷启动-强化学习后训练-最终指令微调的全流程方案，训练出这个仅含 70 亿参数的紧凑模型。在自建基准 ShortVid-Bench 上的定量评估与定性对比表明，该模型在现实世界视频理解任务中表现优异，并能通过零样本或少样本微调适配多种下游应用。实际生产部署数据显示，该模型显著提升了用户参与度和满意度，其卓越效率得到验证：在 H20 GPU 上对 1 分钟视频的推理时间仅需 10 秒（压力测试结果）。

## BANG: Dividing 3D Assets via Generative Exploded Dynamics
[BANG：基于生成式爆炸动力学的3D资产分解](https://arxiv.org/abs/2507.21493)

3D创作始终是人类特有的能力优势，这源于我们通过视觉认知、思维解析和手工操作实现物体解构与重组的天赋。然而，现有3D设计工具难以复现这一自然过程，往往需要大量专业美术知识和人工操作。本文提出BANG——一种创新生成方法，通过融合3D生成与结构推理技术，实现对3D物体的直观部件级分解。该技术的核心是"生成式爆炸动力学"，可为输入几何体生成平滑的爆炸视图序列，在逐步分离部件的同时保持其几何结构与语义关联的完整性。
  BANG采用预训练的大规模潜在扩散模型，通过轻量化爆炸视图适配器进行微调，从而精确控制分解过程。系统集成时序注意力模块确保状态转换的连贯性与时间一致性。通过引入边界框、表面区域等空间控制信号，BANG支持用户指定待分解部件及分解方式。这种交互机制可结合GPT-4等多模态模型，实现2D到3D的直观操作，打造更具创造性的工作流程。
  BANG能够生成精细的部件级几何结构，为部件关联功能描述，并支持基于组件的3D创作与制造流程。在3D打印领域，该系统可生成便于打印与组装的分离式部件。本质上，BANG实现了从创意构思到3D资产的流畅转换，提供了一种更符合人类直觉认知的全新创作范式。

## Deep Researcher with Test-Time Diffusion  
[基于测试时扩散的深度研究智能体](https://arxiv.org/abs/2507.16075)  

由大语言模型 (LLMs) 驱动的深度研究智能体发展迅速，但在使用通用测试时扩展算法生成复杂的长篇研究报告时，其性能往往停滞不前。受人类研究中搜索、推理和修订的迭代特性启发，我们提出了测试时扩散深度研究智能体 (TTD-DR)。这一创新框架将研究报告生成建模为扩散过程。TTD-DR 以初步草稿启动该过程，该草稿作为可更新的骨架结构，动态指导研究方向。随后，草稿通过“去噪”过程迭代优化，该过程由检索机制动态驱动，并在每一步整合外部信息。核心过程进一步通过智能体工作流各组件的自进化算法增强，确保为扩散过程生成高质量内容。这种以草稿为中心的设计使报告撰写过程更高效且连贯，同时减少了迭代搜索中的信息损失。实验表明，TTD-DR 在需要密集搜索和多跳推理的多项基准测试中达到了最先进水平，显著优于现有深度研究智能体。

## SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment  
[SmallThinker: 原生适配本地部署的高效大语言模型家族](https://arxiv.org/abs/2507.20984)  

当前沿大语言模型 (LLMs) 不断突破能力边界时，其部署仍受限于 GPU 云基础设施。我们提出的 SmallThinker 系列打破了这一范式，这是专为本地设备的三大特性 (计算能力弱、内存有限、存储速度慢) 原生设计 (而非后期适配) 的 LLM 家族。不同于传统方案仅对云端模型进行压缩，我们从底层架构重新设计，使模型能在资源受限环境下高效运行。核心技术突破在于将硬件限制转化为设计准则的部署感知架构：首先，采用细粒度混合专家 (MoE) 与稀疏前馈网络相结合的两级稀疏结构，在保持模型容量的同时显著降低计算需求；其次，针对低速存储的 I/O 瓶颈，设计了预注意力路由机制，使协同设计的推理引擎能在计算注意力时并行预取专家参数，有效掩盖存储延迟；第三，通过 NoPE-RoPE 混合稀疏注意力机制大幅降低 KV 缓存的内存占用。我们发布了 SmallThinker-4B-A0.6B 和 SmallThinker-21B-A3B 两个版本，其性能不仅达到当前最优水平，甚至超越规模更大的 LLM。特别值得注意的是，该设计在多数情况下无需昂贵 GPU 硬件支持：经 Q4_0 量化后，两个模型在消费级 CPU 上均可实现超过 20 token/s 的推理速度，内存占用仅分别为 1GB 和 8GB。模型已开源：hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct 和 hf.co/PowerInfer/SmallThinker-21BA3B-Instruct。

