## Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights
[拖放式大语言模型：零样本提示到权重](https://arxiv.org/abs/2506.16406)

现代参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）方法（如低秩适应（low-rank adaptation, LoRA））虽然降低了大语言模型（large language models, LLMs）的定制成本，但仍需为每个下游数据集执行独立的优化过程。本文提出拖放式大语言模型（DnD），这是一种基于提示的条件参数生成器，通过将少量未标注的任务提示直接映射为 LoRA 权重更新，从而消除逐任务训练的需求。
该方法采用轻量级文本编码器将提示批次压缩为条件嵌入，再通过级联超卷积解码器将其转换为完整的 LoRA 矩阵集合。在多样化提示-检查点对数据集上完成训练后，DnD 可在数秒内生成任务专用参数，具有以下优势：
	1.	相比全微调降低 12,000× 的计算开销；
	2.	在未见过的常识推理、数学、编码及多模态基准测试中，平均性能超越最优训练 LoRA 方法达 30%；
	3.	即使未接触目标域数据或标签，仍展现出强大的跨域泛化能力。
实验结果表明，基于提示的条件参数生成是替代梯度调整方法、实现大语言模型快速专业化的有效方案。项目地址：https://jerryliang24.github.io/DnD

## Light of Normals: Unified Feature Representation for Universal Photometric Stereo
[法线统一表征：通用光度立体的特征表示方法](https://arxiv.org/abs/2506.18882)

通用光度立体（Photometric Stereo，PS）的目标是在不依赖特定光照模型的条件下，从任意光照场景中重建高质量表面法线。尽管SDM-UniPS和Uni MS-PS等最新方法取得了进展，但仍面临两个核心挑战：1) 光照变化与表面法线特征之间的强耦合关系，观测强度的模糊性导致难以区分亮度变化源自光照改变还是法线方向变化；2) 复杂曲面的高频几何细节保持问题，其中精细几何结构会引发自阴影、互反射及细微法线变化，而传统特征处理方法难以精确捕捉这些细节。

## Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal Document Understanding
[视觉引导分块：基于多模态文档理解的 RAG 增强方法](https://arxiv.org/abs/2506.16035)

检索增强生成 (Retrieval-Augmented Generation, RAG) 系统革新了信息检索与问答领域，但传统基于文本的分块方法难以有效处理复杂文档结构、跨页表格、嵌入式图表及页面间的上下文关联。本文提出一种新型多模态文档分块方法，利用大型多模态模型 (Large Multimodal Models, LMMs) 实现 PDF 文档的批处理，同时保持语义连贯与结构完整。该方法采用可配置的页面批处理机制并保留跨批上下文，能准确处理跨页表格、嵌入式视觉元素及流程性内容。我们在包含人工标注查询的精选 PDF 文档数据集上评估了该方法，实验表明其显著提升了分块质量与下游 RAG 性能。相较于传统基础 RAG 系统，视觉引导方法展现出更高的准确性，定性分析证实其具有更优的文档结构保留能力与语义连贯性。

## OmniGen2: Exploration to Advanced Multimodal Generation
[OmniGen2：探索先进多模态生成](https://arxiv.org/abs/2506.18871)

本研究提出OmniGen2——一个多功能开源生成模型，旨在为文本到图像(text-to-image)、图像编辑和上下文生成(in-context generation)等多样化任务提供统一解决方案。相较于OmniGen v1，OmniGen2采用独立设计的文本与图像模态解码路径，通过独立参数和分离式图像分词器(decoupled image tokenizer)实现模态解耦。该设计使模型能基于现有多模态理解模型直接构建，无需重新适配VAE输入，同时保持原始文本生成能力。为训练OmniGen2，我们开发了包含图像编辑和上下文生成数据的全流程数据构建方案，并针对图像生成任务设计了专用反射机制(reflection mechanism)，基于OmniGen2构建了专项反射数据集。尽管参数规模适中，该模型在文本到图像和图像编辑等多项基准测试中表现优异。为评估上下文生成（亦称主题驱动任务），我们提出了OmniContext新基准测试。实验表明，OmniGen2在一致性方面达到开源模型的SOTA(state-of-the-art)性能。我们将公开模型、训练代码、数据集及数据构建流程以推动相关研究。项目页面：https://vectorspacelab.github.io/OmniGen2；GitHub链接：https://github.com/VectorSpaceLab/OmniGen2

## ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation
[ShareGPT-4o-Image: 实现多模态模型的 GPT-4o 级图像生成能力](https://arxiv.org/abs/2506.18095)

多模态生成模型的最新突破使得生成符合指令的照片级真实图像成为可能，然而诸如 GPT-4o-Image 等领先系统仍为专有且无法公开访问。为开放这些能力，我们提出了 ShareGPT-4o-Image，这是首个包含 4.5 万条文本到图像和 4.6 万条文本加图像到图像数据的数据集，所有数据均利用 GPT-4o 的图像生成能力合成，旨在继承其先进的图像生成特性。基于该数据集，我们开发了 Janus-4o——一个支持文本到图像及文本加图像到图像生成的多模态大语言模型。Janus-4o 不仅显著提升了前代模型 Janus-Pro 的文本到图像生成能力，还新增了文本加图像到图像的生成功能。特别值得注意的是，该模型仅使用 9.1 万条合成样本，在 8 台 A800 GPU 服务器上训练 6 小时，就实现了完全从零开始训练的文本加图像到图像生成的优异性能。我们期望 ShareGPT-4o-Image 数据集和 Janus-4o 模型的发布能够推动符合指令的照片级真实图像生成领域的开放研究。

## JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent
[JarvisArt: 基于智能照片润色智能体解放人类艺术创造力](https://arxiv.org/abs/2506.17612)

照片润色已成为当代视觉叙事的重要组成部分，能够帮助用户呈现美学效果并表达创作意图。虽然 Adobe Lightroom 等专业工具具备强大的功能，但需要较高的专业知识和大量人工调整。相比之下，现有基于 AI 的解决方案虽然实现了自动化，但普遍存在调节灵活性不足和泛化性能较差的问题，难以满足多样化、个性化的编辑需求。为此，我们提出 JarvisArt——一个由多模态大语言模型 (MLLM) 驱动的智能体系统，能够理解用户意图、模拟专业艺术家的决策流程，并智能调度 Lightroom 内超过 200 种润色工具。该系统采用两阶段训练流程：首先通过思维链式监督微调建立基础推理和工具使用能力，再通过润色专用相对策略优化 (GRPO-R) 进一步提升决策能力和工具掌握度。我们还设计了 Agent-to-Lightroom 协议来实现与 Lightroom 的无缝集成。为评估系统性能，我们构建了 MMArt-Bench 基准测试集，其数据源自真实用户编辑记录。JarvisArt 具备友好的交互界面、出色的泛化性能，以及对全局和局部调整的精细控制能力，为智能照片润色领域开辟了新方向。值得注意的是，在 MMArt-Bench 基准测试中，该系统在内容保真度的平均像素级指标上较 GPT-4o 提升 60%，同时保持相当的指令遵循能力。项目主页：https://jarvisart.vercel.app/。

## Matrix-Game: Interactive World Foundation Model
[Matrix-Game: 交互式世界基础模型](https://arxiv.org/abs/2506.18701)

我们提出Matrix-Game，这是一种面向可控游戏世界生成的交互式世界基础模型。该模型采用两阶段训练流程：首先通过大规模无标注预训练实现环境理解，再通过动作标注训练实现交互式视频生成。为此，我们构建了Matrix-Game-MC数据集，包含超过2,700小时的无标注Minecraft游戏视频片段，以及1,000余小时带有细粒度键盘鼠标动作标注的高质量标注片段。本模型采用基于参考图像、运动上下文和用户动作的可控图像-世界生成范式。凭借170亿参数量，Matrix-Game能精准控制角色动作与镜头运动，同时保持优异的视觉质量和时序一致性。为评估性能，我们开发了GameWorld Score基准测试体系，从视觉质量、时序质量、动作可控性和物理规则理解四个维度对Minecraft世界生成进行量化评估。实验表明，Matrix-Game在所有指标上均显著优于现有开源Minecraft世界模型（包括Oasis和MineWorld），尤其在可控性和物理一致性方面优势明显。双盲人工评估进一步验证了Matrix-Game的优越性，其能够在多样化游戏场景中生成具有感知真实性且精确可控的视频。为促进交互式图像-世界生成领域研究，我们将在https://github.com/SkyworkAI/Matrix-Game开源Matrix-Game模型权重及GameWorld Score基准。

## PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models
[PAROAttention：面向视觉生成模型高效稀疏与量化注意力的模式感知重排序](https://arxiv.org/abs/2506.16054)

在视觉生成领域，注意力机制的二次计算复杂度会带来极高的内存和计算开销，特别是在生成高分辨率图像或多帧视频所需的长序列Token时尤为明显。为解决这一问题，现有研究已探索了稀疏化 (sparsification) 和量化 (quantization) 等技术方案。然而，这些技术在低密度和低位宽条件下存在显著局限性。通过系统性分析，我们发现根本难点在于视觉注意力模式具有分散性和不规则性特征。为此，我们提出了一种创新策略：通过*重排序*操作重构注意力模式来应对这些挑战，而非设计专门的稀疏化与量化方案来适应原有模式。基于视觉特征提取的局部聚合特性，我们开发了新型**模式感知Token重排序 (Pattern-Aware token ReOrdering, PARO)** 技术，将异构的注意力模式统一转换为硬件友好的块状结构模式。这种模式统一化极大简化并优化了稀疏化与量化过程。我们系统评估了不同设计方案在性能与效率之间的权衡关系，最终形成了针对统一模式的定制化方法。所提出的**PAROAttention**方案在视频和图像生成任务中实现了无损质量指标，其生成结果与全精度 (FP) 基线几乎完全一致，同时仅需维持20%-30%的密度和**INT8/INT4**位宽，最终获得**1.9倍**到**2.7倍**的端到端延迟加速。

## AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models
[AnimaX：基于联合视频-姿态扩散模型的3D无生命体动画生成](https://arxiv.org/abs/2506.19851)

我们提出AnimaX——一种前馈3D动画框架，该框架将视频扩散模型(video diffusion models)的运动先验与基于骨骼动画的可控结构相结合。传统运动合成方法通常受限于固定骨骼拓扑结构，或需在高维变形空间中进行代价高昂的优化。相较之下，AnimaX能有效将视频运动知识迁移至3D领域，支持任意骨骼结构的多样化可动网格。本方法将3D运动表示为多视角、多帧的2D姿态映射图(pose maps)，并通过模板渲染和文本运动提示实现联合视频-姿态扩散。我们采用共享位置编码和模态感知嵌入技术，确保视频与姿态序列间的时空对齐，从而有效迁移视频先验至运动生成任务。最终通过三角测量将多视角姿态序列转换为3D关节位置，并利用反向运动学(inverse kinematics)生成网格动画。基于包含16万条绑定动画序列的新建数据集训练后，AnimaX在VBench评测中实现了泛化性、运动保真度及效率的业界最优表现，为类别无关性(category-agnostic)3D动画提供了可扩展解决方案。项目主页：
\href{https://anima-x.github.io/}{https://anima-x.github.io/}。

## LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning
[LongWriter-Zero：通过强化学习实现超长文本生成](https://arxiv.org/abs/2506.18841)  

大语言模型（LLMs）的超长文本生成是需求广泛的应用场景，但由于最大生成长度限制和序列增长导致的质量衰减问题，这仍是重大技术挑战。现有方法（如 LongWriter）通常采用"教学"策略，即在合成生成长文本上进行监督微调（SFT）。然而，该方法严重依赖合成SFT数据，这类数据不仅构建成本高昂，还存在连贯性不足、一致性缺失，以及明显的人工痕迹和结构单一性问题。本文提出激励驱动的方法，完全从零开始且无需任何标注或合成数据，通过强化学习（RL）促使LLM自发形成超长高质量文本生成能力。我们基于类似 R1-Zero 的基础模型进行RL训练，引导模型执行支持写作规划与优化的推理过程。为此，采用专用奖励模型来提升模型的长度控制能力、写作质量和结构规范性。实验表明，基于Qwen2.5-32B训练的LongWriter-Zero在长文本写作任务中全面超越传统SFT方法，在WritingBench和Arena-Write所有指标上达到最先进水平，甚至优于DeepSeek R1和Qwen3-235B等百亿级模型。相关数据和模型检查点已开源：https://huggingface.co/THU-KEG/LongWriter-Zero-32B

## Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs
[Skywork-SWE: 揭示大语言模型中软件工程的数据扩展定律](https://arxiv.org/abs/2506.19290)  

软件工程(SWE)已成为评估下一代大语言模型智能体的关键试验场，其要求模型具备两项核心能力：持续迭代式问题求解(如>50次交互轮次)和长上下文依赖关系处理(如>32k tokens)。然而，当前SWE数据构建流程仍面临显著效率瓶颈，因其高度依赖人工标注的代码文件筛选，以及需要配置独立运行时环境来执行单元测试验证。这导致现有数据集规模普遍受限，通常仅包含数千个GitHub来源的实例。  

为此，我们提出了一种渐进式自动化数据构建管道，可系统性扩展SWE数据集的规模与多样性。该数据集包含来自2,531个独立GitHub仓库的10,169个真实场景Python任务实例，每个实例均配备自然语言描述的任务说明和用于自动化单元测试验证的独立运行时环境镜像。我们从构建的SWE数据集中精选出8,000余个通过运行时验证的训练样本序列。  

基于这些样本微调Skywork-SWE模型时，我们观察到显著的数据规模扩展现象：随着训练数据量增长，模型在软件工程任务上的表现持续提升且未出现饱和趋势。值得注意的是，Skywork-SWE模型在SWE-bench Verified基准测试中取得38.0% pass@1准确率(未使用验证模块或多轮采样)，在基于OpenHands智能体框架构建的Qwen2.5-Coder-32B大语言模型中创下state-of-the-art(SOTA)记录。通过引入测试阶段扩展技术，该性能进一步提升至47.0%准确率，超越所有32B参数以下模型的先前最佳结果。我们开源Skywork-SWE-32B模型检查点以促进后续研究。

## Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition
[Hunyuan-GameCraft：混合历史条件驱动的高动态游戏交互视频生成](https://arxiv.org/abs/2506.17201)

扩散模型（diffusion-based）与可控视频生成领域的最新突破，使得高质量时序连贯的视频合成成为可能，这为构建沉浸式游戏交互体验提供了技术基础。然而现有方法在动态表现、泛化能力、长程一致性及计算效率等方面仍存在局限，难以生成多样化的游戏过程视频。为此，我们提出Hunyuan-GameCraft框架，专门针对游戏环境的高动态交互视频生成需求。该框架首先将标准键鼠输入统一编码至共享的相机表征空间，实现不同相机运动与位移操作之间的无缝过渡。继而提出混合历史条件训练策略，通过自回归（autoregressive）方式扩展视频序列的同时，有效保留游戏场景信息。为提高推理效率与可玩性，采用模型蒸馏技术降低计算成本，并保持长时序列的一致性，使其能够适应复杂交互场景的实时需求。模型训练采用覆盖100余款3A游戏的超百万条实录数据集确保多样性，并通过精细标注的合成数据集进行微调以提升控制精度。经筛选的游戏场景数据显著改善了生成结果的视觉保真度、真实感及动作可控性。大量实验证明，Hunyuan-GameCraft在交互式游戏视频生成的拟真度与可玩性方面均显著超越现有模型。

