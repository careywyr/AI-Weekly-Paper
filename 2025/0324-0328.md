## I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders  
[全面解析：基于稀疏自编码器的大语言模型推理特征阐释](https://arxiv.org/abs/2503.18878)  

大语言模型 (LLMs) 在自然语言处理领域取得了突破性进展。近期研究推动了一类新型推理大语言模型的发展：例如，开源模型 DeepSeek-R1 通过融合深层语义理解与复杂逻辑推理，实现了当前最优性能。尽管这些能力表现卓越，其内部推理机制仍缺乏系统性研究。本文采用稀疏自编码器 (SAEs) —— 一种将神经网络潜在表征解耦为稀疏可解释特征的技术，用于识别 DeepSeek-R1 系列模型中的核心推理特征。我们首先提出从 SAE 表征中提取候选“推理特征”的方法，并通过实证分析与可解释性技术验证这些特征与模型推理能力的直接关联。关键贡献在于，通过调控这些特征可显著提升推理性能，首次为大语言模型的推理机制提供了可解释性论证。代码发布于 https://github.com/AIRI-Institute/SAE-Reasoning

## Qwen2.5-Omni Technical Report
[Qwen2.5-Omni 技术报告](https://arxiv.org/abs/2503.20215)

本报告介绍了 Qwen2.5-Omni，这是一个端到端多模态模型，能够感知文本、图像、音频和视频等多种模态，并以流式方式同步生成文本和自然语音响应。为实现多模态信息输入的流式处理，音频与视觉编码器均采用分块处理技术。为保持视频与音频输入的时间戳同步，我们采用交错排列方式组织音视频数据，并提出新型位置嵌入方法 TMRoPE (Time-aligned Multimodal RoPE)。为避免文本与语音生成时的模态干扰，我们提出 **Thinker-Talker** 架构：其中 Thinker 作为大语言模型负责文本生成，Talker 则是双通道自回归模型，直接利用 Thinker 的隐层表征输出音频 Token。两者均采用端到端的训练与推理方式。针对音频 Token 的流式解码，我们引入滑动窗口 DiT (Diffusion Transformer) 以限制感受野，从而降低初始数据包延迟。Qwen2.5-Omni 性能与同规模 Qwen2.5-VL 相当，优于 Qwen2-Audio，并在 Omni-Bench 等多模态基准测试中取得业界领先性能。特别值得注意的是，根据 MMLU 和 GSM8K 等基准测试，该模型在端到端语音指令跟随任务中的表现与文本输入能力相当。在语音生成方面，Qwen2.5-Omni 的流式 Talker 在鲁棒性与自然度指标上优于当前多数流式与非流式方案。

## Video-T1: Test-Time Scaling for Video Generation
[Video-T1: 视频生成的测试时扩展](https://arxiv.org/abs/2503.18942)

随着训练数据规模、模型参数量和计算成本的提升，视频生成技术在数字创作领域取得了显著成果，使用户能够在多个领域实现创意表达。近期，大语言模型 (LLMs) 研究者将规模扩展延伸至测试阶段，通过增加推理计算量显著提升了模型性能。不同于通过高昂训练成本扩展视频基础模型的方法，我们探索了测试时扩展 (Test-Time Scaling, TTS) 在视频生成中的潜力，旨在解答：当视频生成模型被允许使用可观推理计算资源时，面对具有挑战性的文本提示，其生成质量能获得多大提升。本工作将视频生成的测试时扩展重新定义为搜索问题，即从高斯噪声空间向目标视频分布采样更优轨迹的过程。具体而言，我们构建了包含测试时验证器的搜索空间以提供反馈，并采用启发式算法指导搜索流程。针对给定文本提示，首先探索通过增加推理时噪声候选的线性搜索策略。鉴于全帧同步去噪需要极高测试时计算成本，我们进一步设计了更高效的视频生成 TTS 方法——帧树 (Tree-of-Frames, ToF)，该方法以自回归方式自适应扩展和修剪视频分支。在文本条件视频生成基准上的大量实验表明，增加测试时计算量能持续显著提升视频生成质量。项目页面：https://liuff19.github.io/Video-T1

## When Less is Enough: Adaptive Token Reduction for Efficient Image Representation
[适量即优：面向高效图像表征的自适应 Token 缩减](https://arxiv.org/abs/2503.16660)

视觉编码器通常会产生大量视觉 Token，虽然能提供信息丰富的表征，但会显著增加计算需求。这促使我们思考：所有生成的 Token 是否具有同等价值？是否可以通过舍弃部分 Token 来降低计算成本而不影响模型性能？本文提出了一种基于特征重构理论的新方法——低价值特征可以从高价值特征中重建，并据此评估特征效用。具体实现上，我们整合了自编码器与 Gumbel-Softmax 选择机制，能够有效识别并保留最具信息量的视觉 Token。为验证该方法，我们将 LLaVA-NeXT 模型采用本方法筛选的特征与随机选择特征进行了性能对比。实验表明，在基于 OCR 的任务中，移除超过 50% 的视觉上下文仅导致轻微性能下降，而随机丢弃同等比例特征则会造成显著的性能损失。对于通用领域任务，即使随机保留 30% 的 Token 也能达到与使用完整 Token 集相当的模型表现。这些结果表明，自适应高效多模态剪枝是实现可扩展、低开销推理且保持模型性能的有效途径。

## Long-Context Autoregressive Video Modeling with Next-Frame Prediction
[基于下一帧预测的长上下文自回归视频建模](https://arxiv.org/abs/2503.19325)

长上下文自回归建模虽在语言生成领域取得重大突破，但视频生成领域仍无法有效利用扩展的时间上下文。为探索长上下文视频建模，我们提出帧自回归模型（FAR）作为视频自回归建模的基准框架。该框架通过建模连续帧间的时间因果依赖关系（类比语言模型中token间的因果依赖，即Token AR），在收敛性能上优于Token AR和视频扩散Transformer。基于FAR的研究发现，长上下文视觉建模主要受视觉冗余问题制约：现有RoPE方法缺乏有效的远程上下文时间衰减机制，且长视频序列的外推能力不足。同时，由于视觉token数量呈指数级增长，长视频训练面临巨大计算开销。针对这些问题，我们提出局部性与长程依赖的平衡策略：首先设计推理阶段技术FlexRoPE，通过为RoPE引入可调时间衰减，实现16倍视觉上下文长度的外推；其次提出长短时上下文联合建模方案，其中高分辨率短时窗口保障时序细节一致性，而token高效的长时窗口则捕获全局信息。该方法使得长视频序列训练可在可控的token上下文长度下进行。实验证明，FAR在长短视频生成任务中均达到当前最优性能，为视频自回归建模提供了高效基准框架。

## Video-R1: Reinforcing Video Reasoning in MLLMs  
[Video-R1：强化多模态大语言模型的视频推理能力](https://arxiv.org/abs/2503.21776)  

受DeepSeek-R1通过基于规则的强化学习（RL）激发推理能力的启发，我们提出Video-R1，首次系统探索R1范式在多模态大语言模型（MLLM）中实现视频推理的方法。然而，直接使用GRPO算法进行视频推理的RL训练存在两大挑战：(i) 视频推理缺乏时序建模；(ii) 高质量视频推理数据稀缺。为此，我们提出T-GRPO算法，促使模型利用视频时序信息进行推理。同时，训练数据不局限于视频，还引入高质量图像推理数据。我们构建了两个数据集：用于SFT冷启动的Video-R1-COT-165k和用于RL训练的Video-R1-260k，二者都包含图像和视频数据。实验表明，Video-R1在VideoMMMU、VSI-Bench等视频推理基准，以及MVBench、TempCompass等通用视频基准上均取得显著提升。其中，Video-R1-7B在视频空间推理基准VSI-bench上达到35.8%准确率，超越商业闭源模型GPT-4o。所有代码、模型和数据均已开源。

## Position: Interactive Generative Video as Next-Generation Game Engine
[立场：交互式生成视频作为下一代游戏引擎](https://arxiv.org/abs/2503.17359)

传统游戏引擎中预设内容的局限性使得现代游戏开发面临创意与成本的双重挑战。近期视频生成模型取得突破性进展，能够合成逼真且可交互的虚拟环境，这为游戏创作带来了革命性可能。本文主张将交互式生成视频（Interactive Generative Video, IGV）作为生成式游戏引擎（Generative Game Engine, GGE）的基础架构，从而在下一代游戏中实现无限新颖内容的生成。GGE 充分发挥 IGV 的独特优势：无限高质量内容生成、物理感知的世界建模、用户可控的交互性、长期记忆能力以及因果推理。我们提出了详细的技术框架，阐明 GGE 的核心模块，并制定了分层成熟度路线图（L0-L4）以指导其发展。这项工作为 AI 时代的游戏开发开辟了新方向，展望了由 AI 驱动的生成式系统彻底重塑游戏创作与体验的未来图景。

## MAPS: A Multi-Agent Framework Based on Big Seven Personality and Socratic Guidance for Multimodal Scientific Problem Solving  
[MAPS：基于大七人格与苏格拉底引导的多智能体科学问题求解框架](https://arxiv.org/abs/2503.16905)  

多模态科学问题 (MSPs) 是指需要整合文本、图表等多模态信息的复杂问题，是人工智能领域的重要挑战。尽管传统科学问题的研究已取得进展，但MSPs仍存在两大核心难题：科学问题解决过程中的多模态综合推理能力不足，以及系统缺乏反思与迭代优化能力。为此，我们提出基于大七人格特质与苏格拉底式引导的多智能体框架 (MAPS)。该框架通过七个功能各异的智能体，结合反馈机制与苏格拉底问答法来指导MSPs求解。针对第一个难题，我们设计了分阶段递进的四智能体协作策略，各智能体分别负责问题解决流程中的特定环节；针对第二个难题，我们开发了基于苏格拉底诘问法的评审智能体 (Critic Agent)，用于激发系统批判性思维与自主学习能力。在EMMA、Olympiad和MathVista数据集上的实验表明，该框架在所有任务中平均超越当前SOTA模型15.84%。消融实验进一步验证了模型性能提升及其泛化能力。

## A Comprehensive Survey on Long Context Language Modeling  
[长上下文语言建模全面综述](https://arxiv.org/abs/2503.17407)  

高效处理长上下文始终是自然语言处理领域的核心挑战。随着长文档、对话等文本数据的快速增长，开发能高效处理大规模输入的长上下文语言模型（Long Context Language Models, LCLMs）具有重要意义。本文系统梳理了大语言模型在长上下文建模领域的最新进展，围绕三个核心维度展开：(1) 高效LCLMs的构建方法；(2) LCLMs的训练与部署优化；(3) LCLMs的评估与分析框架。针对第一个维度，我们阐述了面向长上下文处理的数据策略、架构设计和工作流方法；针对第二个维度，详细剖析了LCLM训练推理所需的基础设施支撑；针对第三个维度，提出了长上下文理解与长文本生成的评估体系，以及LCLMs的行为分析和机制解释方法。此外，我们系统梳理了LCLMs的现有应用场景，并展望了未来发展方向。本综述旨在为研究者与工程师提供长上下文大语言模型领域的最新文献参考，相关GitHub仓库持续更新前沿论文与代码库：  
\href{https://github.com/LCLM-Horizon/A-Comprehensive-Survey-For-Long-Context-Language-Modeling}{\color[RGB]{175,36,67}{LCLM-Horizon}}。

## Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy
[Dita：面向通用视觉-语言-动作策略的扩散Transformer扩展框架](https://arxiv.org/abs/2503.19757)

尽管当前基于多样化机器人数据集训练的视觉-语言-动作(Vision-Language-Action)模型在有限领域数据下已展现出优异的泛化能力，但其采用紧凑动作头(compact action heads)预测离散或连续动作的设计，限制了模型对异构动作空间的适应能力。本文提出Dita框架，通过统一的多模态扩散过程，利用Transformer架构直接对连续动作序列进行去噪。与现有基于浅层网络融合嵌入的条件去噪方法不同，Dita采用上下文条件机制(in-context conditioning)，实现了去噪动作与历史观测原始视觉Token之间的细粒度对齐。该设计显式建模了动作差分(action deltas)和环境细微特征。通过结合扩散动作去噪器与Transformer的可扩展性优势，Dita能够有效整合跨具身(cross-embodiment)数据集，涵盖多视角相机、多样化观测场景、任务及动作空间。这种协同设计显著提升了对各类环境变量的鲁棒性，并成功实现了长时序(long-horizon)任务的执行。在广泛基准测试中，Dita在仿真环境达到最先进或可比性能。值得注意的是，仅需10样本微调，Dita即可通过第三人称相机输入，实现对现实环境变量和复杂长时序任务的鲁棒适应。该架构为通用机器人策略学习提供了轻量级、开源的多功能基准。项目页面：https://robodita.github.io。

