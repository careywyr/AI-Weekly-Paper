## QeRL：超越效率——面向大语言模型的量化增强强化学习
[QeRL：超越效率——面向大语言模型的量化增强强化学习](https://arxiv.org/abs/2510.11696)

我们提出 QeRL，一种面向大语言模型 (LLMs) 的量化增强强化学习框架。尽管强化学习对大语言模型的推理能力至关重要，但其资源消耗大，需要大量 GPU 内存和较长推演周期。QeRL 通过结合 NVFP4 量化与低秩适应 (LoRA) 解决这些问题，在降低内存开销的同时加速强化学习的推演阶段。除提升效率外，我们发现量化噪声能增加策略熵，从而增强探索能力，帮助在强化学习过程中发现更优策略。为进一步优化探索，QeRL 引入自适应量化噪声 (AQN) 机制，可在训练过程中动态调整噪声强度。实验表明，QeRL 在推演阶段实现超过 1.5 倍的加速。此外，该框架是首个能在单张 H100 80GB GPU 上完成 32B 大语言模型强化学习训练的解决方案，同时显著提升整体训练速度。与 16 位 LoRA 和 QLoRA 相比，QeRL 不仅获得更快的奖励增长和更高最终准确率，在 7B 模型的数学基准测试（如 GSM8K (90.8%) 和 MATH 500 (77.4%)）中更达到与全参数微调相当的性能。这些成果证明 QeRL 是实现大语言模型强化学习训练的高效且有效框架。

## 基于表示自编码器的扩散变换器
[基于表示自编码器的扩散变换器](https://arxiv.org/abs/2510.11690)

潜在生成建模采用预训练自编码器将像素映射至潜在空间以供扩散过程使用，这已成为扩散变换器 (DiT) 的标准策略。然而，自编码器组件始终缺乏实质性演进。目前大多数 DiT 仍沿用原始 VAE 编码器，这带来若干局限：过时的骨干网络影响架构简洁性，低维潜在空间限制信息容量，以及纯重建训练导致的表征能力不足，最终制约生成质量。本文探索使用预训练表示编码器（如 DINO、SigLIP、MAE）与训练好的解码器组合替代 VAE，构建称为表示自编码器 (RAE) 的新型架构。该模型不仅能提供高质量重建结果和语义丰富的潜在空间，还支持可扩展的变换器架构。由于此类潜在空间通常具有高维特性，关键挑战在于如何使扩散变换器在其中高效运行。我们系统分析了问题根源，提出基于理论的解决方案，并通过实验验证其有效性。该方法无需辅助表示对齐损失即可实现更快收敛。采用配备轻量级宽幅 DDT 头的 DiT 变体，在 ImageNet 上获得了优异的图像生成效果：256×256 分辨率下无引导 FID 为 1.51，有引导时 256×256 和 512×512 分辨率下均达 1.13。RAE 展现出显著优势，应作为扩散变换器训练的新标准方案。

## 空间强制：视觉-语言-动作模型的隐式空间表示对齐
[空间强制：视觉-语言-动作模型的隐式空间表示对齐](https://arxiv.org/abs/2510.12276)

视觉-语言-动作 (VLA) 模型近期展现出强大潜力，能够驱动机器人根据语言指令执行精确动作。然而，大多数 VLA 模型基于仅使用 2D 数据预训练的视觉-语言模型构建，这些模型缺乏准确的空间感知能力，从而限制了其在三维物理环境中的操作性能。现有解决方案尝试引入显式三维传感器输入（如深度图或点云），但由于传感器噪声、硬件异构性以及现有数据集深度覆盖不完整等问题，这些方法面临诸多挑战。其他从二维图像估计三维线索的替代方法，则受限于深度估计器的性能瓶颈。我们提出空间强制 (SF)，一种简单有效的对齐策略，无需依赖显式三维输入或深度估计器，即可隐式引导 VLA 模型构建空间理解能力。SF 通过将 VLA 的中间视觉嵌入与预训练三维基础模型输出的几何表示进行对齐，在中间网络层实施对齐约束，从而指导 VLA 编码更丰富的空间表征以提升动作精度。仿真与真实环境中的大量实验表明，SF 实现了最先进的性能，显著优于基于二维和三维数据的 VLA 方法。此外，SF 将训练速度最高提升 3.8 倍，并在多种机器人任务中展现出更优的数据效率。项目页面详见 https://spatial-forcing.github.io/

## D2E：基于桌面数据的视觉-动作预训练规模化以迁移至具身AI
[D2E：基于桌面数据的视觉-动作预训练规模化以迁移至具身AI](https://arxiv.org/abs/2510.05684)

大语言模型利用互联网规模的文本数据，但具身AI仍受限于物理轨迹数据收集的过高成本。桌面环境——尤其是游戏——提供了一种可行的替代方案：它们能够大规模提供丰富的感知运动交互，同时维持对具身学习至关重要的结构化观察-动作耦合。我们提出了D2E（桌面到具身AI）框架，证明桌面交互可作为机器人具身AI任务的有效预训练基座。与先前工作局限于领域专用（例如，针对Minecraft的VPT）或数据私有（例如，SIMA）不同，D2E建立了一个从可扩展桌面数据收集到具身领域验证迁移的完整流水线。我们的框架包含三个组件：(1) OWA工具包，将多样桌面交互统一为标准格式，并实现152倍压缩；(2) Generalist-IDM，通过时间戳事件预测在未见游戏中实现强大的零样本泛化，支持互联网规模的伪标签；(3) VAPT，将桌面预训练表示迁移至物理操作和导航任务。使用1300+小时数据（包括259小时人类演示和1000+小时伪标签游戏玩法），我们在LIBERO操作基准上达到总成功率96.6%，在CANVAS导航基准上达到83.3%。这验证了数字交互中的感知运动基元具有足够的不变性，能够有效迁移到物理具身任务，从而确立了桌面预训练作为机器人学的实用范式。我们将公开所有工作成果，包括OWA工具包、人类收集和伪标签的数据集，以及VAPT训练模型，可在https://worv-ai.github.io/d2e/获取。

## 用相机思考：面向相机中心理解与生成的统一多模态模型
[用相机思考：面向相机中心理解与生成的统一多模态模型](https://arxiv.org/abs/2510.08673)

相机中心理解与生成是空间智能的核心基础，但现有研究多将其分离处理。本文提出Puffin模型，作为一种统一的相机中心多模态模型，沿相机维度扩展了空间感知能力。Puffin融合语言回归与基于扩散的生成技术，能够从任意视点解析并构建场景。为弥合相机与视觉语言模态间的差异，我们创新性地将相机参数视为语言元素，实现“用相机思考”的范式。该方法引导模型在几何上下文推理过程中，将空间锚定的视觉特征与摄影术语进行关联对齐。Puffin基于Puffin-4M数据集训练，该数据集包含400万组视觉-语言-相机三元组。通过整合全局相机参数与像素级相机映射，模型实现了灵活可靠的空间内容生成。实验表明，Puffin在相机中心生成与理解任务上的性能超越专用模型。经指令调优后，该模型可泛化应用于空间想象、场景探索、摄影指导等多类跨视角任务。我们将公开代码、模型、数据集流水线及评估基准，以促进多模态空间智能研究发展。

## 通过自监督预训练推进端到端像素空间生成建模
[通过自监督预训练推进端到端像素空间生成建模](https://arxiv.org/abs/2510.12586)

像素空间生成模型通常比潜在空间模型更难训练且性能普遍较差，导致存在持续的效能与效率差距。本文提出一种新颖的两阶段训练框架，针对像素空间扩散模型和一致性模型有效缩小了这一差距。第一阶段，我们预训练编码器从干净图像中提取有意义的语义特征，同时将其与同一确定性采样轨迹上的点对齐，该轨迹将样本从先验分布映射到数据分布。第二阶段，我们将编码器与随机初始化的解码器集成，并对完整模型进行端到端微调，适用于扩散模型和一致性模型。我们的训练框架在 ImageNet 数据集上表现出强大的实证性能：扩散模型在 ImageNet-256 上达到 FID 2.04，在 ImageNet-512 上达到 FID 2.35（使用 75 次函数评估 NFE），在生成质量和效率上均大幅超越先前像素空间方法，并在可比训练成本下与领先的 VAE 基模型性能相当。此外，在 ImageNet-256 上，我们的一致性模型在单步采样中实现了 FID 8.82 的优异结果，显著优于其潜在空间版本。据我们所知，这是首次无需依赖预训练 VAE 或扩散模型，直接在高分辨率图像上成功训练一致性模型。

## 当模型产生幻觉时，我们从中学习：基于 PsiloQA 的多语言跨度级幻觉检测
[当模型产生幻觉时，我们从中学习：基于 PsiloQA 的多语言跨度级幻觉检测](https://arxiv.org/abs/2510.04849)

幻觉检测始终是大语言模型 (LLMs) 安全可靠部署的核心挑战，尤其是在要求事实准确性的应用场景中。现有幻觉基准大多在序列级别运行且仅支持英语，缺乏全面评估所需的细粒度多语言监督数据。本研究提出了 PsiloQA——一个涵盖 14 种语言跨度级幻觉标注的大规模多语言数据集。该数据集通过自动化三阶段流程构建：首先使用 GPT-4o 基于维基百科生成问答对，接着在无上下文环境下通过多样化 LLMs 诱导生成可能包含幻觉的答案，最后通过对比标准答案与检索上下文，利用 GPT-4o 自动标注幻觉跨度。我们系统评估了多种幻觉检测方法（包括不确定性量化、基于 LLM 的标记方法和微调编码器模型），结果表明基于编码器的模型在跨语言场景下表现最优。此外，PsiloQA 展现出卓越的跨语言泛化能力，并能有效支持向其他基准的知识迁移，其构建成本显著低于人工标注数据集。本数据集及相关研究成果将推动多语言场景下可扩展细粒度幻觉检测技术的发展。

## 规模化语言中心的全模态表示学习
[规模化语言中心的全模态表示学习](https://arxiv.org/abs/2510.11693)

近期，基于多模态大语言模型 (MLLMs) 并通过对比学习 (CL) 微调的多模态嵌入方法取得了显著成果，但其性能优势的内在机制尚不明确。本文指出，MLLM 方法的核心优势源自生成式预训练中形成的隐式跨模态对齐：语言解码器学会在共享表示空间内利用多模态信号，以生成单模态输出。通过分析各向异性和核相似性结构，我们实证发现 MLLM 表示中自发形成了潜在对齐，使得 CL 能够作为轻量化的精炼阶段。基于这一发现，我们提出了语言中心的全模态嵌入框架 LCO-Emb。在不同骨干网络和基准测试上的大量实验表明，该框架能有效实现跨模态的顶尖性能。此外，我们发现了生成-表示缩放定律 (GRSL)，证明通过对比学习精炼获得的表示能力与 MLLM 的生成能力呈正缩放关系。这表明提升生成能力已成为增强表示质量的有效途径。我们从理论上解释了 GRSL，严格论证了 MLLM 生成质量与其表示性能上限的关联，并在一个高难度低资源视觉-文档检索任务中验证了该定律，证明在 CL 前进行持续生成式预训练可进一步提升模型的嵌入潜力。代码、模型及相关资源公开于 https://github.com/LCO-Embedding/LCO-Embedding。

## DITING：一个用于网络小说翻译基准评估的多智能体框架
[DITING：一个用于网络小说翻译基准评估的多智能体框架](https://arxiv.org/abs/2510.09116)

大语言模型 (LLMs) 显著推动了机器翻译 (MT) 的发展，但其在网络小说翻译中的有效性仍不明确。现有基准依赖表面级指标，无法捕捉该体裁的独特特征。为弥补这些不足，我们提出了 DITING——首个全面的网络小说翻译评估框架，从六个维度评估叙事与文化保真度：成语翻译、词汇歧义、术语本地化、时态一致性、零代词解析和文化安全，该框架基于超过 1.8 万对专家标注的中英句子对。我们进一步提出 AgentEval，这是一个推理驱动的多智能体评估框架，通过模拟专家审议机制，实现超越词汇重叠的翻译质量评估，在七种自动指标中与人类评判的相关性最高。为便于指标对比，我们开发了 MetricAlign 元评估数据集，包含 300 对带有错误标签和标量质量分数的句子对。对十四种开源、闭源及商业模型的综合评估表明：中文训练的 LLMs 优于参数规模更大的国外同类模型，其中 DeepSeek-V3 实现了最忠实且风格一致的翻译效果。本研究建立了基于 LLM 的网络小说翻译探索新范式，并提供了可推动未来研究的公共资源。

## 智能体熵平衡策略优化
[智能体熵平衡策略优化](https://arxiv.org/abs/2510.14545)

近期，智能体强化学习 (Agentic RL) 在提升网络智能体的多轮次、长周期工具使用能力方面取得显著进展。尽管主流智能体 RL 算法基于熵的指导自主探索高不确定性工具调用步骤，但过度依赖熵信号可能引入额外限制，导致训练失效。本文深入分析熵带来的挑战，提出智能体熵平衡策略优化 (AEPO)，该算法旨在平衡 rollout 和策略更新阶段的熵。AEPO 包含两个核心组件：(1) 动态熵平衡 rollout 机制，通过熵预监控自适应分配全局与分支采样预算，并对连续高熵工具调用步骤实施分支惩罚以避免过度分支问题；(2) 熵平衡策略优化，在高熵裁剪项中插入梯度停止操作以保留并适度缩放高熵 token 的梯度，同时结合熵感知优势估计优先学习高不确定性 token。在 14 个挑战性数据集上的实验表明，AEPO 持续超越 7 种主流 RL 算法。仅使用 1K RL 样本时，集成 AEPO 的 Qwen3-14B 取得显著成果：GAIA 的 Pass@1 为 47.6%，Humanity's Last Exam 为 11.2%，WebWalker 为 43.0%；GAIA 的 Pass@5 为 65.0%，Humanity's Last Exam 为 26.0%，WebWalker 为 70.0%。进一步分析显示，AEPO 在维持策略熵稳定的同时提升 rollout 采样多样性，助力可扩展网络智能体训练。

## 机器人学习：教程
[机器人学习：教程](https://arxiv.org/abs/2510.12403)

机器人学习正处在一个转折点，这得益于机器学习的飞速发展以及大规模机器人数据的日益普及。从传统的基于模型方法转向数据驱动的学习范式，正在为自主系统开启前所未有的能力。本教程将梳理现代机器人学习的全景，从强化学习和行为克隆的基本原理出发，逐步延伸到能够跨多种任务乃至不同机器人平台操作的通用型语言条件模型。本文旨在为研究人员和实践者提供指南，目标是让读者掌握必要的概念理解与实用工具，从而推动机器人学习的进步，并提供了在$\texttt{lerobot}$中实现的即用示例。

## WithAnyone：面向可控与身份一致的图像生成
[WithAnyone：面向可控与身份一致的图像生成](https://arxiv.org/abs/2510.14975)

身份一致性生成已成为文本到图像领域的重要研究方向，近期模型在生成与参考身份对齐的图像方面取得了显著进展。然而，由于缺乏包含同一主体多图像的大规模配对数据集，现有方法大多依赖基于重建的训练范式。这种模式容易引发称为“复制粘贴”的缺陷——模型直接复制参考人脸，而非在姿态、表情或光照的自然变化下保持身份一致性。这种过度相似问题会削弱生成过程的可控性，并限制其表现多样性。为解决这些局限，我们（1）构建了面向多人场景的大规模配对数据集MultiID-2M，为每个身份提供多样化参考；（2）建立了量化评估基准，可同时衡量复制粘贴伪影及身份保真度与生成多样性之间的权衡关系；（3）提出采用对比身份损失的新型训练范式，通过配对数据平衡保真度与多样性。这些工作最终形成了WithAnyone模型，该基于扩散的模型在保持高身份相似度的同时，能有效减少复制粘贴现象。大量定性与定量实验表明，WithAnyone显著降低了复制粘贴伪影，提升了对姿态和表情的控制能力，并保持了优异的感知质量。用户研究进一步证实，本方法在实现高身份保真度的同时，支持具有表现力的可控生成。

## FlashWorld：数秒内生成高质量 3D 场景
[FlashWorld：数秒内生成高质量 3D 场景](https://arxiv.org/abs/2510.13678)

我们提出 FlashWorld，一种生成模型，能够从单张图像或文本提示在数秒内生成 3D 场景，速度比现有方法快 10~100 倍，且渲染质量更优。该方法摒弃了传统的多视图导向 (MV-oriented) 范式——即先生成多视图图像再进行 3D 重建，转而采用 3D 导向方法，在多视图生成过程中直接输出 3D 高斯表示。尽管 3D 导向方法能保证三维一致性，但其视觉质量通常较差。FlashWorld 通过双模式预训练阶段和跨模式后训练阶段，有效融合了两种范式的优势。具体而言，我们基于视频扩散模型的先验知识，首先预训练一个双模式多视图扩散模型，该模型同时支持 MV 导向和 3D 导向生成模式。为缩小 3D 导向生成的质量差距，我们进一步提出跨模式后训练蒸馏技术，通过将 3D 导向模式的分布对齐至高质量的 MV 导向模式，在保持三维一致性的同时显著提升视觉质量，并减少推理时所需的去噪步数。此外，我们在训练过程中引入海量单视图图像和文本提示，以增强模型对分布外 (out-of-distribution) 输入的泛化能力。大量实验验证了本方法在效率与质量上的优越性。

## 服务型 AI：基于 AI 眼镜的主动辅助
[服务型 AI：基于 AI 眼镜的主动辅助](https://arxiv.org/abs/2510.14359)

在人工智能从被动工具发展为主动自适应伙伴的时代，我们提出了服务型 AI (AI4Service) 这一新范式，旨在日常生活中提供主动实时的辅助服务。现有 AI 服务大多仍处于被动响应状态，仅对用户显式指令作出反应。我们认为，真正智能实用的助手应具备预测用户需求的能力，并能适时主动采取行动。为实现这一愿景，我们提出 Alpha-Service 统一框架，解决两个核心挑战：通过第一人称视角视频流检测服务时机以确定干预时机，以及提供通用化与个性化服务的方法。该框架借鉴冯·诺依曼体系结构，基于 AI 眼镜硬件平台，包含五大核心组件：负责环境感知的输入单元、进行任务调度的中央处理单元、实现工具调用的算术逻辑单元、支持长期个性化的存储单元，以及处理自然人机交互的输出单元。作为初步实践，我们通过部署于 AI 眼镜的多智能体系统实现了 Alpha-Service。典型案例（包括实时二十一点游戏顾问、博物馆导览助手和购物搭配推荐系统）表明，该系统能够无缝感知环境、推断用户意图，并在无需显式提示的情况下提供及时有效的辅助服务。

## KORMo：面向所有人的韩语开放推理模型
[KORMo：面向所有人的韩语开放推理模型](https://arxiv.org/abs/2510.09426)

本研究首次针对非英语语言（特别是韩语）开展了大规模探索，构建了一个完全开放的双语大语言模型 (LLM) ，该模型主要基于合成数据训练。我们推出了 KORMo-10B，这是一个 108 亿参数的模型，从零开始在韩英双语语料库上训练，其中韩语部分的 68.74% 为合成数据。通过系统性实验，我们证明：在精心策划以平衡语言覆盖和多样化指令风格的前提下，合成数据不会在大规模预训练中引发不稳定性或性能退化。此外，该模型在广泛的推理、知识和指令遵循基准测试中，性能与当代开放权重多语言基线模型相当。我们的实验揭示了两个关键发现：(1) 合成数据能够可靠支持长期预训练，且不会导致模型崩溃；(2) 双语指令微调可使模型在韩语中实现接近母语水平的推理和话语连贯性。通过完整发布所有组件（包括数据、代码、训练方案和日志），本研究为在低资源环境下开发合成数据驱动的完全开放模型 (FOMs) 建立了透明框架，并为未来多语言 LLM 研究树立了可复现的范例。

## UniMoE-Audio：基于动态容量混合专家的统一语音与音乐生成
[UniMoE-Audio：基于动态容量混合专家的统一语音与音乐生成](https://arxiv.org/abs/2510.13344)

统一多模态模型的最新进展显示出向全面内容生成的明显趋势。然而，音频领域仍面临重大挑战，音乐和语音常被独立开发，阻碍了通用音频合成的进展。这种割裂源于任务间的固有冲突和严重的数据不平衡，制约了真正统一音频生成模型的发展。为解决这一问题，我们提出UniMoE-Audio，这是一种基于新型动态容量混合专家 (MoE) 框架的统一语音与音乐生成模型。在架构上，UniMoE-Audio引入了Top-P路由策略以实现动态专家数分配，并采用混合专家设计，包括：路由专家处理领域特定知识、共享专家提取领域无关特征，以及空专家支持自适应计算跳过。为应对数据不平衡，我们设计了三阶段训练方案：1) 独立专家训练阶段，利用原始数据集无干扰地向各“原型专家”注入领域特定知识；2) MoE集成与预热阶段，将这些专家整合到UniMoE-Audio架构中，使用平衡数据子集预热门控模块和共享专家；3) 协同联合训练阶段，在完全平衡数据集上端到端训练整个模型，以增强跨域协同效应。大量实验表明，UniMoE-Audio不仅在主流语音和音乐生成基准上实现了最先进性能，还展现出卓越的协同学习能力，缓解了简单联合训练中常见的性能下降问题。我们的研究凸显了专用MoE架构与精制训练策略在推动通用音频生成领域的巨大潜力。主页：https://mukioxun.github.io/Uni-MoE-site/home.html

## 从像素到词汇——迈向大规模原生视觉-语言基元
[从像素到词汇——迈向大规模原生视觉-语言基元](https://arxiv.org/abs/2510.14979)

原生视觉-语言模型 (Vision-Language Models, VLMs) 体系正逐渐成为典型模块化 VLMs 的有力竞争者，这一趋势由持续演进的模型架构与训练范式所驱动。然而，两个持续存在的挑战阻碍了其广泛探索与推广：(1) 哪些根本性约束使原生 VLMs 区别于模块化 VLMs？这些障碍的突破极限何在？(2) 如何降低原生 VLMs 的研究门槛并推动其普惠化，从而加速领域发展。本文明晰了这些挑战，并提出构建原生 VLMs 的指导原则。具体而言，原生 VLM 基元应满足：(i) 在共享语义空间中实现像素与词汇表征的有效对齐；(ii) 无缝融合原本独立的视觉与语言模块优势；(iii) 内在具备支持统一视觉-语言编码、对齐与推理的跨模态特性。基于此，我们推出 NEO——一个从第一性原理构建的新型原生 VLMs 系列，其在多样化现实场景中可与顶尖模块化方案抗衡。仅通过 3.9 亿图像-文本样本，NEO 即能从零开始高效构建视觉感知能力，同时在我们精心设计的基元所构建的密集单体模型 (monolithic model) 中有效缓解视觉-语言冲突。我们将 NEO 定位为可扩展强大多原生 VLMs 的基石，并配套提供丰富的可复用组件，以构建高性价比、可扩展的技术生态。代码与模型已开源：https://github.com/EvolvingLMMs-Lab/NEO。

## 注意力揭示大语言模型推理：预规划与锚点节奏实现细粒度策略优化
[注意力揭示大语言模型推理：预规划与锚点节奏实现细粒度策略优化](https://arxiv.org/abs/2510.13554)

大语言模型的推理模式仍不透明，而强化学习通常在整个生成过程中采用统一的信用分配，模糊了关键步骤与常规步骤间的界限。本研究将注意力定位为一种特权基础，使其不仅能揭示大语言模型的内部逻辑——不仅仅是计算的副产品，更是推理本身的机制性蓝图。我们首先区分了注意力头在局部聚焦与全局聚焦信息处理上的差异：局部聚焦头在注意力矩阵对角线附近产生指示短语块的锯齿模式，而全局聚焦头则识别出对未来token具有广泛下游影响的token。我们通过两个指标形式化描述这些现象：1) 窗口平均注意力距离，用于度量滑动窗口内向后注意力的范围；2) 未来注意力影响，通过计算token从后续token接收的平均注意力来量化其全局重要性。这些信号共同揭示了一种循环的预规划与锚点机制：模型首先进行长距离上下文参考以生成引导性token，随后或同步产生语义锚点token来组织后续推理。基于这些发现，我们提出了三种新型强化学习策略，能够动态地对关键节点（预规划token、锚点token及其时间耦合）进行目标信用分配，并在多种推理任务中实现了稳定的性能提升。通过使优化过程与模型内在推理节奏对齐，我们将不透明的优化转化为可操作的结构感知过程，旨在为大语言模型推理的透明化和高效优化提供潜在路径。

## ImagerySearch：突破语义依赖约束的自适应测试时视频生成搜索
[ImagerySearch：突破语义依赖约束的自适应测试时视频生成搜索](https://arxiv.org/abs/2510.14847)

视频生成模型已取得显著进展，尤其在现实场景中表现优异；但在创意场景中其性能明显下降。这类提示通常包含具有长距离语义关系的罕见共现概念，超出了训练数据分布范围。现有方法多采用测试时扩展来提升视频质量，但其固定的搜索空间和静态奖励设计限制了在创意场景中的适应能力。为弥补这一不足，我们提出ImagerySearch——一种基于提示的自适应测试时搜索策略，能够根据提示的语义关系动态调整推理搜索空间和奖励函数。这使得在具有挑战性的创意场景下能够生成更连贯且视觉合理的视频。为评估该方向的进展，我们推出了LDT-Bench——首个专门针对长距离语义提示的基准数据集，包含2,839个多样化概念对和用于评估创意生成能力的自动化评估协议。大量实验表明，ImagerySearch在LDT-Bench上持续优于强大的视频生成基线模型和现有测试时扩展方法，并在VBench上实现了具有竞争力的改进，证明了其在各类提示类型中的有效性。我们将公开LDT-Bench基准数据集和代码，以推动创意视频生成领域的后续研究。

## AutoPR：自动化您的学术推广！
[AutoPR：自动化您的学术推广！](https://arxiv.org/abs/2510.09558)

随着同行评审研究数量的激增，学者们日益依赖社交平台进行学术发现，而作者们则投入大量精力推广其研究成果，以确保可见性和引用量。为简化此流程并减少对人力的依赖，我们提出了自动推广（AutoPR），这是一种新颖的方法，旨在将研究论文转化为准确、引人入胜且及时的公共内容。为支持严格评估，我们发布了PRBench，一个多模态基准数据集，链接了512篇同行评审文章与高质量推广帖子，从三个维度评估系统性能：保真度（准确性与语调）、参与度（目标受众定位与吸引力）以及匹配度（时机与渠道优化）。我们还引入了PRAgent，一个多智能体框架，通过三个阶段实现AutoPR的自动化：基于多模态准备的内容提取、协作合成生成优化输出，以及平台特定适应以优化规则、语调和标签，从而最大化覆盖范围。与PRBench上的直接大语言模型流程相比，PRAgent表现出显著提升，包括总观看时间增长604%、点赞数增加438%，且整体参与度至少提高2.9倍。消融实验显示，平台建模和目标推广对这些提升贡献最大。我们的研究将AutoPR定位为一个可解决、可衡量的研究问题，并为可扩展、高效的自动化学术交流提供了发展路线。

## 潜在精炼解码：通过优化信念状态增强扩散语言模型
[潜在精炼解码：通过优化信念状态增强扩散语言模型](https://arxiv.org/abs/2510.11052)

自回归 (AR) 模型仍是自然语言生成的基准方法，但受限于严格的序列解码机制，始终面临高延迟问题。近期出现的扩散类方法（如 LlaDA 和 Dream）通过并行生成缓解了该问题，但仍存在两个核心缺陷：信息丢失（非最终化 Token 的预测分布每步被丢弃）与过早固化（局部决策缺乏全局协调）。本文提出潜在精炼解码 (LRD)——一个包含潜在精炼模块与预测反馈循环的双阶段框架。第一阶段将掩码位置维持为预测 Token 与掩码嵌入的分布混合状态，使模型能建立全局一致的信念状态；第二阶段逐步固化高置信度 Token，同时保留不确定 Token 用于迭代反馈。KL 散度动力学为收敛判定与早停提供了原则性可靠准则。在代码生成（HumanEval +6.3，MBPP +2.6）与数学推理（GSM8K +2.9，MATH500 +3.8）任务上的实验表明，LRD 在实现最高 10.6 倍加速的同时显著提升准确率，成为并行序列生成领域强有力的通用替代方法。

## 基于大语言模型的氛围编码综述
[基于大语言模型的氛围编码综述](https://arxiv.org/abs/2510.12399)

大语言模型 (LLM) 的发展推动了从代码生成辅助到自主编码智能体的范式转变，催生出一种称为"氛围编码"的新型开发方法论。该方法中，开发者通过观察输出结果而非逐行理解代码来验证 AI 生成的实现。尽管具有变革潜力，这种新兴范式的实际效果仍未得到充分探索，实证研究揭示了意外生产力损失以及人机协作中的基础性挑战。为填补这一研究空白，本文首次对基于大语言模型的氛围编码进行了全面系统的综述，为这一变革性开发方法建立了理论基础与实践框架。通过对 1000 余篇研究论文的系统分析，我们考察了整个氛围编码生态系统，重点分析了关键基础设施组件，包括：用于代码生成的大语言模型、基于大语言模型的编码智能体、编码智能体的开发环境以及反馈机制。我们首先通过将氛围编码形式化为约束马尔可夫决策过程，将其确立为正式学科，该模型刻画了人类开发者、软件项目与编码智能体之间的动态三元关系。基于此理论基础，我们将现有实践归纳为五种典型开发模式：无约束自动化模式、迭代对话协作模式、规划驱动模式、测试驱动模式以及上下文增强模式，从而构建了该领域首个完整的分类体系。重要的是，我们的分析表明，成功的氛围编码不仅依赖于智能体能力，更取决于系统化的语境工程、完善的开发环境以及人机协同的开发模式。

## 多模态提示优化：为何不利用多模态提升 MLLMs 性能
[多模态提示优化：为何不利用多模态提升 MLLMs 性能](https://arxiv.org/abs/2510.09201)

大语言模型 (LLMs) 已取得显著成就，其多模态扩展 (MLLMs) 进一步解锁了图像、视频及其他超越文本的模态能力。然而，尽管有这一转变，旨在减轻手动提示设计负担并最大化性能的提示优化方法仍局限于文本形式，最终限制了 MLLMs 的完整潜力。基于这一差距，我们提出了多模态提示优化这一新问题，将先前提示优化的定义扩展至由文本和非文本提示对所定义的多模态空间。为解决该问题，我们提出多模态提示优化器 (MPO)，这是一个统一框架，不仅通过保持对齐的更新实现多模态提示的联合优化，还利用早期评估作为基于贝叶斯选择策略的先验，指导候选提示的选择过程。通过涵盖图像、视频乃至分子等超越文本的多种模态的广泛实验，我们证明 MPO 优于领先的仅文本优化方法，从而将多模态提示优化确立为实现 MLLMs 潜力的关键步骤。

## TAG：抑制幻觉扩散采样的切向放大引导
[TAG：抑制幻觉扩散采样的切向放大引导](https://arxiv.org/abs/2510.04533)

当前扩散模型在图像生成领域已达到最先进性能，但常出现语义不一致或伪影问题。现有多种推理时引导方法虽能提升生成质量，但通常依赖外部信号或架构修改来实现间接操作，这会引入额外计算开销。本文提出切向放大引导（TAG），这是一种更高效、直接的引导方法，仅通过轨迹信号进行操作，无需修改底层扩散模型。TAG 以中间样本为投影基，放大估计分数相对于该基的切向分量以修正采样轨迹。我们通过一阶泰勒展开对该引导过程进行数学形式化，证明放大切向分量可将状态导向更高概率区域，从而减少不一致性并提升样本质量。TAG 作为即插即用、架构无关的模块，能以最小计算开销提升扩散采样保真度，为扩散引导技术提供了新思路。

