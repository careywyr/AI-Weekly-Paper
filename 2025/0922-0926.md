## Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR
[Baseer: 面向阿拉伯文档转Markdown OCR的视觉语言模型](https://arxiv.org/abs/2509.18174)

阿拉伯文档OCR (Optical Character Recognition) 因该语言的连笔书写、字体多样、变音符号及从右至左的排版方向而始终面临挑战。尽管现代多模态大语言模型 (Multimodal Large Language Models, MLLMs) 已显著提升了对高资源语言的文档理解能力，但其在阿拉伯语上的性能仍受限。本文提出Baseer——一个专为阿拉伯文档OCR微调的视觉语言模型。通过融合合成与真实文档的大规模数据集，Baseer采用纯解码器微调策略，在适应预训练MLLM的同时保留通用视觉特征。我们还推出了Misraj-DocOCR基准数据集，该数据集经专家验证、质量优异，专用于严格评估阿拉伯OCR系统。实验表明，Baseer显著优于现有开源与商业方案，词错误率 (Word Error Rate, WER) 低至0.25，创下阿拉伯文档OCR领域的最新最优成果。本研究结果印证了通用MLLMs进行领域专用适应的优势，并为阿拉伯语等形态复杂语言的高精度OCR奠定了坚实基础。

## Qwen3-Omni Technical Report
[Qwen3-Omni 技术报告](https://arxiv.org/abs/2509.17765)

我们推出 Qwen3-Omni，这是首个在文本、图像、音频和视频模态上均保持顶尖性能的单一多模态模型，且相较于单模态模型未出现性能退化。该模型在 Qwen 系列中与同等规模单模态模型性能相当，尤其在音频任务上表现卓越。在 36 项音频与视听基准测试中，Qwen3-Omni 在 32 项测试中取得开源领域最优性能，在 22 项测试中达成综合最优性能，超越了 Gemini-2.5-Pro、Seed-ASR 和 GPT-4o-Transcribe 等强闭源模型。Qwen3-Omni 采用 Thinker-Talker 专家混合架构，统一了文本、图像、音频与视频的感知与生成能力，可生成流畅文本与自然实时语音。其支持 119 种语言的文本交互、19 种语言的语音理解及 10 种语言的语音生成。为降低流式合成中的首包延迟，Talker 模块采用多码本方案自回归预测离散语音编码，并利用码本表征能力，以轻量级因果卷积网络替代计算密集型块扩散，实现从首帧开始的流式生成。在冷启动场景下，模型理论首包端到端延迟为 234 毫秒。为强化多模态推理，我们引入 Thinking 模型，可显式推理任意模态输入。针对研究社区缺乏通用音频字幕生成模型的现状，我们通过微调 Qwen3-Omni-30B-A3B 得到 Qwen3-Omni-30B-A3B-Captioner，该模型能为任意音频输入生成细节丰富、低幻觉率的描述文本。Qwen3-Omni-30B-A3B、Qwen3-Omni-30B-A3B-Thinking 与 Qwen3-Omni-30B-A3B-Captioner 均已基于 Apache 2.0 许可证开源发布。

## RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation
[RPG：统一可扩展代码库生成的仓库规划图](https://arxiv.org/abs/2509.16198)

大语言模型在函数级和文件级代码生成方面表现优异，但完整代码库的从零生成仍面临根本性挑战。该过程需要在提案级与实现级阶段进行连贯可靠的规划，而自然语言因存在歧义性和冗余性，难以准确表征复杂软件结构。为此，我们提出仓库规划图 (RPG)，这是一种持久化表示方法，通过将功能模块、文件结构、数据流和函数统一编码为图结构，实现提案级与实现级规划的一体化。RPG 用显式蓝图替代模糊的自然语言，支持长期规划和可扩展的代码库生成。基于 RPG，我们开发了 ZeroRepo——一个基于图驱动的从零生成代码库的框架。该框架包含三个阶段：通过提案级规划和实现级细化构建图谱，随后进行图引导的代码生成及测试验证。为评估该框架，我们构建了 RepoCraft 基准数据集，包含六个实际项目共 1,052 项任务。在 RepoCraft 上，ZeroRepo 生成的代码库平均达到 3.6 万行代码，分别是最强基线 Claude Code 的 3.9 倍和其他基线的约 64 倍。其功能覆盖率达 81.5%，测试通过率为 69.7%，较 Claude Code 分别提升 27.3 和 35.8 个百分点。进一步分析表明，RPG 能够对复杂依赖关系进行建模，通过近线性扩展实现渐进式复杂规划，并提升大语言模型对代码库的理解能力，从而加速智能体的代码库导航。

## VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models: 基于方差的课程强化学习用于大语言模型
[VCRL: 基于方差的课程强化学习用于大语言模型](https://arxiv.org/abs/2509.19803)

基于策略的强化学习在提升大语言模型 (LLM) 的数学推理能力方面发挥着重要作用。然而，现有的基于滚动 (rollout) 的强化学习方法 (如 GRPO、DAPO、GSPO 等) 未能显式考虑大语言模型对不同难度样本的学习能力，这与人类从易到难进行数学推理的认知过程相悖。直观上，我们发现 RLVR 中滚动组奖励的方差部分反映了当前样本对大语言模型的难度：过于简单或困难的样本方差较低，而中等难度样本方差较高。基于此，我们提出 VCRL，一种课程强化学习框架，它根据组奖励方差动态控制训练样本的难度。在五个数学基准和两种模型上的实验表明，VCRL 优于当前的大语言模型强化学习基线方法。

## LIMI: Less is More for Agency  
[LIMI：少即是多——智能体自主性新范式](https://arxiv.org/abs/2509.17567)  

我们将智能体自主性定义为AI系统作为自主智能体涌现的能力，能够主动发现问题、构建假设，并通过与环境及工具的自主参与执行解决方案。这一核心能力标志着AI自主性时代的来临，推动力来自行业的关键转变：迫切需要AI系统不仅能思考，更能实际工作。虽然当前AI在推理与响应生成方面表现卓越，但产业界需要的是能执行任务、操作工具并实现实际成果的自主智能体。随着智能体自主性成为区分认知系统与生产型工作者的决定性特征，高效培育机器自主性已至关重要。现有方法遵循语言建模的传统缩放定律，假定更多数据会产生更强自主性。我们从根本上颠覆这一范式。LIMI证明自主性遵循完全不同的发展规律。通过战略性地聚焦于协同软件开发与科学研究工作流程，我们证明复杂的智能体自主性可通过极少但精心设计的自主行为示范产生。仅使用78个精心构建的训练样本，LIMI在综合自主性基准测试中达到73.5%，显著超越当前最优模型：Kimi-K2-Instruct（24.1%）、DeepSeek-V3.1（11.9%）、Qwen3-235B-A22B-Instruct（27.5%）和GLM-4.5（45.1%）。尤为显著的是，LIMI相比使用10,000个样本训练的模型性能提升53.7%——在样本量减少128倍的情况下实现了更卓越的智能体自主性。我们的研究确立了自主性效率原则：机器自主性的培育不依赖数据规模，而取决于对高质量自主行为示范的战略性筛选与设计。

## MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources
[MMR1：通过方差感知采样与开放资源增强多模态推理](https://arxiv.org/abs/2509.21268)

大型多模态推理模型虽进展迅速，但其发展受两大因素制约：缺乏开放、大规模、高质量的长思维链（CoT）数据，以及强化学习（RL）算法在后训练阶段的不稳定性。作为RL微调的标准框架，组相对策略优化（GRPO）在低奖励方差下易出现梯度消失，从而削弱优化信号并影响收敛。本研究提出三项贡献：（1）提出方差感知采样（VAS），这是一种基于方差促进分数（VPS）的数据选择策略，通过结合结果方差与轨迹多样性来提升奖励方差、稳定策略优化；（2）发布大规模精选资源，包含约160万条长思维链冷启动数据与1.5万组RL问答对，确保质量、难度与多样性，同时提供完全可复现的端到端训练代码库；（3）开源多尺度多模态推理模型系列，为社区建立标准化基线。数学推理基准实验验证了所提数据与VAS的有效性，综合消融实验进一步揭示了各组件的作用。此外，我们从理论上证明奖励方差对期望策略梯度模长存在下界约束，而VAS是实现该理论保障的实用机制。代码、数据与模型检查点详见：https://github.com/LengSicong/MMR1。

## SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines
[SciReasoner：构建跨学科科学推理基础](https://arxiv.org/abs/2509.21320)

我们提出了一种科学推理基础模型，能够将自然语言与多样化科学表示形式进行对齐。该模型在包含科学文本、纯序列及序列-文本对的2060亿token语料库上预训练，随后通过4000万条指令的监督微调（SFT）进行对齐，并采用退火冷启动自举方法激发长格式思维链，结合任务特定奖励设计的强化学习，以培养严谨的科学推理能力。模型支持五大能力类别，涵盖工作流中的103项任务：(i) 文本与科学格式间的精确转换，(ii) 文本/知识提取，(iii) 属性预测，(iv) 属性分类，(v) 无条件与条件序列生成及设计。相较于专业系统，本方法扩展了指令覆盖范围，提升了跨领域泛化性能与输出保真度。我们详细阐述了数据构建与训练流程，并证明跨学科学习能强化知识迁移及下游任务可靠性。模型、指令调优数据集及评估代码已开源，地址为 https://huggingface.co/SciReason 与 https://github.com/open-sciencelab/SciReason。

## Video models are zero-shot learners and reasoners
[视频模型是零样本学习器与推理器](https://arxiv.org/abs/2509.20328)

大语言模型 (LLMs) 卓越的零样本能力推动自然语言处理从特定任务模型转向统一、通用的基础模型。这一转变源于简单的基本要素：基于网络规模数据训练的大型生成模型。值得注意的是，相同要素也适用于当前生成式视频模型。视频模型是否会沿袭大语言模型的发展路径，从专用语言理解迈向通用视觉理解？我们证明 Veo 3 能解决大量未经过专门训练的任务：目标分割、边缘检测、图像编辑、物理属性理解、物体可供性 (affordances) 识别、工具使用模拟等。这些对视觉世界的感知、建模与操控能力，实现了迷宫求解、对称性判定等初级视觉推理功能。Veo 涌现的零样本能力表明，视频模型正朝着成为统一通用视觉基础模型的方向演进。

## Tree Search for LLM Agent Reinforcement Learning
[基于树搜索的 LLM 智能体强化学习](https://arxiv.org/abs/2509.21240)

强化学习 (RL) 领域的最新进展显著提升了大语言模型 (LLMs) 的智能体能力。然而，在长期与多轮智能体任务中，现有仅依赖结果奖励的方法普遍面临监督信号稀疏的问题。为解决这一挑战，本文提出树基分组相对策略优化 (Tree-GRPO) —— 一种基于树搜索的分组智能体强化学习方法，其中每个树节点对应完整的智能体交互步骤。通过共享公共前缀，树搜索采样能够在固定 token 或工具调用预算内显著增加可实现的轨迹 (rollout) 数量。此外，研究发现树形结构轨迹天然支持构建逐步过程监督信号，即使仅依赖最终结果奖励。基于此，Tree-GRPO 在树内与树间层面分别估计分组相对优势。理论分析表明，树内分组相对策略优化的目标函数等价于步骤级直接偏好学习的目标。在涵盖 11 个数据集与 3 类问答任务的实验中，该方法相较于链基强化学习方法展现出显著优势。

## OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models
[OmniInsert：基于扩散Transformer模型的任意参考无掩码视频插入](https://arxiv.org/abs/2509.17627)

基于扩散模型的视频插入技术近期取得了显著进展。然而，现有方法虽然依赖复杂的控制信号，却难以保持主体一致性，这限制了其实际应用。本文专注于无掩码视频插入任务，旨在解决三个关键挑战：数据稀缺、主体与场景平衡以及插入和谐性。针对数据稀缺问题，我们提出了新的数据管道InsertPipe，能够自动构建多样化的交叉配对数据集。基于该数据管道，我们开发了OmniInsert——一个支持单主体和多主体参考的无掩码视频插入统一框架。具体而言，为保持主体与场景平衡，我们引入了简洁有效的条件特定特征注入机制，可区分注入多源条件；同时提出渐进式训练策略，使模型能够平衡来自主体和源视频的特征注入。此外，我们设计了主体聚焦损失函数以提升主体的细节表现。为增强插入和谐性，我们提出了插入偏好优化方法，通过模拟人类偏好来优化模型，并在参考阶段引入上下文感知重述模块，实现主体与原始场景的无缝融合。针对该领域缺乏基准测试的问题，我们构建了InsertBench基准，包含多样化场景及精心筛选的主体。在InsertBench上的评估表明，OmniInsert性能优于最先进的闭源商业解决方案。代码将开源发布。

## Seedream 4.0: Toward Next-generation Multimodal Image Generation
[Seedream 4.0: 迈向下一代多模态图像生成](https://arxiv.org/abs/2509.20427)

我们推出 Seedream 4.0，这是一个高效、高性能的多模态图像生成系统，将文本到图像 (T2I) 合成、图像编辑与多图像组合功能统一集成于单一框架。我们开发了基于高效扩散 Transformer 架构与强大 VAE 的模型，能够显著减少图像 Token 的数量，从而实现高效的模型训练，并支持快速生成原生高分辨率图像 (如 1K-4K)。Seedream 4.0 基于涵盖多样化分类体系及知识中心化概念的数十亿文本-图像对进行预训练。通过对数百个垂直场景 (Vertical Scenarios) 的系统化数据采集与优化策略的结合，确保了训练过程的大规模稳定性与强泛化能力。通过引入精调优化的 VLM 模型，我们采用多模态后训练方法联合优化 T2I 与图像编辑任务。在推理加速方面，我们整合了对抗蒸馏、分布匹配、量化技术及推测解码方案，在未使用 LLM/VLM 作为位置编码 (PE) 模型的情况下，生成 2K 图像的推理时间最快可达 1.8 秒。综合评估表明，Seedream 4.0 在 T2I 与多模态图像编辑任务中均达到业界领先水平。特别在复杂任务中展现出卓越的多模态理解能力，包括精准图像编辑、上下文推理、多图像参考支持及多图生成功能。该系统将传统 T2I 系统拓展为更具交互性与多维度的创意工具，持续推动生成式人工智能在创意设计与专业应用领域的边界突破。Seedream 4.0 现已开放访问：https://www.volcengine.com/experience/ark?launch=seedream。

## Reinforcement Learning on Pre-Training Data
[预训练数据上的强化学习](https://arxiv.org/abs/2509.19249)

计算资源呈指数级增长，而高质量文本数据增长有限，二者间日益扩大的差距制约了大语言模型 (LLM) 的传统扩展方法。为应对此挑战，我们提出了预训练数据上的强化学习 (RLPT)，这是一种新的训练时扩展范式，旨在优化大语言模型。与先前主要依赖监督学习进行训练扩展的方法不同，RLPT 使模型策略能够自主探索有意义的轨迹，从预训练数据中学习，并通过强化学习 (RL) 提升其能力。现有强化学习策略 (如基于人类反馈的强化学习 RLHF 和带可验证奖励的强化学习 RLVR) 需依赖人工标注构建奖励，而 RLPT 通过直接从预训练数据中获取奖励信号，消除了这一依赖。具体而言，RLPT 采用下一段推理目标，对模型在前文条件下准确预测后续文本片段的行为给予奖励。该机制使得强化学习能在预训练数据上规模化应用，鼓励模型在更广泛上下文中探索更丰富的轨迹，从而提升其泛化推理能力。我们在多个模型上对通用领域及数学推理基准进行了广泛实验，验证了 RLPT 的有效性。例如，将其应用于 Qwen3-4B-Base 模型时，在 MMLU、MMLU-Pro、GPQA-Diamond、KOR-Bench、AIME24 和 AIME25 基准上分别实现了 $3.0$、$5.1$、$8.1$、$6.0$、$6.6$ 和 $5.3$ 的绝对性能提升。实验结果还显示出良好的扩展特性，表明随着计算资源增加，性能有望持续提升。此外，RLPT 为扩展大语言模型的推理边界及提升 RLVR 性能奠定了坚实基础。

## Do You Need Proprioceptive States in Visuomotor Policies?
[视觉运动策略是否需要本体感受状态？](https://arxiv.org/abs/2509.18644)

基于模仿学习的视觉运动策略已广泛应用于机器人操作，通常同时采用视觉观察和本体感受状态用于精确控制。然而，本研究发现，这种常见做法会导致策略过度依赖本体感受状态输入，从而对训练轨迹过拟合，并造成空间泛化能力差。相反，我们提出无状态策略，移除本体感受状态输入，仅基于视觉观察预测动作。该策略构建于相对末端执行器动作空间，并确保完整的任务相关视觉观察，在本研究中由双广角腕部摄像头提供。实验结果表明，无状态策略比基于状态的策略实现了显著更强的空间泛化能力：在真实世界任务中，如拾取放置、挑战性衬衫折叠和复杂全身操作，覆盖多个机器人 embodiment，高度泛化任务的平均成功率从0%提升至85%，水平泛化任务从6%提升至64%。此外，无状态策略在数据效率和跨 embodiment 适应方面也显示出优势，提升了实际部署的实用性。了解更多请访问：https://statefreepolicy.github.io。

## MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer
[MANZANO：一种简单可扩展的统一多模态模型，采用混合视觉分词器](https://arxiv.org/abs/2509.16197)

兼具视觉内容理解与生成能力的统一多模态大语言模型 (LLMs) 展现出巨大潜力。然而，现有开源模型往往面临这两种能力之间的性能权衡。本文提出 Manzano——一个简单可扩展的统一框架，通过结合混合图像分词器与精心设计的训练方案，显著缓解了这种矛盾。该框架采用共享视觉编码器连接两个轻量级适配器，在共享语义空间中分别生成用于图像理解的连续嵌入和用于文本生成图像的离散标记。统一自回归大语言模型以文本和图像标记形式预测高级语义，辅助扩散解码器随后将图像标记转换为像素。该架构结合理解与生成数据的统一训练方案，支持两种能力的可扩展联合学习。Manzano 在统一模型领域达到最先进水平，其性能可与专用模型相媲美，尤其在文本密集型任务评估中表现突出。实验表明，该方法任务冲突极低，且模型规模扩展带来持续性能提升，验证了混合分词器设计的有效性。

## Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification
[潜在分区网络：生成建模、表示学习与分类的统一原则](https://arxiv.org/abs/2509.15591)

生成建模、表示学习和分类是机器学习 (ML) 中的三个核心问题，但其最先进 (SoTA) 的解决方案在很大程度上仍相互独立。本文探讨：能否用一个统一原则解决所有三个问题？这种统一可以简化 ML 流程，并增强任务间的协同效应。我们提出潜在分区网络 (LZN) 作为实现该目标的一步。LZN 的核心是创建一个共享的高斯潜在空间，用于编码所有任务的信息。每种数据类型 (例如图像、文本、标签) 均配置一个编码器，将样本映射到互不相交的潜在分区，以及一个解码器，将潜在变量映射回数据。ML 任务被定义为这些编码器和解码器的组合：例如，标签条件图像生成使用标签编码器和图像解码器；图像嵌入使用图像编码器；分类使用图像编码器和标签解码器。我们在三个复杂度递增的场景中验证了 LZN 的潜力：(1) LZN 能增强现有模型 (图像生成)：与 SoTA 的 Rectified Flow 模型结合时，LZN 在未修改训练目标的情况下，将 CIFAR10 的 FID 从 2.76 提升至 2.59。(2) LZN 能独立解决任务 (表示学习)：LZN 无需辅助损失函数即可实现无监督表示学习，在 ImageNet 下游线性分类任务中，分别以 9.3% 和 0.2% 的优势超越开创性的 MoCo 和 SimCLR 方法。(3) LZN 能同时解决多任务 (联合生成与分类)：通过图像和标签的编码器/解码器，LZN 凭借其设计可联合执行两类任务，在改善 FID 的同时，于 CIFAR10 上实现了 SoTA 分类精度。代码与训练模型详见 https://github.com/microsoft/latent-zoning-networks。项目网站位于 https://zinanlin.me/blogs/latent_zoning_networks.html。

## MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe
[MiniCPM-V 4.5：通过架构、数据与训练方案构建高效多模态大语言模型](https://arxiv.org/abs/2509.18154)

多模态大语言模型 (MLLMs) 正处于快速发展阶段，代表了人工智能技术的前沿方向。然而，其训练与推理效率已成为制约模型普及与规模化应用的核心瓶颈。为解决这一挑战，我们推出MiniCPM-V 4.5——一款专为高效能与强性能设计的80亿参数模型。我们在模型架构、数据策略和训练方法上实现了三项核心改进：采用统一3D-Resampler架构实现图像与视频的高效紧凑编码；建立无需复杂数据处理即可同时学习文档知识与文本识别的统一学习范式；设计混合强化学习策略使其擅长处理短推理与长推理任务。OpenCompass综合评估结果表明，MiniCPM-V 4.5在性能上超越了广泛使用的GPT-4o-latest等专有模型，以及参数量更大的Qwen2.5-VL 72B等开源模型。值得注意的是，该模型在实现强劲性能的同时保持了极高的效率。例如在广泛采用的VideoMME基准测试中，MiniCPM-V 4.5在300亿参数以下模型中达到最优性能，其GPU内存占用仅为Qwen2.5-VL 7B的46.7%，推理时间仅需后者的8.7%。

