## Reinforcement Pre-Training
[强化预训练](https://arxiv.org/abs/2506.08007)

本研究提出强化预训练 (RPT) 作为大语言模型和强化学习 (RL) 的新型扩展方法。具体而言，我们将下一 token 预测重构为基于 RL 的推理任务，模型通过正确预测给定上下文的下一 token 来获得可验证奖励。RPT 提供了一种可扩展方案，能够利用海量文本数据实现通用强化学习，而无需依赖特定领域的标注数据。通过提升下一 token 推理能力，RPT 显著提高了语言模型在 token 预测任务上的准确性。此外，RPT 为后续强化微调提供了优质的预训练基础。缩放曲线显示，增加训练计算量能持续提升下一 token 预测准确率。实验结果证明，RPT 是推进语言模型预训练的一种高效且具有前景的扩展方案。

## Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA
[明天还会是真的吗？多语言常青问题分类以提升可信问答](https://arxiv.org/abs/2505.21115)

大语言模型 (LLMs) 在问答 (QA) 任务中经常生成错误答案。一个关键但研究不足的影响因素是问题的时间相关特性——即属于常青问题 (答案长期稳定) 还是可变问题 (答案会随时间变化)。本研究提出了 EverGreenQA，首个带有常青标签的多语言问答数据集，同时支持评估和训练任务。基于 EverGreenQA，我们对 12 个现代大语言模型进行了基准测试，评估其通过显式语言判断或隐式不确定性信号来编码问题时间特性的能力。我们还训练了 EG-E5 轻量级多语言分类器，在该任务上取得了当前最优 (State-of-the-Art) 性能。最后，我们展示了常青分类在三个实际应用中的价值：提升自我知识评估准确性、优化问答数据集筛选，以及解释 GPT-4o 的检索机制。

## Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models
[自信即关键：大语言模型的少样本强化学习微调](https://arxiv.org/abs/2506.06395)

大语言模型 (LLMs) 虽具备卓越的推理能力，但训练后微调对其行为与任务目标的对齐仍至关重要。现有强化学习 (RL) 方法往往依赖高成本的人工标注或外部奖励模型。我们提出基于自信的强化学习方法 (Reinforcement Learning via Self-Confidence, RLSC)，该方法利用模型自身的置信度作为奖励信号，无需人工标注、偏好模型或奖励工程。在 Qwen2.5-Math-7B 上的实验表明，仅需每个问题 16 个样本和 10-20 次训练迭代，RLSC 就能实现多项基准的显著提升：AIME2024 (+13.4%)、MATH500 (+21.2%)、Minerva Math (+21.7%)、Olympiadbench (+20.8%) 和 AMC23 (+9.7%)。RLSC 为推理模型提供了一种简单、可扩展的微调方案，仅需少量样本和无监督信号。

## Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning
[Lingshu：面向统一多模态医学理解与推理的通用基础模型](https://arxiv.org/abs/2506.07044)

多模态大语言模型 (Multimodal Large Language Models, MLLMs) 凭借其大规模数据集和先进训练策略，在通用视觉元素理解方面展现出卓越性能。然而，由于医学领域与通用领域在数据特征和任务需求上存在本质差异，现有模型在医疗应用中的表现仍不尽如人意。具体而言，当前医学 MLLMs 存在三大关键局限：(1) 医学知识覆盖主要局限于影像数据，(2) 因数据质量控制不足导致幻觉 (hallucination) 问题突出，(3) 缺乏面向复杂医疗场景的专业推理能力。针对这些挑战，我们首先提出一套完整的数据处理方案：(1) 从医学影像、专业文本和通用数据中高效提取丰富医学知识；(2) 生成精准的医学描述、视觉问答 (VQA) 及推理样本。基于此，我们构建了知识覆盖全面的多模态医学数据集。依托该数据集，我们提出医学专用 MLLM 模型 Lingshu。该模型通过多阶段渐进式训练，逐步掌握医学专业知识并提升任务解决能力。此外，我们创新性地探索了基于可验证奖励的强化学习范式对提升模型医学推理能力的潜力。同时开发了 MedEvalKit 评估框架，整合主流多模态及文本医学基准，实现标准化、公平高效的模型评估。我们在三大基础医学任务（多模态 QA、文本 QA 和医学报告生成）上的实验表明，Lingshu 在多数任务上持续超越现有开源多模态模型...

## ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning
[ReasonMed：推进医学推理的370K多智能体生成数据集](https://arxiv.org/abs/2506.09513)

虽然基于推理的大语言模型(LLMs)在数学和编程领域表现卓越，但其在知识密集型医学问答任务中的潜力尚未得到充分研究。为此，我们提出ReasonMed——目前规模最大的医学推理数据集，该数据集通过筛选多种LLMs生成的170万条初始推理路径，最终提炼出37万条高质量样本。ReasonMed采用\textit{多智能体验证与优化流程}构建，其中特别设计了\textit{错误修正器}组件，用于检测并纠正验证器识别出的推理错误环节。基于ReasonMed数据集，我们系统探索了医学推理模型的最佳训练方案，研究发现：结合详细的思维链(CoT)推理与精炼的答案摘要，能够实现最优的微调效果。基于此方案训练的ReasonMed-7B模型，在10B参数量级以下的模型中创造了新的性能标杆，相较此前最优模型提升4.17%，在PubMedQA基准上甚至以4.60%的优势超越LLaMA3.1-70B。

## MiniCPM4: Ultra-Efficient LLMs on End Devices
[MiniCPM4：终端设备上的超高效大语言模型](https://arxiv.org/abs/2506.07900)

本文介绍了专为终端设备设计的高效大语言模型 MiniCPM4。我们通过在四个关键维度的系统性创新实现高效性：模型架构、训练数据、训练算法和推理系统。具体而言，在模型架构方面，我们提出可训练的稀疏注意力机制 InfLLM v2，显著加速长上下文处理的预填充和解码阶段。在训练数据方面，我们开发了高效精准的预训练数据过滤生成策略 UltraClean (超净数据集) 和全面的监督微调数据集 UltraChat v2 (超聊数据集)，仅用 8 万亿训练 Token 即可实现优异模型性能。在训练算法方面，我们提出用于高效预训练策略搜索的 ModelTunnel v2，并通过引入分块展开 (chunk-wise rollout) 实现负载均衡的强化学习，以及数据高效的三元大语言模型 BitCPM 改进了现有后训练方法。在推理系统方面，我们开发的 CPM.cu 集成稀疏注意力、模型量化和推测采样 (speculative sampling) 技术，实现高效的预填充和解码。为满足多样化设备端需求，MiniCPM4 提供 0.5B 和 8B 两种参数规模版本。充分的评估结果表明，MiniCPM4 在多个基准测试中均优于同规模开源模型，凸显其高效性与有效性。值得注意的是，MiniCPM4-8B 在处理长序列时相较于 Qwen3-8B 展现出显著速度优势。通过进一步适配，MiniCPM4 已成功应用于可信调查生成、基于模型上下文协议的工具使用等多样化场景，充分展现其广泛适用性。

## Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance
[Saffron-1：大语言模型安全保证的推理缩放范式研究](https://arxiv.org/abs/2506.06444)  

现有安全保证研究主要聚焦于训练阶段对齐（alignment）技术，旨在为大语言模型注入安全行为。然而最新研究表明，这些方法容易受到各类越狱攻击（jailbreak attacks）的影响。与此同时，推理缩放（inference scaling）技术虽然显著提升了大语言模型的推理能力，但在安全保证领域的应用尚未得到充分探索。针对这一空白，本研究首次将推理缩放技术应用于构建抵御新兴威胁的鲁棒大语言模型安全机制。我们发现，传统推理缩放技术在安全场景下表现欠佳，其效果甚至不及N选优采样（Best-of-N Sampling）等基础方法。这种低效性源于一个新发现的挑战——探索效率困境（exploration-efficiency dilemma），该问题由频繁的过程奖励模型（PRM）评估所导致的高计算开销引发。为解决这一困境，我们提出SAFFRON——一个专为安全保证设计的创新推理缩放范式。该范式的核心是引入多分支奖励模型（MRM），可显著降低奖励模型评估次数。具体实现方案包括：(1) MRM的部分监督训练目标，(2) 防止超出分布范围探索的保守约束机制，(3) 基于字典树的键值缓存策略，支持树搜索过程中的跨序列缓存共享。大量实验验证了该方法的有效性。我们还开源了训练完成的多分支奖励模型（Saffron-1）和配套的token级安全奖励数据集（Safety4M），以推动大语言模型安全领域的后续研究。相关代码、模型和数据已发布于https://github.com/q-rz/saffron ，项目主页为https://q-rz.github.io/p/saffron 。

## Geopolitical biases in LLMs: what are the "good" and the "bad" countries according to contemporary language models
[大语言模型中的地缘政治偏见：当代语言模型对"友好"与"非友好"国家的认知](https://arxiv.org/abs/2506.06751)

本研究通过分析大语言模型对存在国家立场冲突(美国、英国、苏联和中国)的历史事件解读，评估了其针对不同国家的地缘政治偏见。我们提出了一个包含中立客观事件描述及多国对立观点的新型数据集。研究结果表明模型存在显著的地缘政治倾向性，往往偏向特定国家的叙事立场。此外，简单的去偏见提示方法收效甚微。通过使用经过人工干预的参与者标签进行实验发现，模型对观点归属具有敏感性，在标签置换情况下，有时会放大偏见或识别出逻辑矛盾。这项工作揭示了大语言模型中的国家叙事倾向问题，质疑了简单去偏见方法的有效性，并为未来地缘政治偏见研究提供了方法论框架和基准数据集。

## Seedance 1.0: Exploring the Boundaries of Video Generation Models
[Seedance 1.0：探索视频生成模型的边界](https://arxiv.org/abs/2506.09113)

扩散模型技术的重大突破推动了视频生成领域的快速发展，然而当前的基础模型仍需解决三大关键挑战：提示词遵循、运动自然度与视觉质量的平衡优化。本报告介绍了 Seedance 1.0，这是一个高性能、高效率的视频生成基础模型，其核心技术改进包括：(1) 采用精准且语义丰富的视频标注增强多源数据治理，实现多样化场景的全面学习；(2) 通过创新的训练范式设计高效架构，原生支持多片段视频生成，并联合优化文本到视频和图像到视频任务；(3) 精心设计的后训练优化方案，结合细粒度监督微调和视频专用强化学习人类反馈（RLHF）的多维度奖励机制，实现全方位性能提升；(4) 采用多阶段蒸馏策略和系统级优化，实现约10倍的推理加速。在 NVIDIA-L20 硬件上，Seedance 1.0 仅需 41.4 秒即可生成一段 5 秒的 1080p 分辨率视频。与当前最先进的视频生成模型相比，Seedance 1.0 展现出显著优势：具备卓越的时空连贯性和结构一致性，在复杂多主体场景中精确遵循指令，并能保持多片段叙事的连贯性和主体表征一致性。

## PartCrafter: Structured 3D Mesh Generation via Compositional Latent Diffusion Transformers
[PartCrafter: 通过组合式潜在扩散 Transformer 实现结构化 3D 网格生成](https://arxiv.org/abs/2506.05573)

我们提出 PartCrafter，这是首个能够从单张 RGB 图像同时生成多个语义明确且几何特征各异的 3D 网格的结构化生成模型。不同于现有方法仅能生成整体 3D 形状或需要先分割图像再逐块重建的两阶段流程，PartCrafter 采用统一组合式架构，无需依赖预分割输入。基于单一输入图像，该系统可并行处理多个 3D 部件的去噪过程，支持从独立物体到复杂多物体场景的端到端部件感知生成。PartCrafter 以完整物体预训练的 3D 网格扩散 Transformer (DiT) 为基础，沿用其预训练权重、编码器与解码器，并引入两大创新：(1) 组合式潜在空间，每个 3D 部件由一组解耦的潜在 Token 表征；(2) 分层式注意力机制，实现部件内部与跨部件间的结构化信息流动，在保持生成全局一致性的同时保留部件级细节。为构建部件级监督数据，我们从大规模 3D 数据集中提取部件标注构建新数据集。实验表明，PartCrafter 在生成可分解 3D 网格方面超越现有方法（包括处理输入图像中不可见部件的情况），证明了部件感知生成先验对 3D 理解与合成的有效性。代码与训练数据将开源发布。

## ComfyUI-R1: Exploring Reasoning Models for Workflow Generation
[ComfyUI-R1：探索工作流生成的推理模型](https://arxiv.org/abs/2506.09790)

AI生成内容(AI-generated content)的发展已从单一模型(monolithic models)演进为模块化工作流(modular workflows)，特别是在ComfyUI等平台上，这为创意流程提供了定制化能力。然而，构建高效工作流需要深厚的专业知识来协调众多专用组件，导致用户面临较高的学习门槛。为解决这一挑战，我们提出了ComfyUI-R1——首个面向自动化工作流生成的大规模推理模型。基于我们精心构建的4K工作流数据集，我们开发了长链思维推理(chain-of-thought reasoning, CoT)数据，包含节点选择(node selection)、工作流规划(workflow planning)和代码级工作流表示(code-level workflow representation)。ComfyUI-R1采用两阶段训练框架：(1) 冷启动阶段的CoT微调，使模型适应ComfyUI领域；(2) 基于细粒度规则-指标混合奖励(fine-grained rule-metric hybrid reward)的强化学习，以提升推理能力，同时确保格式有效性、结构完整性和节点级保真度。实验结果显示，这个70亿参数的模型实现了97%的格式有效率，并在通过率(pass rate)、节点级(node-level)和图级(graph-level) F1分数等指标上表现优异，显著超越了采用GPT-4o和Claude系列等领先闭源模型的现有最佳方法。深入分析揭示了推理过程的关键作用，以及将工作流转化为代码的独特优势。定性比较表明，我们的模型在合成包含多样化节点的复杂工作流方面具有明显优势，充分展现了长链CoT推理在AI艺术创作中的巨大潜力。

## Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation
[自回归对抗后训练实现实时交互式视频生成](https://arxiv.org/abs/2506.09350)

现有大规模视频生成模型存在计算密集问题，难以应用于实时交互场景。本研究提出自回归对抗后训练（Autoregressive Adversarial Post-Training，AAPT）方法，可将预训练潜在视频扩散模型转化为实时交互式视频生成器。该模型采用单次神经函数评估（1NFE）的自回归架构逐帧生成潜在帧，支持实时流式输出并接收用户交互指令来控制下一帧生成。与现有方法不同，我们创新性地将对抗训练引入自回归生成范式，不仅设计了高效利用KV缓存的单步生成架构，还通过教师强制（teacher-forcing）训练策略有效降低了长视频生成中的误差累积。实验表明，80亿参数模型在单块H100上可实现736x416分辨率、24fps的实时视频流生成，8块H100集群上则可实现1280x720分辨率、长达1分钟（1440帧）的生成。项目网站：https://seaweed-apt.com/2

