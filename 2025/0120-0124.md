## Evolving Deeper LLM Thinking
[大语言模型思维的深度进化](https://arxiv.org/abs/2501.09891)

我们探索了一种进化搜索策略，用于扩展大语言模型中的推理计算时间。我们提出的方法，Mind Evolution，利用语言模型生成、重组和优化候选响应。该方法在解决方案评估器可用时，避免了形式化底层推理问题的需求。在控制推理成本的前提下，我们发现 Mind Evolution 在自然语言规划任务中显著优于其他推理策略，如 Best-of-N 和 Sequential Revision。在 TravelPlanner 和 Natural Plan 基准测试中，Mind Evolution 使用 Gemini 1.5 Pro 解决了超过 98% 的问题实例，且无需使用形式化求解器。

## PaSa: An LLM Agent for Comprehensive Academic Paper Search
[PaSa: 一个用于全面学术论文搜索的大语言模型智能体](https://arxiv.org/abs/2501.10120)

我们介绍了 PaSa，一个由大语言模型驱动的高级论文搜索智能体。PaSa 可以自主完成一系列决策，包括调用搜索工具、阅读论文和筛选相关参考文献，以最终为复杂的学术查询提供全面而准确的结果。我们使用强化学习对 PaSa 进行了优化，使用了一个合成数据集 AutoScholarQuery，该数据集包含 35k 个细粒度的学术查询和来自顶级 AI 会议出版物的相应论文。此外，我们开发了 RealScholarQuery，一个收集真实场景中学术查询的基准，以评估 PaSa 在更现实场景中的表现。尽管在合成数据上进行了训练，PaSa 在 RealScholarQuery 上显著优于现有的基线，包括 Google、Google Scholar、使用 GPT-4 进行查询改写的 Google、chatGPT（支持搜索的 GPT-4o）、GPT-o1 和 PaSa-GPT-4o（通过提示 GPT-4o 实现的 PaSa）。值得注意的是，PaSa-7B 在召回率@20 和召回率@50 上分别比最好的基于 Google 的基线（使用 GPT-4o 的 Google）高出 37.78% 和 39.90%。它还在召回率上比 PaSa-GPT-4o 高出 30.36%，在精确率上高出 4.25%。模型、数据集和代码可在 https://github.com/bytedance/pasa 获取。

## Multiple Choice Questions: Reasoning Makes Large Language Models (LLMs) More Self-Confident Even When They Are Wrong
[多项选择题：推理使大语言模型更加自信，即使答案是错误的](https://arxiv.org/abs/2501.09775)

评估大语言模型最常用的方法之一是多项选择题 (MCQ) 测试。MCQ 基准测试能够大规模测试大语言模型在几乎所有主题上的知识，因为其结果可以自动处理。为了帮助大语言模型回答问题，提示中可以包含一些少样本示例。此外，可以要求大语言模型直接选择选项来回答问题，或者先提供推理再选择答案，这被称为思维链。除了检查所选答案是否正确外，评估还可以通过大语言模型对其响应的估计概率来评估其信心。本文研究了大语言模型对其答案的信心如何取决于模型是被要求直接回答还是先提供推理再回答。对七个不同模型在广泛主题上的评估结果表明，大语言模型在回答前提供推理时对其答案更有信心。无论答案是否正确，这种情况都会发生。我们假设这种行为是由于推理修改了所选答案的概率，因为大语言模型基于输入问题和支持所选答案的推理来预测答案。因此，大语言模型的估计概率似乎具有内在的局限性，应在评估程序中加以理解。有趣的是，人类中也观察到了相同的行为，解释答案会增加对其正确性的信心。

## GameFactory: Creating New Games with Generative Interactive Videos
[GameFactory: 使用生成式交互视频创建新游戏](https://arxiv.org/abs/2501.08325)

生成式游戏引擎有潜力通过自主创建新内容并减少手动工作量来革新游戏开发。然而，现有的基于视频的游戏生成方法未能有效应对场景泛化的关键挑战，限制了它们对具有固定风格和场景的现有游戏的适用性。在本文中，我们提出了 GameFactory，一个专注于探索游戏视频生成中场景泛化的框架。为了能够创建全新且多样化的游戏，我们利用了在开放域视频数据上预训练的视频扩散模型。为了弥合开放域先验与小规模游戏数据集之间的领域差距，我们提出了一种多阶段训练策略，将游戏风格学习与动作控制解耦，在保持开放域泛化的同时实现动作可控性。我们以 Minecraft 为数据源，发布了 GF-Minecraft，一个高质量且多样化的动作注释视频数据集，用于研究。此外，我们扩展了框架，以实现自回归动作可控的游戏视频生成，从而能够生成无限长度的交互式游戏视频。实验结果表明，GameFactory 有效地生成了开放域、多样化且动作可控的游戏视频，代表了 AI 驱动游戏生成的重要一步。我们的数据集和项目页面公开在 https://vvictoryuki.github.io/gamefactory/。

## MMVU: Measuring Expert-Level Multi-Discipline Video Understanding
[MMVU: 衡量专家级多学科视频理解](https://arxiv.org/abs/2501.12380)

我们介绍了 MMVU，这是一个全面的专家级多学科基准，用于评估视频理解中的基础模型。MMVU 包含 3,000 个由专家标注的问题，涵盖科学、医疗保健、人文与社会科学以及工程四个核心学科的 27 个主题。与之前的基准相比，MMVU 具有三个关键进展。首先，它要求模型应用领域特定知识并执行专家级推理来分析专业领域视频，超越了当前视频基准中通常评估的基本视觉感知能力。其次，每个示例均由人类专家从头标注，并通过严格的数据质量控制确保数据集的高质量。最后，每个示例还附带了专家标注的推理依据和相关领域知识，便于深入分析。我们对 32 个前沿多模态基础模型在 MMVU 上进行了广泛评估。最新的具备 System-2 能力的模型 o1 和 Gemini 2.0 Flash Thinking 在测试模型中表现最佳，但仍无法达到人类专家的水平。通过深入的错误分析和案例研究，我们为未来在专家级、知识密集型的专业领域视频理解方面的进展提供了具有可操作性的见解。

## Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training
[Agent-R: 通过迭代自训练训练语言模型智能体进行反思](https://arxiv.org/abs/2501.11425)

大语言模型 (LLMs) 智能体在交互环境中解决复杂任务时日益重要。现有的工作主要集中在通过从更强的专家那里进行行为克隆来提升性能，然而这种方法在现实应用中常常表现不佳，主要是由于无法从错误中恢复。然而，步骤级别的评估数据难以收集且成本高昂。因此，自动化和动态构建自我评估数据集对于赋予模型智能体能力至关重要。在这项工作中，我们提出了一个迭代自训练框架，Agent-R，使语言智能体能够实时进行反思。与基于正确性进行奖励或惩罚的传统方法不同，Agent-R 利用 MCTS 构建训练数据，从错误轨迹中重建正确轨迹。智能体反思的一个关键挑战在于需要及时修正，而不是等到一次 rollout（即一次完整的任务执行过程）结束。为了解决这个问题，我们引入了一种模型引导的评估构建机制：执行模型在失败轨迹中识别出第一个错误步骤（在其当前能力范围内）。从该步骤开始，我们将其与树中共享相同父节点的相邻正确路径连接。这种策略使模型能够基于其当前策略进行反思学习，从而提高学习效率。为了进一步探索这种自我优化范式的可扩展性，我们研究了错误修正能力和数据集构建的迭代优化。我们的研究结果表明，Agent-R 持续提升了模型从错误中恢复的能力，并实现了及时的错误修正。在三个交互环境中的实验表明，Agent-R 有效地使智能体能够纠正错误行动，同时避免循环，相比基线方法取得了更优的性能（+5.59%）。

## Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models
[细节中的挑战：实现负载均衡损失以训练专业化专家混合模型](https://arxiv.org/abs/2501.11873)

本文重新审视了在训练专家混合模型 (Mixture-of-Experts, MoEs) 时负载均衡损失 (Load-balancing Loss, LBL) 的实现。具体来说，MoEs 的 LBL 定义为 N_E sum_{i=1}^{N_E} f_i p_i，其中 N_E 是专家的总数，f_i 表示专家 i 被选择的频率，p_i 表示专家 i 的平均门控分数。现有的 MoE 训练框架通常采用并行训练策略，因此 f_i 和 LBL 在小批次内计算，然后在并行组之间进行平均。本质上，训练数十亿参数大语言模型 (LLMs) 的小批次通常包含非常少的序列。因此，小批次的 LBL 几乎是在序列级别上，路由机制被推动在每个序列内均匀分配词元。在这种严格的约束下，即使是来自特定领域序列（例如代码）的词元也会被均匀路由到所有专家，从而抑制了专家的专业化。在这项工作中，我们提出使用全局批次来计算 LBL 以放松这一约束。因为全局批次包含比小批次更多样化的序列，这将鼓励在语料库级别上的负载均衡。具体来说，我们引入了一个额外的通信步骤来同步小批次之间的 f_i，然后使用它来计算 LBL。通过在基于 MoEs 的大语言模型（总参数高达 42.8B，词元数量高达 400B）上的实验，我们惊讶地发现，全局批次的 LBL 策略在预训练困惑度和下游任务中都带来了显著的性能提升。我们的分析表明，全局批次的 LBL 还大大提高了 MoE 专家的领域专业化。

## UI-TARS: Pioneering Automated GUI Interaction with Native Agents
[UI-TARS: 开创性的自动化 GUI 交互与原生智能体](https://arxiv.org/abs/2501.12326)

本文介绍了 UI-TARS，一种原生 GUI 智能体模型，它仅依赖屏幕截图作为输入并执行模拟人类的交互（例如，键盘和鼠标操作）。与依赖高度封装的商业模型（例如 GPT-4o）以及专家设计的提示和工作流程的主流智能体框架不同，UI-TARS 是一个端到端模型，在这些复杂框架中表现更优。实验证明了其卓越的性能：UI-TARS 在 10 多个评估感知、基础化和 GUI 任务执行的 GUI 智能体基准测试中达到了最先进的性能。值得注意的是，在 OSWorld 基准测试中，UI-TARS 在 50 步时得分为 24.6，在 15 步时得分为 22.7，优于 Claude（分别为 22.0 和 14.9）。在 AndroidWorld 中，UI-TARS 得分为 46.6，超过了 GPT-4o（34.5）。UI-TARS 包含几项关键创新：（1）增强的感知：利用大规模的 GUI 截图数据集进行上下文感知的 UI 元素理解和精确的标注能力；（2）统一的动作建模，将动作标准化为跨平台的统一空间，并通过大规模动作轨迹实现精确的基础化和交互；（3）系统 2 推理，将深思熟虑的推理纳入多步决策中，涉及任务分解、反思思维、里程碑识别等多种推理模式；（4）基于反思在线轨迹的迭代训练，通过自动收集、过滤和通过反思进行精炼数百台虚拟机上的新交互轨迹，解决了数据瓶颈问题。通过迭代训练和反思调优，UI-TARS 不断从错误中学习，并在最少的人工干预下适应意外情况。我们还分析了 GUI 智能体的发展路径，以指导该领域的进一步发展。

## TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space
[TokenVerse: 基于 Token 调制空间的多概念个性化方法](https://arxiv.org/abs/2501.12224)

我们提出了 TokenVerse —— 一种基于预训练文本到图像扩散模型的多概念个性化方法。该框架能够从单张图像中分离出复杂的视觉元素和属性，并支持从多张图像中提取的概念的灵活组合生成。与现有方法不同，TokenVerse 能够处理每张图像包含多个概念的多个图像，并支持多种概念，包括对象、配件、材质、姿势和光照。我们采用基于 DiT 的文本到图像模型，其中输入文本通过注意力和调制（平移和缩放）影响生成过程。我们发现，调制空间具有语义性，能够对复杂概念进行局部控制。基于这一发现，我们设计了一个优化框架，该框架以图像和文本描述为输入，并为每个词在调制空间中找到一个独特的方向。这些方向可用于生成新图像，按需组合学习到的概念。我们展示了 TokenVerse 在复杂个性化场景中的有效性，并展示了其相对于现有方法的优势。项目网页在 https://token-verse.github.io/

## InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model
[InternLM-XComposer2.5-Reward: 简单而有效的多模态奖励模型](https://arxiv.org/abs/2501.12368)

尽管大视觉语言模型 (Large Vision Language Models, LVLMs) 在视觉理解方面表现优异，但它们偶尔会生成错误的输出。虽然通过强化学习或测试阶段的缩放的奖励模型 (Reward Models, RMs) 提供了提高生成质量的潜力，但仍然存在一个关键的差距：公开可用的多模态奖励模型 (RMs) 用于 LVLMs 的稀缺，且专有模型的实现细节通常不明确。我们通过 InternLM-XComposer2.5-Reward (IXC-2.5-Reward) 弥补了这一不足，这是一个简单而有效的多模态奖励模型，能够将 LVLMs 与人类偏好对齐。为了确保 IXC-2.5-Reward 的鲁棒性和通用性，我们构建了一个高质量的多模态偏好语料库，涵盖文本、图像和视频输入，跨越多个领域，如指令遵循、通用理解、文本丰富的文档、数学推理和视频理解。IXC-2.5-Reward 在最新的多模态奖励模型基准测试中表现出色，并在仅文本的奖励模型基准测试中表现出竞争力。我们进一步展示了其三个关键应用：(1) 为强化学习训练提供监督信号。我们将 IXC-2.5-Reward 与近端策略优化 (Proximal Policy Optimization, PPO) 结合，生成了 IXC-2.5-Chat，该模型在指令遵循和多模态开放式对话方面表现出持续的改进；(2) 在测试时缩放时从候选响应中选择最佳响应；(3) 从现有的图像和视频指令调优训练数据中过滤异常或噪声样本。为了确保可重复性并推动进一步研究，我们已在 https://github.com/InternLM/InternLM-XComposer 开源了所有模型权重和训练配方。

## Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation
[Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation](https://arxiv.org/abs/2501.12202)

我们推出了 Hunyuan3D 2.0，这是一个先进的大规模 3D 合成系统，专门用于生成高分辨率的纹理 3D 资产。该系统包含两个核心组件：一个大规模形状生成模型——Hunyuan3D-DiT，以及一个大规模纹理合成模型——Hunyuan3D-Paint。形状生成模型基于可扩展的基于流的扩散 Transformer，旨在生成与给定条件图像精确对齐的几何形状，为下游应用奠定了坚实的基础。纹理合成模型则利用强大的几何和扩散先验知识，为生成或手工制作的网格生成高分辨率且生动的纹理贴图。此外，我们还开发了 Hunyuan3D-Studio——一个多功能且用户友好的生产平台，简化了 3D 资产的重新创建过程。该平台使专业和业余用户都能高效地操作甚至动画化他们的网格。我们系统地评估了我们的模型，结果表明 Hunyuan3D 2.0 在几何细节、条件对齐、纹理质量等方面均优于之前的最先进模型，包括开源模型和闭源模型。Hunyuan3D 2.0 已公开发布，旨在填补开源 3D 社区在大规模基础生成模型方面的空白。我们的模型的代码和预训练权重可在以下网址获取：
https://github.com/Tencent/Hunyuan3D-2

## Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks
[Mobile-Agent-E: 面向复杂任务的自进化移动助手](https://arxiv.org/abs/2501.11733)

智能手机在现代生活中已成为不可或缺的设备，然而在移动设备上执行复杂任务仍然存在诸多挑战。近年来，基于大模态模型（LMM）的移动智能体在移动环境中的感知与行动能力方面取得了显著进展。然而，现有方法仍存在明显的局限性：它们难以充分满足现实世界中人类的需求，在处理需要复杂推理和长期规划的任务时表现不佳，并且缺乏从历史经验中学习和改进的机制。为解决这些问题，我们提出了 Mobile-Agent-E，这是一个能够通过历史经验实现自我进化的分层多智能体框架。所谓分层，指的是将高层规划与低层动作执行明确分离。该框架包括一个管理者（Manager），负责通过将复杂任务分解为子目标来制定整体计划，以及四个下属智能体——感知器（Perceptor）、操作器（Operator）、动作反射器（Action Reflector）和记录器（Notetaker），分别负责细粒度的视觉感知、即时动作执行、错误验证和信息聚合。Mobile-Agent-E 还引入了一个创新的自进化模块，该模块维护了一个包含提示（Tips）和快捷方式（Shortcuts）的持久长期记忆。提示是从先前任务中总结出的关于如何高效与环境交互的通用指导和经验教训。快捷方式则是为特定子程序定制的可重用、可执行的原子操作序列。通过引入提示和快捷方式，Mobile-Agent-E 能够持续优化其性能和效率。此外，我们还提出了 Mobile-Eval-E，这是一个新的基准测试，包含需要长期规划和多应用交互的复杂移动任务。实验结果表明，Mobile-Agent-E 在三个基础模型骨干上相比之前的最先进方法实现了 22% 的绝对性能提升。项目页面：https://x-plug.github.io/MobileAgent。

## DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
[DeepSeek-R1: 通过强化学习激励大语言模型的推理能力](https://arxiv.org/abs/2501.12948)

我们推出了第一代推理模型 DeepSeek-R1-Zero 和 DeepSeek-R1。DeepSeek-R1-Zero 是一个通过大规模强化学习 (RL) 直接训练的模型，无需监督微调 (SFT) 作为前置步骤，展现了卓越的推理能力。通过强化学习，DeepSeek-R1-Zero 自然形成了许多强大且有趣的推理行为。然而，该模型也存在一些问题，例如可读性较差和语言混合现象。为了解决这些问题并进一步提升推理性能，我们推出了 DeepSeek-R1，它在强化学习之前引入了多阶段训练和冷启动数据。DeepSeek-R1 在推理任务上的表现与 OpenAI-o1-1217 相当。为了支持研究社区，我们开源了 DeepSeek-R1-Zero、DeepSeek-R1 以及基于 Qwen 和 Llama 从 DeepSeek-R1 中提取的六个密集模型 (1.5B, 7B, 8B, 14B, 32B, 70B)。

## VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding
[VideoLLaMA 3: 面向图像和视频理解的前沿多模态基础模型](https://arxiv.org/abs/2501.13106)

在本文中，我们提出了 VideoLLaMA3，这是一个更为先进的多模态基础模型，专门用于图像和视频理解。VideoLLaMA3 的核心设计理念是围绕视觉展开的。所谓“围绕视觉”有两层含义：一是以视觉为中心的训练范式，二是以视觉为中心的框架设计。我们以视觉为中心的训练范式的核心观点是，高质量的图像-文本数据对于图像和视频理解都至关重要。因此，我们没有准备大量的视频-文本数据集，而是专注于构建大规模且高质量的图像-文本数据集。VideoLLaMA3 的训练过程分为四个阶段：1）视觉对齐阶段，该阶段用于预热视觉编码器和投影器；2）视觉-语言预训练阶段，该阶段使用涵盖多种类型（包括场景图像、文档、图表）的大规模图像-文本数据以及纯文本数据，联合调整视觉编码器、投影器和大语言模型；3）多任务微调阶段，该阶段结合图像-文本 SFT 数据进行下游任务训练，并使用视频-文本数据为视频理解奠定基础；4）视频微调阶段，该阶段进一步提升模型在视频理解方面的能力。在框架设计方面，为了更好地捕捉图像中的细粒度细节，预训练的视觉编码器被调整为将不同大小的图像编码为具有相应数量的视觉 Token，而不是固定数量的 Token。对于视频输入，我们根据其相似性减少视觉 Token 的数量，从而使视频的表示更加精确和紧凑。得益于以视觉为中心的设计，VideoLLaMA3 在图像和视频理解基准测试中表现出色。

## Kimi k1.5: Scaling Reinforcement Learning with LLMs
[Kimi k1.5: 使用大语言模型扩展强化学习](https://arxiv.org/abs/2501.12599)

通过下一个 Token 预测进行语言模型预训练已被证明对扩展计算能力有效，但受限于可用训练数据的数量。扩展强化学习 (RL) 为人工智能的持续改进提供了新的可能性，大语言模型 (LLMs) 可以通过奖励机制进行探索来扩展其训练数据。然而，之前发表的工作并未产生具有竞争力的结果。鉴于此，我们报告了 Kimi k1.5 的训练方法和实践，这是我们最新使用 RL 训练的多模态大语言模型，包括其 RL 训练技术、多模态数据配方和基础设施优化。长上下文处理能力的扩展和改进的策略优化方法是我们方法的关键要素，它建立了一个简单有效的 RL 框架，而不依赖于更复杂的技术，如蒙特卡洛树搜索、价值函数和过程奖励模型。值得注意的是，我们的系统在多个基准和任务类型上实现了最先进的推理性能——例如，AIME 上的 77.5，MATH 500 上的 96.2，Codeforces 上的 94 百分位，MathVista 上的 74.9——与 OpenAI 的 o1 相当。此外，我们提出了有效的长到短方法，使用长 CoT 技术改进短 CoT 模型，产生了最先进的短 CoT 推理结果——例如，AIME 上的 60.8，MATH500 上的 94.6，LiveCodeBench 上的 47.3——显著优于现有的短 CoT 模型，如 GPT-4o 和 Claude Sonnet 3.5（高达 +550%）。

## FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces
[FilmAgent (FilmAgent): 一个用于虚拟 3D 空间中端到端 (end-to-end) 电影自动化的多智能体 (multi-agent) 框架](https://arxiv.org/abs/2501.12909)

虚拟电影制作需要复杂的决策过程，包括剧本创作、虚拟摄影以及精确的演员定位和动作。受到基于语言智能体的社会自动化决策最新进展的启发，本文介绍了 FilmAgent，这是一个新颖的基于大语言模型 (LLM-based) 的多智能体协作框架，用于在我们构建的 3D 虚拟空间中实现端到端的电影自动化。FilmAgent 模拟了各种剧组角色，包括导演、编剧、演员和摄影师，并涵盖了电影制作工作流程的关键阶段：(1) 创意开发将头脑风暴的想法转化为结构化的故事大纲；(2) 剧本创作详细描述每个场景的对话和角色动作；(3) 摄影确定每个镜头的摄像机设置。一组智能体通过迭代反馈和修订进行协作，从而验证中间剧本并减少幻觉。我们在 15 个创意和 4 个关键方面对生成的视频进行了评估。人类评估显示，FilmAgent 在所有方面均优于所有基线 (baselines)，平均得分为 3.98 分（满分 5 分），展示了多智能体协作在电影制作中的可行性。进一步的分析表明，尽管 FilmAgent 使用了较不先进的 GPT-4o 模型，但它超越了单智能体 o1，展示了协调良好的多智能体系统的优势。最后，我们讨论了 OpenAI 的文本到视频模型 Sora 与我们的 FilmAgent 在电影制作中的互补优势和劣势。

## Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback
[测试时偏好优化：通过迭代文本反馈进行动态对齐](https://arxiv.org/abs/2501.12895)

大语言模型 (LLMs) 展示了令人印象深刻的性能，但在不重新训练的情况下难以快速适应人类偏好。在这项工作中，我们提出了测试时偏好优化 (Test-time Preference Optimization, TPO)，这是一个在推理过程中将 LLM 输出与人类偏好动态对齐的框架，无需更新模型参数。TPO 不依赖于纯数值奖励，而是将奖励信号转化为文本反馈，并将其作为文本奖励来迭代优化其响应。在涵盖指令遵循、偏好对齐、安全性和数学的基准测试中，TPO 逐步提升了与人类偏好的对齐效果。值得注意的是，仅经过几次 TPO 步骤后，初始未对齐的 Llama-3.1-70B-SFT 模型就能超越对齐的对应模型 Llama-3.1-70B-Instruct。此外，TPO 在推理过程中在搜索宽度和深度方面都能高效扩展。通过案例研究，我们展示了 TPO 如何发挥 LLM 的内在能力来解释奖励信号并采取行动。我们的研究结果表明，TPO 是一种实用、轻量级的测试时偏好优化方案，能够实现动态对齐。我们的代码公开在 https://github.com/yafuly/TPO。

## Autonomy-of-Experts Models
[专家自主模型](https://arxiv.org/abs/2501.13074)

混合专家 (Mixture-of-Experts, MoE) 模型通常使用路由器 (router) 将 token 分配给特定的专家模块，仅激活部分参数，并且通常优于密集模型。我们认为，路由器的决策与专家的执行之间的分离是一个关键但被忽视的问题，导致专家选择次优和学习效果不佳。为了解决这个问题，我们提出了专家自主 (Autonomy-of-Experts, AoE)，这是一种新的 MoE 范式，其中专家自主选择自己来处理输入。AoE 基于这样的洞察：专家知道其自身有效处理 token 的能力，这种意识反映在其内部激活的幅度上。在 AoE 中，路由器被移除；相反，专家预先计算输入的内部激活，并根据其激活范数进行排序。只有排名靠前的专家继续执行前向传播，而其他专家则中止。通过低秩权重分解，减少了预先计算激活的计算成本。这种先自评估再专家间比较的方法确保了改进的专家选择和有效的学习。我们预训练了参数规模从 700M 到 4B 的语言模型，证明了 AoE 在相当效率下优于传统的 MoE 模型。

## SRMT: Shared Memory for Multi-agent Lifelong Pathfinding
[SRMT: Shared Memory for Multi-agent Lifelong Pathfinding](https://arxiv.org/abs/2501.13200)

多智能体强化学习 (MARL) 在解决多种环境下的合作与竞争多智能体问题上取得了显著进展。MARL 中的一个主要挑战是需要明确预测智能体的行为以实现合作。为了解决这一问题，我们提出了共享循环记忆 Transformer (SRMT)，它通过池化和全局广播个体工作记忆，将记忆 Transformer 扩展到多智能体场景中，使智能体能够隐含地交换信息并协调其行动。我们在模拟瓶颈导航任务中的部分可观测多智能体路径规划问题上评估了 SRMT，该任务要求智能体通过狭窄的走廊，并在 POGEMA 基准任务集上进行了评估。在瓶颈任务中，SRMT 始终优于各种强化学习基线，尤其是在奖励稀疏的情况下，并且能够有效地推广到比训练期间更长的走廊。在 POGEMA 地图上，包括迷宫、随机和 MovingAI，SRMT 与最近的 MARL、混合和基于规划的算法表现相当。这些结果表明，将共享循环记忆纳入基于 Transformer 的架构中可以增强去中心化多智能体系统中的协调能力。训练和评估的源代码可在 GitHub 上获取：https://github.com/Aloriosa/srmt。

## Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models
[Sigma: 查询、键和值的差异化调整以实现高效语言模型](https://arxiv.org/abs/2501.13629)

我们介绍了 Sigma，一种专为系统领域设计的高效大语言模型，得益于一种新颖的架构，包括 DiffQKV 注意力机制，并在我们细致收集的系统领域数据上进行了预训练。DiffQKV 注意力机制通过差异化调整注意力机制中的查询 (Query, Q)、键 (Key, K) 和值 (Value, V) 组件，显著提升了 Sigma 的推理效率。这些调整基于它们对模型性能和效率指标的不同影响。具体来说，我们 (1) 进行了大量实验，证明了模型对 K 和 V 组件压缩的不同敏感性，从而开发了差异化压缩的 KV；(2) 提出了增强的 Q，扩展了 Q 头的维度，以最小的推理速度影响增强了模型的表示能力。通过严格的理论和实证分析，我们发现 DiffQKV 注意力机制显著提升了效率，在长上下文场景中，推理速度比传统的分组查询注意力 (Grouped-Query Attention, GQA) 提升了高达 33.36%。我们在来自各种来源的 6T Token 上对 Sigma 进行了预训练，包括我们精心收集的 19.5B 系统领域数据和 1T Token 的合成与重写数据。在通用领域，Sigma 的表现与其他顶尖模型相当。在系统领域，我们引入了首个综合性基准 AIMicius，Sigma 在所有任务中表现卓越，显著优于 GPT-4，绝对提升高达 52.5%。

## Improving Video Generation with Human Feedback
[通过人类反馈改进视频生成](https://arxiv.org/abs/2501.13918)

视频生成技术通过修正流（rectified flow）方法取得了显著进展，但诸如运动不流畅、视频与提示之间不一致等问题仍然存在。在本研究中，我们开发了一个系统化的流程，利用人类反馈来缓解这些问题并优化视频生成模型。具体而言，我们首先构建了一个针对现代视频生成模型的大规模人类偏好数据集，并引入了多维度的成对注释。随后，我们提出了 VideoReward，一个多维视频奖励模型，并研究了注释和各种设计选择对其奖励效果的影响。基于统一强化学习的框架，旨在通过 KL 正则化最大化奖励，我们借鉴扩散模型的方法，为基于流的模型引入了三种对齐算法。这些算法包括两种训练阶段策略：流的直接偏好优化（Flow-DPO）和流的奖励加权回归（Flow-RWR），以及一种推理阶段技术 Flow-NRG，它将奖励指导直接应用于噪声视频。实验结果表明，VideoReward 显著优于现有的奖励模型，而 Flow-DPO 相比 Flow-RWR 和标准监督微调方法表现出更优的性能。此外，Flow-NRG 允许用户在推理阶段为多个目标分配自定义权重，满足个性化的视频质量需求。项目页面：https://gongyeliu.github.io/videoalign。

