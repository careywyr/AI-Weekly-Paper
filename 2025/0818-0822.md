## Intern-S1: A Scientific Multimodal Foundation Model
[Intern-S1：科学多模态基础模型](https://arxiv.org/abs/2508.15763)

近年来，大量开源基础模型不断涌现，在诸多热门领域取得显著突破，其性能已逼近闭源模型。然而，在高价值但挑战性更强的科学专业领域，现状却是或仍依赖于专家模型，或通用基础模型的发展远滞后于热门领域，既不足以推动科研变革，又使开源模型与闭源模型间存在巨大差距。为弥合这一差距并向通用人工智能 (AGI) 迈进，我们提出 Intern-S1——一个兼具通用理解推理能力与多科学模态数据分析专长的专业通才模型。Intern-S1 采用多模态专家混合架构 (MoE)，激活参数280亿，总参数达2410亿，经过5T token的持续预训练（其中科学领域数据超2.5T token）。在训练后阶段，模型于InternBootCamp中先后经历离线和在线强化学习 (RL)，我们提出奖励混合机制 (MoR) 以协同超1000项任务的同步RL训练。通过算法、数据与训练系统的整合创新，Intern-S1在在线强化学习训练中达到顶尖性能。基于综合评估基准测试，该模型在通用推理任务上展现出入围开源模型第一梯队的竞争力，在科学领域显著超越现有开源模型，并在分子合成规划、反应条件预测、晶体热力学稳定性预测等专业任务中胜过多项闭源前沿模型。模型已开源于：https://huggingface.co/internlm/Intern-S1。

## DINOv3  
[DINOv3](https://arxiv.org/abs/2508.10104)  

自监督学习（Self-supervised Learning）有望消除手动数据标注的需求，使模型能够无缝扩展到海量数据集和更大规模的架构。这种训练范式不针对特定任务或领域进行定制，因而具备通过单一算法从多样化数据源（包括自然图像和航拍图像等）学习视觉表示的潜力。本技术报告介绍了 DINOv3——通过采用简单而有效的策略实现该愿景的重要里程碑。首先，我们通过精细的数据准备、方案设计和优化流程，充分发挥数据集与模型规模扩展的优势。其次，我们提出了称为 Gram 锚定（Gram Anchoring）的新方法，有效解决了长训练周期中密集特征图退化这一已知但长期未解决的问题。最后，我们采用后处理策略进一步增强了模型在分辨率适应性、模型规模可调性以及文本对齐能力方面的灵活性。最终，我们提出了一个通用视觉基础模型，无需微调即可在多种场景下超越专业领域的先进模型。DINOv3 生成的高质量密集特征在各种视觉任务中均表现出卓越性能，显著超越了先前基于自监督和弱监督学习（Weakly-supervised Learning）的基础模型。我们还开源了 DINOv3 视觉模型套件，通过为不同资源约束和部署场景提供可扩展解决方案，推动各类任务和数据领域的尖端技术发展。

## Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL  
[智能体链：通过多智能体蒸馏与智能体强化学习实现端到端智能体基础模型](https://arxiv.org/abs/2508.13167)  

大语言模型（LLMs）与多智能体系统的最新进展在深度研究、沉浸式编程（vibe coding）和数学推理等复杂问题解决任务中展现出卓越能力。然而，现有大多数多智能体系统基于手动提示/工作流工程与复杂智能体框架构建，导致计算效率低下、能力受限，且无法受益于以数据为中心的学习。本研究提出智能体链（CoA），一种新颖的LLM推理范式，可在单一模型内实现端到端复杂问题解决，其方式与多智能体系统（即使用多工具与多智能体的多轮问题求解）相同。在智能体链问题求解过程中，模型动态激活不同工具智能体与角色化智能体（role-playing agents），以端到端方式模拟多智能体协作。为激发LLM的端到端智能体链问题解决能力，我们引入多智能体蒸馏框架，将最先进的多智能体系统蒸馏为智能体链轨迹，用于智能体监督微调（agentic supervised fine-tuning）。随后，我们在可验证的智能体任务上使用智能体强化学习（agentic reinforcement learning），进一步提升模型在智能体链问题求解中的能力。我们将最终得到的模型称为智能体基础模型（AFMs）。实证研究表明，AFM在网页智能体与代码智能体设置下的多种基准测试中均实现了新的最先进性能。我们完整公开所有研究内容，包括模型权重、训练与评估代码以及训练数据，为智能体模型与智能体强化学习的未来研究提供高起点（solid starting point）。

## Ovis2.5 Technical Report
[Ovis2.5 技术报告](https://arxiv.org/abs/2508.11737)  

我们推出 Ovis2.5——Ovis2 的继任模型，专为原生分辨率视觉感知与强大多模态推理而设计。Ovis2.5 整合了原生分辨率视觉 Transformer，可直接处理原生可变分辨率的图像，避免固定分辨率切片导致的性能退化，同时保留精细细节与全局布局（这对复杂图表等视觉密集内容至关重要）。为强化推理能力，我们训练模型突破线性思维链限制，执行反思操作（包括自检与修订）。这一高级能力在推理时以可选“思考模式”开放，允许用户通过增加延迟来提升困难输入的准确度。模型通过五阶段课程学习进行训练，逐步构建能力：从基础视觉与多模态预训练开始，经大规模指令微调进阶，最终采用 DPO 和 GRPO 完成对齐与推理增强。为高效扩展训练规模，我们采用多模态数据组合与混合并行策略，显著提升端到端训练速度。我们发布两个开源模型：Ovis2.5-9B 和 Ovis2.5-2B。后者延续 Ovis2“小模型，大性能”理念，非常适合资源受限的端侧场景。在 OpenCompass 多模态排行榜中，Ovis2.5-9B 平均得分 78.3，较前代 Ovis2-8B 显著提升，成为 400 亿参数以下开源 MLLM 的性能最优模型；Ovis2.5-2B 得分 73.9，创下同规模模型最高纪录。除综合得分外，Ovis2.5 在 STEM 基准测试中领先，在定位（grounding）与视频任务上表现强劲，并实现了同规模开源模型中复杂图表分析的最高水平。

## SSRL: Self-Search Reinforcement Learning
[SSRL: 自搜索强化学习](https://arxiv.org/abs/2508.10874)

我们研究了大语言模型 (LLMs) 作为强化学习 (RL) 中智能体搜索任务高效模拟器的潜力，从而降低对昂贵外部搜索引擎交互的依赖。为此，我们首先通过结构化提示和重复采样评估了 LLMs 的内在搜索能力，并将其称为自搜索 (Self-Search)。研究结果表明，LLMs 在推理预算方面表现出优异的可扩展性，在问答基准测试中实现了较高的 pass@k 指标，包括具有挑战性的 BrowseComp 任务。基于这些发现，我们提出了自搜索强化学习 (SSRL)，通过基于格式和基于规则的奖励机制来增强 LLMs 的自搜索能力。SSRL 使模型能够内部迭代优化知识运用效率，无需访问外部工具。实证评估表明，经过 SSRL 训练的策略模型为搜索驱动的 RL 训练提供了高成本效益的稳定环境，降低了对外部搜索引擎的依赖性，并促进了稳健的仿真到真实 (sim-to-real) 迁移。我们得出以下结论：1) LLMs 拥有的世界知识能够被有效激发以实现高性能；2) SSRL 展示了利用内部知识减少幻觉现象的潜力；3) 经过 SSRL 训练的模型可与外部搜索引擎无缝集成，无需额外工作。我们的研究发现突显了 LLMs 在支持更具可扩展性的 RL 智能体训练方面的潜力。

## Thyme: Think Beyond Images  
[Thyme: 突破图像思维的边界](https://arxiv.org/abs/2508.11630)  

自OpenAI提出"基于图像的思考"概念以来，近期研究致力于探索如何通过激发推理过程中视觉信息的运用，以增强模型在感知与推理任务中的性能。然而，据我们所知，目前尚无开源工作能够提供与专有模型（如O3）同等丰富的功能——这些模型既能通过代码执行多样化图像操控，又能同步强化逻辑推理能力。本文通过引入Thyme（突破图像思维的边界）在这一方向作出初步尝试，该创新范式使多模态大语言模型（Multimodal Large Language Models, MLLMs）能够通过生成和执行可执行代码，自主实现多样化的图像处理与计算操作，从而超越现有"基于图像的思考"方法。该方案不仅支持动态的实时图像操控（如裁剪、旋转、对比度增强），还允许进行数学运算，同时保持操作时机与方式的高度自主决策权。我们通过两阶段训练策略激活该能力：首先在50万条精选样本数据集上进行监督微调（SFT）以学习代码生成，随后通过强化学习（Reinforcement Learning, RL）阶段优化决策过程。在RL阶段，我们手动收集并设计高分辨率问答对以提升学习难度，同时提出GRPO-ATS（自适应温度采样的组相对策略优化），该算法对文本与代码生成采用差异化温度参数，以平衡推理探索与代码执行精度。我们开展了全面的实验分析与消融研究。在近20个基准测试上的综合评估表明，Thyme能产生显著且稳定的性能收益，尤其在挑战性高的高分辨率感知与复杂推理任务中表现突出。

## DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization
[DuPO: 通过双重偏好优化实现可靠的大语言模型自验证](https://arxiv.org/abs/2508.14460)

我们提出DuPO——一种基于双重学习的偏好优化框架，通过广义对偶关系生成无需标注的反馈。DuPO解决了两个关键局限性：可验证奖励强化学习 (RLVR) 对昂贵标注数据的依赖及其仅限于可验证任务的应用范围，以及传统双重学习对严格双重任务对（如翻译与回译）的约束。具体而言，DuPO将原始任务输入分解为已知与未知组件，进而构建其对偶任务：利用原始输出和已知信息重构未知部分（例如通过逆向数学解恢复隐藏变量），从而将适用范围扩展到不可逆任务。这种重构质量作为自监督奖励来优化原始任务，并与大语言模型通过单一模型同时实例化双重任务的能力形成协同效应。实证研究表明，DuPO在多项任务中取得显著提升：在756个语言方向上平均翻译质量提升2.13 COMET分值，在三个数学推理挑战基准上平均准确率提高6.4个百分点，作为推理阶段重排序器时性能提升9.3个百分点（通过计算资源换取精度提升）。这些成果使DuPO成为一种可扩展、通用且无需标注的大语言模型优化范式。

## ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning
[ComoRAG: 一种受认知启发的记忆组织化 RAG，用于有状态长叙事推理](https://arxiv.org/abs/2508.10419)

长故事和小说中的叙事理解一直是一个颇具挑战性的领域，这源于其错综复杂的情节以及角色和实体之间相互纠缠且不断演变的关系。鉴于大语言模型 (LLM) 在长上下文上的推理能力减弱且计算成本高昂，基于检索的方法在实践中仍至关重要。然而，传统的 RAG (Retrieval-Augmented Generation) 方法因其无状态、单步的检索过程而存在不足，该过程往往忽略了在长范围上下文中捕捉互连关系的动态本质。在本工作中，我们提出了 ComoRAG，其核心原则是：叙事推理并非一次性过程，而是新证据获取与过去知识巩固之间动态、持续的相互作用，类似于人类大脑利用记忆相关信号进行推理的认知过程。具体而言，当遇到推理瓶颈时，ComoRAG 会与一个动态记忆工作空间进行交互，执行迭代推理循环。在每个循环中，它生成探测性查询以规划新的探索路径，随后将新检索到的多方面证据整合到一个全局记忆池中，从而支持涌现出用于解决查询的连贯上下文。在四个具有挑战性的长上下文叙事基准测试 (200K+ tokens) 上，ComoRAG 的性能优于强大的 RAG 基线模型，与最强基线相比取得了高达 11% 的相对性能提升。进一步分析表明，ComoRAG 对于需要全局理解的复杂查询尤其有效，为基于检索的长上下文理解提供了一种原则性强、受认知启发的范式，以实现有状态推理。我们的代码已公开于 https://github.com/EternityJune25/ComoRAG。

## From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models
[从分数到技能：评估金融大语言模型的认知诊断框架](https://arxiv.org/abs/2508.13491)

大语言模型 (LLMs) 在金融应用中展现出潜力，但由于现有基准测试的不足，其在这一高风险领域的适用性仍未被充分验证。现有基准测试仅依赖分数级评估，通过单一分数概括性能，难以细致理解模型真正掌握的知识及其具体局限性。这些测试还依赖于仅覆盖有限金融概念子集的数据集，忽略了实际应用中的其他关键要素。为解决这些问题，我们提出了 FinCDM——首个专为金融大语言模型设计的认知诊断评估框架，能够在知识-技能层面评估模型，通过分析模型在带有技能标签任务中的响应模式（而非单一聚合分数），识别其具备或缺乏的金融技能与知识。我们构建了 CPA-QKA——首个基于注册会计师 (CPA) 考试开发的认知驱动金融评估数据集，全面覆盖现实世界的会计与金融技能。该数据集由领域专家严格标注，他们撰写、验证并标注问题，实现了高标注者一致性和细粒度知识标签。通过对 30 个专有、开源及领域特定大语言模型的广泛实验，我们发现 FinCDM 能够：揭示隐藏的知识缺口；识别传统基准忽略的测试不足领域（如税务与监管推理）；发现模型间的行为聚类模式。FinCDM 通过实现可解释、技能敏感的诊断，为金融大语言模型评估引入了新范式，支持更具可信度和针对性的模型开发。所有数据集与评估脚本将公开发布以推动进一步研究。

## 4DNeX: Feed-Forward 4D Generative Modeling Made Easy  
[4DNeX：实现简化的前馈式4D生成建模](https://arxiv.org/abs/2508.13154)  

我们提出4DNeX，这是首个基于前馈框架的单图像4D（即动态3D）场景表示生成方法。与现有依赖计算密集型优化或多帧视频输入的方法不同，4DNeX通过微调预训练视频扩散模型，实现了高效的端到端图像到4D生成。具体包括：1）针对4D数据稀缺问题，构建了4DNeX-10M大规模数据集，该数据集采用先进重建方法生成高质量4D标注；2）引入统一6D视频表示，联合建模RGB与XYZ序列，实现外观与几何的结构化学习；3）提出一组简单有效的适配策略，将预训练视频扩散模型改造用于4D建模。4DNeX生成的高质量动态点云支持实现新视角视频合成。广泛实验表明，4DNeX在效率与泛化性方面均优于现有4D生成方法，为图像到4D建模提供了可扩展解决方案，并为模拟动态场景演化的生成式4D世界模型奠定基础。

## FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction
[FutureX：面向未来预测的大语言模型智能体高级实时基准](https://arxiv.org/abs/2508.11987)

未来预测对大语言模型（LLM）智能体而言是项复杂任务，需要高水平的分析思维、信息收集、上下文理解及不确定性决策能力。智能体不仅需收集和解析大规模动态信息，还必须整合多元数据源、评估不确定性并根据新兴趋势调整预测，正如人类专家在政治、经济与金融领域的实践。尽管该任务至关重要，目前仍缺乏评估未来预测智能体的大规模基准，主要源于处理实时更新与获取及时准确答案的技术挑战。为此，我们推出**FutureX**——专为执行未来预测任务的LLM智能体设计的动态实时评估基准。作为当前规模最大且最多样化的未来预测实时基准，FutureX支持每日实时更新，并通过自动化问题采集与答案收集管道消除数据污染。我们评估了25个LLM/智能体模型，包括具备推理与搜索能力的模型，以及集成外部工具的模型（如开源深度研究智能体和闭源深度研究模型）。这项综合评估考察了智能体在动态环境中的自适应推理能力与性能表现。此外，我们深入分析了智能体在面向未来任务中的失败模式与性能缺陷，包括对虚假网页的脆弱性和时序有效性等问题。我们的目标是建立动态无污染的评估标准，推动LLM智能体发展，使其在复杂推理与预测性思维方面达到专业人类分析师的水平。

## BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining  
[BeyondWeb：万亿级预训练合成数据扩展的经验与启示](https://arxiv.org/abs/2508.10975)  

近期大语言模型（LLM）预训练研究表明，单纯扩大数据规模最终会导致收益递减，面临数据瓶颈。为此，采用合成数据进行预训练已成为突破性能边界的重要范式。尽管前景广阔，影响合成数据质量的关键因素仍未被充分认知。本研究提出BeyondWeb框架——专为预训练生成高质量合成数据的系统。该框架显著超越了传统网络规模数据集的能力，在14项基准测试中平均表现分别优于当前最先进的合成预训练数据集Cosmopedia和Nemotron-CC高质量合成子集（Nemotron-Synth）5.1个百分点和2.6个百分点。其训练效率达到开放网络数据的7.7倍，较Nemotron-Synth提升2.7倍。值得注意的是，基于BeyondWeb训练180B token的30亿参数模型，性能优于在Cosmopedia上以相同token量训练的80亿参数模型。我们进一步从BeyondWeb实践中总结出预训练合成数据的关键洞察：效益驱动机制、数据重构策略与粒度、以及模型规模与架构系列对数据质量的影响。总体而言，本研究证明高质量合成预训练数据不存在通用解决方案。要获得最优结果，需要协同优化多重因素，这是需要严谨科学方法和实践经验的挑战性任务。简单方法仅能带来有限改进且成本高昂，而精心设计的方法可实现变革性提升——BeyondWeb正是成功范例。

## LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos
[LongSplat：面向随意拍摄长视频的鲁棒无位姿3D高斯泼溅方法](https://arxiv.org/abs/2508.14041)  

LongSplat 解决了从随意拍摄的长视频中进行新视角合成 (NVS) 的关键挑战，这类视频具有相机运动不规则、相机位姿未知以及场景范围广阔的特点。现有方法通常存在位姿漂移、几何初始化不准确和严重内存限制等问题。为此，我们提出 LongSplat——一个鲁棒的无位姿 3D 高斯泼溅框架，其核心特性包括：(1) 增量联合优化：同步优化相机位姿与 3D 高斯以避免陷入局部最小值并保证全局一致性；(2) 鲁棒位姿估计模块：利用学习获得的 3D 先验知识；(3) 高效八叉树锚点生成机制：根据空间密度将稠密点云转换为锚点。在挑战性基准测试上的大量实验验证表明，LongSplat 实现了最先进的性能，相较于现有方法显著提升了渲染质量、位姿估计精度和计算效率。项目页面：https://linjohnss.github.io/longsplat/

## Speed Always Wins: A Survey on Efficient Architectures for Large Language Models  
[速度制胜：大语言模型高效架构综述](https://arxiv.org/abs/2508.09834)  

大语言模型 (LLMs) 在语言理解、生成与推理任务中展现出卓越性能，并持续拓展多模态模型的能力边界。作为现代大语言模型的基石，Transformer 架构提供了扩展性优异的强基线。然而，传统 Transformer 需消耗大量计算资源，给大规模训练与实际部署带来显著挑战。本文系统综述了针对 Transformer 固有局限性设计的创新架构，以提升模型效率。从语言建模基础出发，涵盖以下技术方向的背景与细节：线性及稀疏序列建模方法、高效全注意力机制变体、稀疏混合专家 (Mixture-of-Experts) 、融合上述技术的混合架构，以及新兴的扩散大语言模型。此外，还探讨了这些技术在多模态任务中的应用，并分析其对开发可扩展、资源高效型基础模型的深远意义。通过将最新研究归类整合，本综述勾勒出现代高效大语言模型的架构蓝图，旨在推动未来研究朝向更高效、多功能的 AI 系统发展。

## Next Visual Granularity Generation
[下一级视觉粒度生成](https://arxiv.org/abs/2508.12811)

我们提出了一种新颖的图像生成方法，其核心是将图像分解为一个结构化序列。该序列中的每个元素均保持相同的空间分辨率，但通过使用不同数量的唯一令牌 (Token) 来捕捉不同级别的视觉粒度。图像生成任务由我们新提出的下一级视觉粒度 (Next Visual Granularity, NVG) 生成框架实现。该框架从一张空图像开始，生成视觉粒度序列，并以结构化的方式逐步对其进行细化，从全局布局演进至精细细节。这种迭代过程编码了一种层次化的、分层的表示，从而实现了跨多个粒度级别对生成过程的细粒度控制。我们在 ImageNet 数据集上训练了一系列用于类条件图像生成的 NVG 模型，并观察到了明显的缩放规律。与 VAR 系列模型相比，NVG 在 FID 分数上持续领先 (3.30 -> 3.03, 2.57 -> 2.44, 2.09 -> 2.06)。我们还开展了全面的分析，以展示 NVG 框架的能力与潜力。我们的代码和模型将开源发布。

