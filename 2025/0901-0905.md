## A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code
[A.S.E: 面向 AI 生成代码安全性评估的仓库级基准](https://arxiv.org/abs/2508.18106)

大语言模型 (LLM) 在软件工程中的日益普及，亟需对其生成代码进行严格的安全性评估。然而现有基准存在明显局限：仅关注孤立代码片段、采用缺乏可复现性的不稳定评估方法，且未能建立输入上下文质量与输出安全性之间的关联。为应对这些挑战，我们提出 A.SE (AI 代码生成安全评估) —— 一个面向仓库级安全代码生成的基准测试框架。A.SE 基于包含已知 CVE 漏洞的真实代码仓库构建测试任务，完整保留构建系统、跨文件依赖等仓库上下文。其容器化的可复现评估框架采用专家定义规则，对代码安全性、构建质量和生成稳定性提供稳定可审计的评估。基于 A.SE 对主流 LLM 的评估显示三个关键发现：(1) Claude-3.7-Sonnet 获得最佳综合性能；(2) 专有模型与开源模型的安全差距微小，Qwen3-235B-A22B-Instruct 取得最高安全分；(3) 在安全补丁生成任务中，简洁的"快速思考"解码策略持续优于复杂的"慢速思考"推理方法。

## Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth
[废话学 (Drivel-ology)：通过深度无意义文本解读挑战大语言模型](https://arxiv.org/abs/2509.03867)  

我们提出废话学 (Drivelology) 这一独特语言现象，其定义为"具有深度的无意义文本"——即句法结构连贯但语用层面矛盾、蕴含情感张力或具有修辞颠覆性的表述。这类表达虽表面看似无意义，实则编码了需要依赖语境推理、道德判断或情感解析才能理解的隐含意义。研究发现，当前大语言模型 (LLMs) 尽管在多数自然语言处理 (NLP) 任务中表现优异，却始终难以把握废话学文本的层次化语义。为此，我们构建了包含1,200余个精选样本的小型多元化基准数据集，涵盖英语、汉语、西班牙语、法语、日语和韩语的典型实例。标注过程面临特殊挑战：每个样本均需经过专家严格评审以验证其符合废话学特征，并通过多轮讨论与争议裁定来实现共识，这凸显了废话学固有的微妙性与主观性。我们在分类、生成与推理任务上评估了多种大语言模型，结果清晰表明其局限性：模型常将废话学与浅层无意义文本混淆，产生逻辑断裂的论证，或完全忽略隐含的修辞功能。这些发现揭示了大语言模型在语用理解层面存在深层表征缺陷，并对"统计流畅性等同于认知理解"的假设形成挑战。我们公开数据集与代码，以推动超越表面连贯性的语言深度建模研究。

## The Landscape of Agentic Reinforcement Learning for LLMs: A Survey  
[面向大语言模型的智能体强化学习综述](https://arxiv.org/abs/2509.02547)  

智能体强化学习 (Agentic RL) 的出现标志着范式转变：从传统应用于大语言模型的强化学习 (LLM RL) 转向将大语言模型从被动序列生成器重构为嵌入复杂动态环境的自主决策智能体。本综述通过对比 LLM-RL 的简化的单步马尔可夫决策过程 (MDPs) 与定义智能体强化学习的时间扩展部分可观测马尔可夫决策过程 (POMDPs)，形式化了这一概念转变。基于此，我们提出了一个全面的双重分类体系：其一围绕核心智能体能力（包括规划、工具使用、记忆、推理、自我改进和感知），其二围绕这些能力在不同任务领域中的应用。本文的核心论点是，强化学习作为关键机制，能够将这些能力从静态的启发式模块转化为自适应、稳健的智能体行为。为支持并加速未来研究，我们将开源环境、基准测试和框架的现状整合为实用指南。通过综合五百余篇近期研究成果，本综述勾勒了这一快速发展领域的整体框架，并强调了影响可扩展通用人工智能 (AGI) 智能体发展的机遇与挑战。

## Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers
[科学大语言模型综述：从数据基础到智能体前沿](https://arxiv.org/abs/2508.21148)  

科学大语言模型 (Sci-LLMs) 正在变革科学研究中知识的表示、整合与应用方式，但其发展受制于科学数据的复杂性。本综述提出了一种以数据为核心的系统性框架，将 Sci-LLMs 的发展重构为模型与底层数据基础的协同进化过程。我们建立了科学数据的统一分类体系与科学知识的分层模型，重点阐释了科学语料库区别于通用自然语言处理数据集的多模态、跨尺度及领域特异性挑战。通过系统梳理近期 Sci-LLMs 的发展（从通用基础模型到跨学科专用模型），并深入分析超过 270 个预训练/微调数据集，本文揭示 Sci-LLMs 对数据生态的特殊要求：需要处理具有异构性、多尺度性和不确定性的语料库，这些语料库既要保持领域不变性，又要支持跨模态推理。在评估方面，我们考察了 190 余个基准数据集，发现评估范式正从静态测试转向采用先进评估协议的流程化与发现导向型评估。这些数据中心化分析揭示了科学数据开发中的持续挑战，并探讨了涉及半自动化标注流程与专家验证的新兴解决方案。最后，我们描绘了向闭环系统的范式转变——基于 Sci-LLMs 的自主智能体通过主动实验与验证，持续赋能动态演进的知识库。整体而言，本研究为构建可信赖、可持续演进的人工智能 (AI) 系统提供了技术路线，使其成为加速科学发现的有效合作伙伴。

## UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning
[UI-TARS-2 技术报告：通过多轮强化学习推进 GUI 智能体发展](https://arxiv.org/abs/2509.02544)

图形用户界面 (GUI) 自主智能体的开发是人工智能领域的重大挑战。尽管原生智能体模型的最新进展通过端到端学习统一感知、推理、行动与记忆模块展现出潜力，但在数据可扩展性、多轮强化学习 (RL) 、纯 GUI 操作的局限性以及环境稳定性方面仍存在待解决的问题。本技术报告提出 UI-TARS-2——一个以原生 GUI 为核心的智能体模型，通过系统化训练方法解决这些挑战：支持可扩展数据生成的数据闭环 (Data Flywheel) 、稳定的多轮 RL 框架、集成文件系统与终端的混合 GUI 环境，以及支持大规模部署的统一沙盒平台。实证评估表明，UI-TARS-2 较前代模型 UI-TARS-1.5 实现显著提升。在 GUI 基准测试中，其成绩分别为 Online-Mind2Web 88.2 分、OSWorld 47.5 分、WindowsAgentArena 50.6 分、AndroidWorld 73.3 分，性能超越 Claude 和 OpenAI 智能体等强基线模型。在游戏环境中，该模型在 15 款游戏测试套件中取得 59.8 的平均归一化分数（约达人类水平性能的 60%），并在 LMGame-Bench 基准上与前沿专有模型（如 OpenAI o3）表现相当。此外，该模型能够泛化至长视野信息检索任务和软件工程基准测试，凸显了其在多样化智能体任务中的鲁棒性。对训练动态的详细分析进一步揭示了大规模智能体强化学习中实现稳定性与效率的关键机制。这些结果证明了 UI-TARS-2 在推动 GUI 智能体技术发展方面的潜力，并展现出对真实世界交互场景的强大泛化能力。

## R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning
[R-4B：通过双模式退火与强化学习激发MLLM的通用自动思考能力](https://arxiv.org/abs/2508.21113)

具备逐步思考能力的多模态大语言模型 (MLLMs) 在复杂推理任务中表现出卓越性能。然而对于无需复杂推理即可解决的简单问题，这种思考过程显得冗余。为解决效率问题，我们提出自动思考模型R-4B，能够根据问题复杂度自适应决策是否启动思考机制。R-4B的核心创新在于通过双模式退火技术同时赋予模型思考与非思考能力，并采用双模式策略优化 (BPO) 提升思考过程激活判断的准确性。具体实现包含两个阶段：首先在精心构建的多领域数据集上进行训练（包含思考模式与非思考模式样本），随后在改进的GRPO框架下进行第二阶段训练，强制策略模型为每个输入查询生成双模式响应。实验表明，R-4B在25个高难度基准测试中达到最先进性能，在大多数任务上超越Qwen2.5-VL-7B，并在推理密集型基准中以更低的计算成本实现与Kimi-VL-A3B-Thinking-2506 (16B) 等更大模型相当的效能。

## SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning
[SimpleTIR: 多轮工具集成推理的端到端强化学习](https://arxiv.org/abs/2509.02479)

大语言模型 (LLMs) 通过与外部工具交互能够显著增强推理能力，这种范式称为工具集成推理 (Tool-Integrated Reasoning, TIR) 。然而，当使用强化学习 (Reinforcement Learning, RL) 将 TIR 扩展到多轮场景时，常常面临训练不稳定和性能崩溃的问题。我们发现这种不稳定性主要源于外部工具反馈引起的分布漂移 (distributional drift) ，导致生成低概率 Token。该问题在连续轮次中不断累积，引发灾难性的梯度范数爆炸，从而破坏训练过程。为解决这一挑战，我们提出 SimpleTIR——一种即插即用 (plug-and-play) 算法，能够稳定多轮 TIR 训练。其核心策略是识别并过滤包含空转 (即既不生成代码块也不产生最终答案的轮次) 的轨迹。通过从策略更新中移除这些有问题的轨迹，SimpleTIR 有效阻断了有害的高幅值梯度 (high-magnitude gradients) ，从而稳定学习过程。大量实验表明，SimpleTIR 在多个具有挑战性的数学推理基准上取得了最先进的性能，特别是在基于 Qwen2.5-7B 基础模型时，将 AIME24 分数从纯文本基线的 22.1 显著提升至 50.5。此外，通过摆脱监督微调的限制，SimpleTIR 鼓励模型发现多样且复杂的推理模式，例如自我修正和交叉验证。

## LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model
[LLaVA-Critic-R1：你的评判模型实际上是强大的策略模型](https://arxiv.org/abs/2509.00676)

在视觉语言建模中，评判模型通常被训练用于评估输出——分配标量评分或进行成对偏好判断——而非生成响应。这种与生成响应的策略模型的分离如此根深蒂固，以至于评判模型很少被考虑直接用于策略执行。本研究挑战了这一传统范式。我们提出将带有偏好标注的评判数据集重组为可验证的训练信号，直接在基础生成模型上实施强化学习，从而得到了LLaVA-Critic-R1——一个经过训练以优化偏好判断，同时保持完整生成能力的多模态评判模型。令人惊讶的是，LLaVA-Critic-R1不仅成为了性能顶尖的评判模型，更是一个具有竞争力的策略模型：在26个视觉推理与理解基准测试中，其表现匹配甚至超越了使用领域内数据训练的专业视觉语言模型（VLM），相比其基础模型（Qwen-2.5-VL-7B）平均提升5.7个百分点。将此方法应用于现有的强大视觉语言模型后，我们得到了LLaVA-Critic-R1+，该模型在保持评判质量的同时进一步提升了策略性能，在70亿参数规模下于MMMU基准上实现了71.9的最先进（SoTA）性能。最后，我们证明了增强的评判能力有利于推理过程：在测试阶段采用自评判机制，无需额外训练即可在五个代表性推理任务上获得平均13.8个百分点的提升。我们的结果表明，基于评判数据的强化学习训练能够产生一个在评估与生成两方面均表现卓越的统一模型，为构建可扩展、自我改进的多模态系统提供了一条简洁路径。

## EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control
[EmbodiedOneVision: 面向通用机器人控制的交错式视觉-文本-动作预训练](https://arxiv.org/abs/2508.21112)  

人类在开放世界中无缝执行多模态推理与物理交互的能力，是通用具身智能系统的核心目标。近期基于大规模机器人与视觉-文本数据联合训练的视觉-语言-动作 (VLA) 模型，在通用机器人控制领域取得了显著进展。然而，这些模型在交错式推理与交互方面仍缺乏人类水平的灵活性。本研究提出 EO-Robotics 框架，包含 EO-1 模型与 EO-Data1.5M 数据集。EO-1 作为统一具身基础模型，通过交错式视觉-文本-动作预训练，在多模态具身推理和机器人控制任务中实现卓越性能。其开发基于两大核心支柱: (i) 采用统一架构处理多模态输入 (图像、文本、视频及动作), (ii) 构建大规模高质量多模态具身推理数据集 EO-Data1.5M，包含超过 150 万个强调视觉-文本-动作交错理解的样本。EO-1 通过在 EO-Data1.5M 上协同训练自回归解码与流匹配去噪 (flow matching denoising) 技术，实现了无缝的机器人动作生成与多模态具身推理。大量实验验证了交错式视觉-文本-动作学习对开放世界理解与泛化的有效性，并在多种具身体系 (embodiments) 的长周期灵巧操作任务中得到实证。本文详细阐述了 EO-1 的架构设计、EO-Data1.5M 的数据构建策略及训练方法，为开发先进具身基础模型提供了重要参考。

## From Editor to Dense Geometry Estimator
[从编辑模型到密集几何估计器](https://arxiv.org/abs/2509.04338)

利用预训练文本到图像 (T2I) 生成模型的视觉先验在密集预测任务中已取得显著成果。然而，密集预测本质上是图像到图像的任务，这表明图像编辑模型相比 T2I 生成模型可能更适合作为微调的基础架构。
基于此动机，我们系统分析了编辑模型与生成模型在密集几何估计任务中的微调行为。研究发现，编辑模型具有内在的结构先验，能够通过「精炼」其固有特征实现更稳定的收敛，最终获得优于生成模型的性能表现。
基于这些发现，我们提出 \textbf{FE2E} 框架，首次将基于 Diffusion Transformer (DiT) 架构的先进编辑模型适配于密集几何预测任务。具体而言，为使编辑模型适应这种确定性任务，我们将其原有的流匹配损失函数重构为「一致速度」训练目标，并采用对数量化方法解决编辑模型原生 BFloat16 格式与任务高精度需求之间的精度冲突。此外，我们利用 DiT 的全局注意力机制，在单次前向传播中实现深度与法向量的零成本联合估计，使得二者的监督信号能够相互增强。在未增加训练数据量的情况下，FE2E 在多个数据集的零样本单目深度估计与法线估计任务中取得了显著性能提升。特别值得注意的是，该方法在 ETH3D 数据集上实现了超过 35\% 的性能增益，其表现优于使用 100$\times$ 训练数据的 DepthAnything 系列模型。项目页面请访问 \href{https://amap-ml.github.io/FE2E/}{这里}。

## Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation
[Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation](https://arxiv.org/abs/2508.20470)  

规模定律已验证基于大规模数据训练的模型在文本、图像及视频领域创造性生成方面的成功与潜力。然而，该范式在 3D 领域面临数据稀缺问题，因为互联网可用的 3D 数据量远少于前述模态。值得庆幸的是，现有大量视频本身包含常识先验，可提供替代监督信号以缓解原生 3D 数据有限导致的泛化瓶颈。一方面，捕获物体或场景多视角的视频为 3D 生成提供空间一致性先验；另一方面，视频内蕴的丰富语义信息使生成内容更贴合文本提示且语义更合理。本文系统探讨如何将视频模态应用于 3D 资产生成，涵盖从数据集构建到模型训练的全流程。我们提出首个具有多视角标注的大规模视频数据集 Droplet3D-4M，并训练支持图像与密集文本输入的生成模型 Droplet3D。大量实验验证了方法的有效性，证明其能生成空间一致且语义合理的内容。与现有主流 3D 解决方案相比，本方法还展现出向场景级应用扩展的潜力，这表明视频常识先验对 3D 创作具有显著促进作用。我们已开源全部资源，包括数据集、代码、技术框架和模型权重：https://dropletx.github.io/。

## VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use
[VerlTool：面向统一化工具使用的智能体强化学习](https://arxiv.org/abs/2509.01055)

具备可验证奖励的强化学习 (Reinforcement Learning with Verifiable Rewards, RLVR) 已证明能有效提升大语言模型 (LLM) 的推理能力，但仍局限于缺乏工具集成的单轮交互。尽管近期出现了支持多轮工具交互的智能体强化学习工具使用方法 (Agentic Reinforcement Learning with Tool use, ARLT)，现有工作开发的任务特定代码库存在碎片化、同步执行瓶颈和跨领域扩展性受限的问题。这些低效性阻碍了更广泛的社区采用与算法创新。我们提出 VerlTool——通过系统化设计原则解决这些局限的统一模块化框架。VerlTool 提供四个核心贡献：(1) 与 VeRL 的上游对齐确保兼容性与简化维护，(2) 通过标准化 API 实现统一工具管理，支持代码执行、搜索、SQL 数据库和视觉处理等多模态功能，(3) 异步环境推进通过消除同步瓶颈实现近 2 倍加速，(4) 在 6 个 ARLT 领域展现竞争力的全面评估。我们的框架将 ARLT 形式化为包含多模态观察令牌（文本/图像/视频）的多轮轨迹，突破了单轮 RLVR 范式的限制。我们在数学推理、知识问答、SQL 生成、视觉推理、网络搜索和软件工程任务上训练并评估模型，在提供统一训练框架的同时获得与专用系统相当的结果。模块化插件架构支持快速工具集成，仅需轻量级 Python 接口定义即可显著降低开发开销，为工具增强的强化学习研究提供可扩展基础。代码已开源：https://github.com/TIGER-AI-Lab/verl-tool。

## Towards a Unified View of Large Language Model Post-Training
[迈向大语言模型后训练的统一视角](https://arxiv.org/abs/2509.04419)

现代语言模型的后训练主要存在两类数据源：在线数据（模型生成的回合数据）和离线数据（人类或其他模型的示范数据）。这两类数据通常分别被强化学习（RL）和监督微调（SFT）等方法使用。本文论证了这些方法并非相互矛盾，而是统一优化过程的不同表现形式。我们推导出统一策略梯度估计器，并通过不同数据分布假设和各种偏差-方差权衡下的共同目标梯度，统一解释了多种后训练方法的计算过程。该梯度估计器由四个可互换组件构成：稳定掩码、参考策略分母、优势估计值和似然梯度。基于理论发现，我们提出混合后训练（HPT）算法，能够动态选择不同的训练信号。HPT旨在实现示范数据的有效利用和稳定探索，同时保持已习得的推理模式。我们通过大量实验和消融研究验证了统一理论框架和HPT的有效性。在六个数学推理基准和两个分布外测试集上，HPT在不同规模和架构的模型上均持续超越强基线。

## ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding
[ELV-Halluc：长视频理解中的语义聚合幻觉基准测试](https://arxiv.org/abs/2508.21496)

视频多模态大语言模型（Video-MLLMs）在视频理解领域取得了显著进展。然而，它们仍容易产生与视频输入不一致或无关的幻觉内容。现有的视频幻觉基准测试主要针对短视频，将幻觉归因于强语言先验、帧缺失或视觉编码器引入的视觉-语言偏差等因素。尽管这些确实是导致短视频中大多数幻觉的原因，但它们过度简化了幻觉的成因。有时，模型会生成错误的输出，但其基于的帧级语义却是正确的。我们将这类幻觉称为语义聚合幻觉（SAH），它产生于将帧级语义聚合为事件级语义组的过程中。由于长视频中多个事件间的语义复杂性更高，SAH 问题显得尤为关键，因此有必要对其成因进行分离和深入研究。针对上述问题，我们提出了 ELV-Halluc——首个专用于长视频幻觉的基准测试，旨在系统研究 SAH。实验结果表明 SAH 确实存在，且其发生率随语义复杂性增加而上升。此外，我们发现模型对于快速变化的语义更容易产生 SAH。我们还探讨了缓解 SAH 的潜在方法，证明位置编码策略有助于减轻 SAH，并进一步采用 DPO 策略来提升模型区分事件内与事件间语义的能力。为此，我们构建了一个包含 8,000 个对抗样本对的数据集，在 ELV-Halluc 和 Video-MME 基准上均取得了性能提升，其中 SAH 比率显著降低了 27.7%。

## Open Data Synthesis For Deep Research
[深度研究的开放数据合成](https://arxiv.org/abs/2509.00375)

大语言模型 (LLMs) 正逐渐被要求超越简单的事实查询，转向需要将问题分解为子问题、协调进行多步推理，并从多样化来源综合证据的深度研究任务。我们将具有可验证答案的深度研究任务形式化为层次约束满足问题 (HCSPs)，这与单约束、多跳或扁平 CSP 公式存在根本性差异。然而，现有基准测试 (例如 Natural Questions, HotpotQA) 未能捕获这种复杂性，而最近的合成数据集往往引入捷径推理、知识泄漏或缺乏足够的结构深度。为填补这一空白，我们引入了 InfoSeek——一个可扩展的复杂深度研究任务合成框架。InfoSeek 使用双智能体系统从大规模网页递归构建研究树，将中间节点转化为有效子问题，并将这些树转换为需要遍历完整层次结构的自然语言问题。该框架支持快速扩展，可生成超过 5 万个训练样本、精选测试集以及通过拒绝采样生成的推理轨迹。实验表明，基于 InfoSeek 训练的模型持续优于强基线。在挑战性基准 BrowseComp-Plus 上，经 InfoSeek 优化的 30 亿参数 LLMs 超越了参数量大得多的 320 亿模型和轻量级商业 API (例如 Gemini2.5-Flash)，同时达到与更强 API (例如 Gemini2.5-Pro) 相当的性能。通过保留中间步骤和检索标签等元信息，InfoSeek 进一步支持包括复合奖励设计和轨迹级探索在内的高级优化策略。我们在[此代码库](https://github.com/VectorSpaceLab/InfoSeek)中提供了代码和数据集。

## Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?
[逆向 IFEval：大语言模型能否摒弃顽固的训练惯例以遵循真实指令？](https://arxiv.org/abs/2509.04292)

大语言模型 (LLMs) 在多样化任务中表现出强大性能，但常存在认知固化现象，难以遵循与监督微调 (SFT) 阶段学习的标准化模式相冲突的指令。为评估这一局限，我们提出逆向 IFEval 基准，用于衡量模型的反直觉能力——即克服训练导致的偏见并遵从对抗性指令的能力。该基准包含八类挑战类型：问题修正、故意文本缺陷、无注释代码和反事实回答等。通过人工循环 (human-in-the-loop) 流程，我们构建了涵盖 23 个领域的 1012 个高质量中英文问题数据集，并在优化的 LLM-as-a-Judge 框架下进行评估。对现有主流大语言模型的实验验证了逆向 IFEval 基准的必要性。研究结果表明，未来的对齐工作不仅需要追求流畅性与事实准确性，还应关注非传统语境下的适应性。我们希望逆向 IFEval 既能作为诊断工具，也为开发新方法奠定基础，这些方法可减轻认知固化、降低对狭隘模式的过拟合，最终提升大语言模型在多样且不可预测的现实场景中的指令遵循可靠性。

## DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks
[DeepResearch Arena：基于学术研讨会任务的大语言模型研究能力首测](https://arxiv.org/abs/2509.01396)  

深度研究AI智能体（AI Agent）因其在协调多阶段研究流程（包括文献综述、方法设计与实证验证）方面的潜力而日益受到关注。尽管取得进展，由于难以收集真正体现研究人员关注点与学术好奇心的前沿研究问题，可靠评估其研究能力仍面临重大挑战。为解决这一问题，我们推出DeepResearch Arena基准——该基准基于学术研讨会构建，捕捉丰富的专家论述与互动，更好反映真实研究环境并降低数据泄露风险。为自动构建该基准，我们提出多智能体分层任务生成（MAHTG）系统，从研讨会转录文本中提取具有研究价值的灵感。MAHTG系统进一步将这些灵感转化为高质量研究任务，在保证任务生成可追溯性的同时有效过滤噪声。通过此系统，我们从200余场学术研讨会中精选出超过10,000个高质量研究任务，涵盖文学、历史与科学等12个学科领域。大量实验表明，DeepResearch Arena对当前最先进AI智能体构成显著挑战，不同模型之间存在明显性能差距。

