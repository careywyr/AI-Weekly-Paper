## Large Language Diffusion Models
[大语言扩散模型](https://arxiv.org/abs/2502.09992)

自回归模型 (ARMs) 被广泛认为是大语言模型 (LLMs) 的基础。我们通过引入 LLaDA 来质疑这一观点，LLaDA 是一种在预训练和监督微调 (SFT) 范式下从头开始训练的扩散模型。LLaDA 通过前向数据掩码过程和反向过程来建模数据分布，参数化由一个普通的 Transformer 来预测被掩码的 Token。通过优化似然下界，它为概率推理提供了一种有原则的生成方法。在广泛的基准测试中，LLaDA 表现出强大的可扩展性，优于我们自建的 ARM 基线。值得注意的是，LLaDA 8B 在上下文学习中与 LLaMA3 8B 等强大的 LLMs 表现相当，并且在 SFT 后，在多轮对话等案例研究中展示了令人印象深刻的指令跟随能力。此外，LLaDA 解决了反转诅咒问题，在反转诗歌补全任务中超越了 GPT-4o。我们的研究结果表明，扩散模型是 ARMs 的一个可行且有前途的替代方案，质疑了上述关键 LLM 能力本质上与 ARMs 相关的假设。

## The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks
[过度思考的隐患：探究代理任务中的推理-行动困境](https://arxiv.org/abs/2502.08235)

大型推理模型 (Large Reasoning Models, LRMs) 代表了 AI 在解决问题能力上的重大突破，然而，它们在交互环境中的表现却可能受到限制。本文介绍并深入分析了 LRMs 中的过度思考现象，即模型倾向于延长内部推理链而忽视与环境互动的现象。通过在软件工程任务中使用 SWE Bench Verified 进行实验，我们观察到三种常见的模式：分析瘫痪 (Analysis Paralysis)、失控行为 (Rogue Actions) 和过早放弃 (Premature Disengagement)。我们提出了一个研究这些行为的框架，该框架与人类专家的评估结果相关，并分析了 4018 条任务轨迹。我们发现，较高的过度思考评分与性能下降密切相关，且推理模型相比非推理模型表现出更强的过度思考倾向。我们的分析表明，在代理环境中通过简单的措施来缓解过度思考，例如选择过度思考评分较低的解决方案，可以将模型性能提升近 30%，同时减少 43% 的计算开销。这些结果表明，缓解过度思考具有重要的实际意义。我们建议通过利用原生函数调用能力和选择性强化学习来缓解过度思考倾向。此外，我们在 https://github.com/AlexCuadron/Overthinking 上开源了评估框架和数据集，以促进这一方向的研究。

## Region-Adaptive Sampling for Diffusion Transformers
[扩散 Transformer 的区域自适应采样](https://arxiv.org/abs/2502.10389)

扩散模型 (Diffusion Models, DMs) 已成为跨领域生成任务的首选。然而，它们对多个顺序前向传递的依赖显著限制了实时性能。现有的加速方法主要集中在减少采样步骤的数量或重用中间结果，未能利用受限于卷积 U-Net 结构的图像内空间区域的变化。通过利用扩散 Transformer (DiTs) 在处理可变数量 Token 方面的灵活性，我们引入了 RAS，一种新颖的、无需训练的采样策略，该策略根据 DiT 模型的关注区域动态分配不同的采样比例给图像内的区域。我们的关键发现是，在每个采样步骤中，模型关注于语义上有意义的区域，并且这些关注区域在连续步骤中表现出强烈的连续性。利用这一发现，RAS 仅更新当前关注区域，而其他区域则使用上一步的缓存噪声进行更新。模型的关注区域基于前一步的输出确定，利用了我们观察到的时间一致性。我们在 Stable Diffusion 3 和 Lumina-Next-T2I 上对 RAS 进行了评估，分别实现了高达 2.36 倍和 2.51 倍的加速，且生成质量几乎没有下降。此外，一项用户研究表明，RAS 在人类评估中提供了可比的生成质量，同时实现了 1.6 倍的加速。我们的方法在实现更高效的扩散 Transformer 方面迈出了重要一步，增强了其在实时应用中的潜力。

## Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model
[Step-Video-T2V 技术报告：视频基础模型的实践、挑战与未来](https://arxiv.org/abs/2502.10248)

我们提出了 Step-Video-T2V，这是一个最先进的文本到视频（Text-to-Video, T2V）预训练模型，拥有 300 亿参数，并能生成最长 204 帧的视频。我们设计了一种深度压缩变分自编码器（Variational Autoencoder, VAE）——Video-VAE，用于视频生成任务。该 VAE 具有 16×16 的空间压缩比和 8×的时间压缩比，同时保持卓越的视频重建质量。为了支持英文和中文输入，我们使用两个双语文本编码器对用户提示（prompt）进行编码。此外，我们训练了一种具有 3D 全注意力（full attention）的扩散 Transformer（DiT），并采用流匹配（Flow Matching）技术，将输入噪声去噪并转换为潜在帧（latent frames）。为了减少伪影并提升生成视频的视觉质量，我们还引入了一种基于视频的直接偏好优化（Direct Preference Optimization, DPO）方法——Video-DPO。

在本报告中，我们详细介绍了训练策略，并分享了关键观察结果和技术见解。Step-Video-T2V 的性能在一个新提出的视频生成基准——Step-Video-T2V-Eval 上进行了评测，结果表明，该模型在文本到视频的生成质量上达到业界最先进水平，并在与开源及商业引擎的对比中表现优越。此外，我们讨论了当前基于扩散模型的范式局限性，并展望了视频基础模型（Video Foundation Model）的未来发展方向。

我们已在 [GitHub](https://github.com/stepfun-ai/Step-Video-T2V) 上开源 Step-Video-T2V 及其评测基准 Step-Video-T2V-Eval。同时，在线版本可通过 [阅文](https://yuewen.cn/videos) 访问。我们的目标是加速视频基础模型的创新，为视频内容创作者提供强大支持。


## ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models
[ZeroBench：当代大型多模态模型的不可解视觉基准](https://arxiv.org/abs/2502.09696)

大型多模态模型（Large Multimodal Models, LMMs）在图像理解方面仍然存在重大缺陷，并且在某些指标上，其空间认知能力甚至不及幼儿或动物。尽管如此，它们仍能在许多流行的视觉基准测试中取得高分，但随着模型技术的快速进步，这些基准测试的难度正不断下降。因此，亟需更具挑战性的基准测试，以确保其长期保持研究价值。  

为此，我们提出了 ZeroBench——一个轻量级的视觉推理基准测试，它对于当前最先进的 LMMs 来说完全无法完成。ZeroBench 由 100 道手工精挑细选的问题和 334 道相对较简单的子问题组成。我们对 20 种 LMMs 进行了评测，结果表明所有模型的得分均为 0.0%。此外，我们还对错误进行了严格分析。为了推动视觉理解的进步，我们已公开发布 ZeroBench。  

## MM-RLHF: The Next Step Forward in Multimodal LLM Alignment
[MM-RLHF：多模态大型语言模型对齐的下一步](https://arxiv.org/abs/2502.10391)

尽管多模态大语言模型（Multimodal Large Language Models, MLLMs）近年来取得了显著进展，但当前最先进的模型仍未经过彻底的人类偏好强化学习（Reinforcement Learning from Human Feedback, RLHF）。这一问题的根源在于，当前的对齐研究主要集中在特定领域（如减少幻觉现象），而关于模型对齐人类偏好是否能系统性提升 MLLM 能力的更广泛问题，仍然缺乏深入研究。  

为此，我们提出 MM-RLHF——一个包含 12 万对精细标注的人类偏好比较数据集。与现有资源相比，该数据集在规模、多样性、标注精细度和质量方面均有显著提升。基于此数据集，我们提出了一系列关键创新，以提升奖励模型的质量并优化对齐算法的效率。特别地，我们引入了一种基于批判的奖励模型（Critique-Based Reward Model），该模型在评分之前先对模型输出进行批判分析，从而相比传统的标量奖励机制，提供了更具可解释性、信息量更丰富的反馈。此外，我们提出了动态奖励缩放（Dynamic Reward Scaling）方法，根据奖励信号调整每个样本的损失权重，从而优化高质量比较数据在训练中的作用。  

我们的方法在 10 个不同维度和 27 个基准测试上进行了严格评测，结果表明模型性能得到了显著且持续的提升。具体而言，使用 MM-RLHF 及我们的对齐算法对 LLaVA-ov-7B 进行微调后，其对话能力提升 19.5%，安全性提升 60%。  

我们已开源偏好数据集、奖励模型、训练与评测代码，以及奖励建模和安全性基准测试。更多详情请访问我们的项目页面：[https://mm-rlhf.github.io](https://mm-rlhf.github.io)。  

## Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention
[Native Sparse Attention: 硬件优化且可原生训练的稀疏注意力](https://arxiv.org/abs/2502.11089)

长上下文建模（long-context modeling）对于下一代语言模型至关重要，但标准注意力机制的高计算成本带来了显著的计算挑战。稀疏注意力（Sparse Attention）提供了一种提高效率的可行方案，同时能够保持模型的强大能力。为此，我们提出 NSA（Natively trainable Sparse Attention），一种可原生训练的稀疏注意力机制，该机制结合算法创新与硬件优化，以高效支持长上下文建模。  

NSA 采用动态分层稀疏策略（Dynamic Hierarchical Sparse Strategy），结合粗粒度的 Token 压缩与细粒度的 Token 选择，从而在保证全局上下文感知能力的同时，提高局部信息的精度。我们的方法在以下两个关键方面取得突破：  

1. 通过算术强度均衡的算法设计（Arithmetic Intensity-Balanced Algorithm Design），并针对现代硬件进行优化，实现了显著的计算加速。  
2. 支持端到端训练，在降低预训练计算成本的同时保持模型性能。  

实验结果（图 1）表明，采用 NSA 预训练的模型在通用基准测试、长上下文任务和基于指令的推理任务中，性能可与全注意力（Full Attention）模型媲美，甚至超越。同时，在 64K 长度的序列上，NSA 在解码（decoding）、前向传播（forward propagation）和反向传播（backward propagation）过程中均显著加速，验证了其在整个模型生命周期中的高效性。  

## SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?
[SWE-Lancer: 先进 LLM 能否从现实世界的自由职业软件工程中赚取 100 万美元？](https://arxiv.org/abs/2502.12115)

我们提出 SWE-Lancer，这是一个包含 1,400 多个自由职业软件工程（freelance software engineering）任务的基准测试数据集，这些任务来源于 Upwork，涉及的真实交易总金额高达 100 万美元。SWE-Lancer 涵盖了**独立开发任务（Independent Engineering Tasks）**（任务范围从 50 美元的 Bug 修复到 32,000 美元的功能实现），以及**管理决策任务**，后者要求模型在多个技术实现方案中进行选择。  

对于**独立开发任务**，我们使用经验丰富的软件工程师进行**三重验证（triple verification）**的端到端测试来评分。而对于**管理决策任务**，则将模型的选择与最初被雇佣的工程经理的决策进行比对，以评估其合理性。  

我们评估了当前最先进的 LLMs，结果表明，它们仍然无法完成大部分任务。为了促进未来研究，我们开源了一个统一的 Docker 镜像，并提供了公开评测子集 SWE-Lancer Diamond（[GitHub 代码库](https://github.com/openai/SWELancer-Benchmark)）。我们希望通过模型性能与实际经济价值的映射，SWE-Lancer 能推动 AI 模型对经济影响的深入研究。  

## Learning Getting-Up Policies for Real-World Humanoid Robots
[学习适用于现实世界人形机器人的起身策略](https://arxiv.org/abs/2502.12152)

在人形机器人能够可靠部署之前，跌倒恢复（fall recovery）是一个关键的前提条件。然而，为机器人手工设计起身控制器十分困难，因为跌倒后可能呈现多种姿态，并且机器人通常需要适应复杂地形。本研究提出了一种学习框架，使人形机器人能够在不同的跌倒姿态和多种地形条件下自主起身。  

与以往成功应用于人形机器人运动学习的任务不同，起身任务涉及复杂的接触模式（contact patterns），这要求精确建模碰撞几何（collision geometry），并解决稀疏奖励（sparser rewards）的问题。为此，我们提出了一个基于课程学习（curriculum learning）的**两阶段方法**：  
- **第一阶段**：探索有效的起身轨迹，仅施加最小的平滑性约束，并无严格的速度或扭矩限制。  
- **第二阶段**：进一步优化轨迹，使其更加平滑、缓慢，并增强其对初始姿态和地形变化的鲁棒性，从而具备可部署性。  

实验表明，我们的方法使 G1 机器人在现实环境中的两种主要跌倒情况下均能成功起身：（a）仰卧（face-up）和（b）俯卧（face-down）。此外，我们在**平坦、可变形、湿滑地形**以及**斜坡**（如草地和雪地）上进行了测试，均取得了成功。据我们所知，这是**首个在现实世界中实现基于学习的人形机器人起身策略的研究**。  

**项目主页**：[https://humanoid-getup.github.io](https://humanoid-getup.github.io)  

## ReLearn: Unlearning via Learning for Large Language Models
[ReLearn: 通过学习实现大语言模型的遗忘](https://arxiv.org/abs/2502.11190)

当前大语言模型（Large Language Models, LLMs）的遗忘（unlearning）方法主要依赖反向优化（reverse optimization）来降低目标 Token 的概率。然而，该方法会破坏模型的 Token 预测能力，进而降低整体性能和文本连贯性（linguistic coherence）。此外，现有评估指标过度关注上下文遗忘，而对文本流畅性和相关性的考量不足。  

为此，我们提出 **ReLearn**，一种结合数据增强（data augmentation）和微调（fine-tuning）的高效遗忘方法，并设计了一套全面的评估框架。该框架引入：  
- **知识遗忘率（Knowledge Forgetting Rate, KFR）** 和 **知识保留率（Knowledge Retention Rate, KRR）**，用于衡量模型对知识的遗忘与保留程度；  
- **语言评分（Linguistic Score, LS）**，用于评估生成文本的质量。  

实验结果表明，ReLearn 能够精准执行目标遗忘，同时保持高质量的文本生成能力。机制分析表明，反向优化会破坏文本生成的连贯性，而 ReLearn 能有效避免这一问题。  

**代码开源地址**：[https://github.com/zjunlp/unlearn](https://github.com/zjunlp/unlearn)  

## I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models  
[我思故我扩散：在扩散模型中实现多模态上下文推理](https://arxiv.org/abs/2502.10458)

本文提出了一种新颖的对齐范式 ThinkDiff，使文本到图像的扩散模型具备多模态的上下文理解和推理能力，其核心方法是结合视觉-语言模型 (VLMs) 的优势。目前，多模态扩散微调方法主要关注像素级重建，而非上下文推理，并且受限于推理相关数据集的复杂性和有限性。ThinkDiff 通过采用视觉-语言训练作为代理任务，解决了这一挑战，使 VLMs 与编码器-解码器大语言模型 (LLM) 的解码器对齐，而非直接与扩散解码器对齐。该方法基于一个关键观察：如果扩散解码器使用相应的 LLM 编码器进行提示嵌入，那么 LLM 解码器与扩散解码器就会共享相同的输入特征空间。因此，通过 LLM 解码器对齐 VLMs，可以简化 VLMs 与扩散解码器的对齐过程。ThinkDiff 无需复杂的训练和数据集，便能有效提升扩散模型的理解、推理和组合能力。实验结果表明，在多模态上下文推理生成的挑战性基准 CoBSAT 上，ThinkDiff 将准确率从 19.2% 提高至 46.3%，训练仅需 4 块 A100 GPU 运行 5 小时。此外，ThinkDiff 在将多张图片和文本组合成逻辑连贯的图像方面表现出色。项目主页：[https://mizhenxing.github.io/ThinkDiff](https://mizhenxing.github.io/ThinkDiff)。

## Soundwave: Less is More for Speech-Text Alignment in LLMs
[Soundwave：在大语言模型中的语音-文本对齐中，少即是多](https://arxiv.org/abs/2502.12900)

目前，端到端语音大语言模型 (LLM) 主要依赖大规模标注数据进行训练，而数据高效训练这一问题尚未被深入研究。我们关注语音与文本之间的两个核心问题：表示空间的差距以及序列长度的不一致性。为了解决这些问题，我们提出了 Soundwave，它采用高效的训练策略与新颖的架构。实验结果表明，Soundwave 在语音翻译和 AIR-Bench 语音任务上超越了当前先进的 Qwen2-Audio，并且仅使用了其 1/50 的训练数据。此外，进一步分析表明，Soundwave 在对话过程中仍然能够保持其智能推理能力。项目地址：[https://github.com/FreedomIntelligence/Soundwave](https://github.com/FreedomIntelligence/Soundwave)。

## Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity  
[在单个向量中压缩 1568 个 Token 并恢复：探索嵌入空间容量的极限](https://arxiv.org/abs/2502.13063)

近期的研究致力于将 Token 序列压缩为更短的实值向量序列，以替代 Token 嵌入或键-值缓存 (key-value cache) 作为输入，从而减少现有语言模型的计算量。尽管这些方法依赖强大的编码器模型，但无损压缩率通常不超过 10 倍。这一现象引人深思，因为理论上，即使采用 16 位精度和适度的向量大小，大型实值向量的最大信息容量远超当前已报告的压缩率。在本研究中，我们通过使用逐样本优化 (per-sample optimization) 过程替代编码器，以探索压缩的极限。研究结果表明，存在可实现高达 1500 倍压缩比的向量，这表明当前方法与实际可达解法之间存在两个数量级的差距。此外，我们的实验进一步表明，压缩的极限并非由输入序列的长度决定，而是取决于需要减少的不确定性，即在无条件情况下该序列的交叉熵损失。这些发现揭示了输入嵌入的理论容量与其实际利用率之间存在显著差距，表明模型设计仍有很大的优化空间。


## Phantom: Subject-consistent video generation via cross-modal alignment
[Phantom: 通过跨模态对齐实现主体一致性视频生成](https://arxiv.org/abs/2502.11079)

视频生成的基础模型正在不断发展，并逐步应用于多个领域，其中主体一致性视频生成仍处于探索阶段。我们将这一任务定义为“主体到视频 (Subject-to-Video)”，即从参考图像中提取主体元素，并通过文本指令生成主体一致的视频。我们认为，主体到视频的核心在于平衡文本与图像这两种模态的提示 (prompt)，从而在深度上同时对齐文本与视觉内容。为此，我们提出 Phantom，这是一种统一的视频生成框架，适用于单主体和多主体参考。该方法基于现有的文本到视频 (text-to-video) 和图像到视频 (image-to-video) 架构，重新设计了联合文本-图像注入模型，并通过文本-图像-视频三元数据 (triplet data) 进行跨模态对齐学习。特别是在人物生成方面，我们强调主体一致性，不仅支持现有的身份保持 (ID-preserving) 视频生成，还具备更优的表现。项目主页：[https://phantom-video.github.io/Phantom/](https://phantom-video.github.io/Phantom/)。

## Continuous Diffusion Model for Language Modeling
[连续扩散模型在语言建模中的应用](https://arxiv.org/abs/2502.11564)

扩散模型已成为建模离散分类数据的一种有前景的替代方案。然而，直接在离散数据空间上运行的扩散模型无法充分利用迭代精炼 (iterative refinement) 的能力，因为在离散状态之间转换时会丢失信号。现有的用于离散数据的连续扩散模型相比于离散方法表现有限，并且两者之间的不明确联系限制了扩散模型在离散数据建模中的发展。在本研究中，我们提出了一种用于语言建模的连续扩散模型，该模型结合了底层分类分布的几何特性。我们建立了离散扩散与统计流形 (statistical manifold) 上的连续流之间的联系，并基于这一类比，提出了一种可以泛化先前离散扩散模型的简单扩散过程设计。此外，我们基于径向对称性 (radial symmetry) 提出了一个无需仿真 (simulation-free) 的训练框架，并采用了一种简单的技术来解决流形的高维性问题。对语言建模基准和其他模态的综合实验表明，我们的方法优于现有的离散扩散模型，并接近自回归 (autoregressive) 模型的性能。代码地址：https://github.com/harryjo97/RDLM。

## Magma: A Foundation Model for Multimodal AI Agents
[Magma: 多模态 AI 智能体的基础模型](https://arxiv.org/abs/2502.13130)

本文提出了 Magma，这是一种用于数字世界和物理世界中多模态 AI 智能体任务的基础模型。Magma 是对视觉-语言 (VL) 模型的重要扩展，它不仅保留了 VL 模型的理解能力 (语言智能)，还具备在视觉-空间世界 (时空智能) 进行规划和执行的能力，从 UI 导航到机器人操作等智能体任务皆可完成。为了赋予这些智能体能力，Magma 预训练于包含图像、视频及机器人数据的大规模异构数据集上，其中，图像中的可交互视觉对象 (如 GUI 中的可点击按钮) 通过“标记集 (Set-of-Mark, SoM)”进行标注以进行动作绑定，而视频中的物体运动轨迹 (如人手或机器人手臂的移动轨迹) 通过“标记轨迹 (Trace-of-Mark, ToM)”进行标注以用于动作规划。大量实验表明，SoM 和 ToM 之间的高度协同作用有效提升了 Magma 的时空智能能力，这对于广泛的任务至关重要 (见图 1)。特别是在 UI 导航和机器人操作任务上，Magma 取得了新的 SOTA 结果，超越了专门为这些任务设计的模型。此外，在图像和视频相关的多模态任务上，Magma 的表现优于许多在更大规模数据集上训练的多模态大模型。我们公开了模型和代码以便复现：https://microsoft.github.io/Magma。

## Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation
[多模态 Mamba: 通过二次到线性蒸馏实现解码器式多模态状态空间模型](https://arxiv.org/abs/2502.13145)

近年来，多模态大语言模型 (MLLMs) 在多个任务上取得了显著进展，但由于其二次计算复杂度、不断增长的键值缓存需求以及对独立视觉编码器的依赖，使得部署面临挑战。为了解决这些问题，我们提出 mmMamba，一种通过渐进式蒸馏 (progressive distillation) 从现有 MLLMs 训练出的线性复杂度原生多模态状态空间模型。我们的方案允许直接将训练好的解码器式 MLLM 转换为线性复杂度架构，而无需预训练基于 RNN 的 LLM 或视觉编码器。我们提出了一种“种子化策略 (seeding strategy)”来从训练好的 Transformer 结构中提取 Mamba，并设计了三阶段蒸馏方案，有效地从 Transformer 迁移知识至 Mamba，同时保留其多模态能力。此外，我们的方法支持灵活的混合架构，可将 Transformer 和 Mamba 层结合，实现可定制的效率-性能权衡。在 103K Tokens 规模的实验中，mmMamba-linear 相比 HoVLE 实现了 20.6 倍的加速，同时减少 75.8% 的 GPU 内存占用，而 mmMamba-hybrid 进一步提高性能，相比 HoVLE 提供 13.5 倍加速并减少 60.2% 内存占用。代码和模型可在 https://github.com/hustvl/mmMamba 获取。

## Rethinking Diverse Human Preference Learning through Principal Component Analysis
[通过主成分分析重新思考多样化的人类偏好学习](https://arxiv.org/abs/2502.13131)

理解人类偏好对于改进基础模型并构建个性化 AI 系统至关重要。然而，人类偏好本质上是多样且复杂的，传统的奖励模型难以完整捕捉其全貌。尽管精细化的偏好数据可以有所帮助，但数据收集成本高，难以扩展。为此，我们提出了一种新的方法——分解奖励模型 (Decomposed Reward Models, DRMs)，该方法无需精细标注即可从二元比较中提取多样化的人类偏好。我们的核心思想是将人类偏好表示为向量，并使用主成分分析 (PCA) 进行分析。通过构建一个基于偏好与拒绝响应之间嵌入差异的数据集，DRMs 识别出捕捉不同偏好维度的正交基向量。这些分解后的奖励可以灵活组合，以满足不同用户的需求，从而提供一种可解释且可扩展的奖励建模方案。实验表明，DRMs 能够有效提取具有意义的偏好维度 (如帮助性、安全性、幽默感等)，并且无需额外训练即可适应新用户。研究结果表明，DRMs 提供了一种个性化且可解释的大语言模型 (LLM) 对齐框架。

## FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading
[FLAG-Trader：融合 LLM 智能体与基于梯度的强化学习的金融交易](https://arxiv.org/abs/2502.11433)

近年来，在多模态金融数据上微调的大语言模型（LLM）在多种金融任务中展现出了卓越的推理能力。然而，在交互式金融市场（如交易）中，这些模型往往难以应对多步推理和目标导向的复杂场景，而这类场景通常需要智能体具备更强的自主决策能力，以优化交易策略。为此，我们提出了 FLAG-Trader，这是一种融合语言处理（通过 LLM）与基于梯度的强化学习（RL）策略优化的统一架构。在该架构中，部分微调的 LLM 作为策略网络，不仅利用预训练的知识，还通过高效的参数微调适应金融交易环境。通过交易奖励驱动的策略梯度优化，我们的框架不仅提升了 LLM 在交易任务中的决策能力，同时也优化了其在其他金融领域任务中的表现。我们提供了大量实证研究来验证这些改进的有效性。

## You Do Not Fully Utilize Transformer's Representation Capacity
[你并未充分利用 Transformer 的表征能力](https://arxiv.org/abs/2502.09245)

与循环神经网络（RNN）不同，后者将历史 Token 聚合为单一的隐藏状态，而 Transformer 具备直接关注所有历史 Token 的能力。然而，标准 Transformer 仅依赖来自紧邻上一层的表征。这一设计选择可能导致表征坍缩，从而限制模型性能。在本文中，我们提出了一种简单而高效的方法——层集成记忆（Layer-Integrated Memory，LIMe），该方法在保持模型整体内存占用不变的同时，通过允许访问早期层的隐藏状态来增强表征能力。通过在多种架构和不同查询机制上的大规模实验，我们验证了 LIMe 在广泛任务上的稳定性能提升。此外，我们通过分析学习到的表征动态，并结合深度电路的研究，揭示了 LIMe 如何跨层集成信息，同时也为未来研究指明了新的方向。

## SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation
[SoFar：基于语言的方向感知桥接空间推理与物体操控](https://arxiv.org/abs/2502.13143)

空间理解能力是具身 AI（Embodied AI）的核心要素之一，它使机器人能够更准确地感知环境并进行交互。近年来，视觉语言模型（VLM）的进步提升了对物体位置及其相对关系的感知能力，然而，这些模型仍然难以精准识别物体方向，而物体方向对于精细操控任务至关重要。解决这一问题不仅依赖于几何推理，还需要一种直观且表达能力强的方向表示方法。在此背景下，我们提出，自然语言相较于传统参考坐标系提供了更灵活的表示空间，使其特别适用于基于指令的机器人系统。

在本文中，我们引入**语义方向（Semantic Orientation）**的概念，该概念通过自然语言在无需固定参考系的情况下定义物体方向（例如，USB 的“插入”方向或刀具的“手柄”方向）。为支持这一方法，我们构建了 **OrienText300K**，一个大规模的 3D 模型数据集，该数据集标注了物体的语义方向，从而将几何信息与语义功能相结合。通过将语义方向集成到 VLM 系统中，我们使机器人在执行操控任务时能够同时考虑位置和方向约束。大量仿真和真实环境实验表明，我们的方法显著增强了机器人操控能力，例如在 Open6DOR 基准测试中达到 48.7% 的准确率，在 SIMPLER 基准测试中达到 74.9% 的准确率。

## SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models
[SafeRoute：大语言模型的高效且精准的安全防护自适应模型选择](https://arxiv.org/abs/2502.12464)

在实际应用中部署大语言模型（LLM）需要强大的安全防护模型，以检测并拦截有害的用户输入。尽管大型安全防护模型表现优异，但其计算成本相当高。为降低计算开销，通常采用较小的蒸馏模型，但这些模型在处理“难例”时表现不佳，而较大的模型则能提供更精准的预测。我们观察到，大多数输入可以由较小的模型可靠地处理，只有一小部分需要更大模型的能力。基于这一观察，我们提出了 SafeRoute，一个用于区分难例与易例的二分类路由器。我们的方法针对路由器判定为难例的数据，选择性地应用较大的安全防护模型，从而在保持准确性的同时提升计算效率。多个基准数据集上的实验结果表明，我们的自适应模型选择策略在计算成本与安全性能之间取得了更优的平衡，并优于现有相关基线方法。

## Qwen2.5-VL Technical Report
[Qwen2.5-VL 技术报告](https://arxiv.org/abs/2502.13923)

我们介绍了 Qwen2.5-VL，这是 Qwen 视觉语言系列的最新旗舰模型，展示了基础能力和创新功能的显著进展。Qwen2.5-VL 在通过增强的视觉识别、精确的物体定位、强大的文档解析和长视频理解，推动了理解和与世界互动的重大飞跃。Qwen2.5-VL 的一个突出特点是能够精确地使用边界框或点来定位物体。它能够从发票、表格和表格中提取强大的结构化数据，还能对图表、图示和布局进行详细分析。为了处理复杂的输入，Qwen2.5-VL 引入了动态分辨率处理和绝对时间编码，使其能够处理不同尺寸的图像和延时较长的视频（最长可达几个小时），并实现秒级事件定位。这使得模型能够在不依赖传统归一化技术的情况下，原生感知空间尺度和时间动态。通过从头开始训练一个原生动态分辨率的视觉变换器（ViT）并结合窗口注意力机制，我们减少了计算开销，同时保持原生分辨率。因此，Qwen2.5-VL 不仅在静态图像和文档理解方面表现出色，而且作为一个互动的视觉代理，能够在实际场景中进行推理、工具使用和任务执行，例如操作计算机和移动设备。Qwen2.5-VL 提供三种不同的模型尺寸，适用于从边缘 AI 到高性能计算的各种用例。旗舰模型 Qwen2.5-VL-72B 与 GPT-4o 和 Claude 3.5 Sonnet 等最先进的模型相媲美，尤其在文档和图表理解方面表现突出。此外，Qwen2.5-VL 保持了强大的语言能力，保留了 Qwen2.5 大型语言模型的核心语言能力。

## On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective
[关于生成性基础模型的可信度：指南、评估与展望](https://arxiv.org/abs/2502.14296)

生成性基础模型（GenFMs）已成为具有变革性的工具。然而，它们的广泛采用引发了关于可信度的关键问题。本文提出了一个全面的框架，通过三大核心贡献来解决这些挑战。首先，我们系统回顾了全球各国政府和监管机构的人工智能治理法律和政策，以及行业实践和标准。基于这些分析，我们提出了一套为 GenFMs 制定的指导原则，这些原则通过广泛的多学科合作开发，融合了技术、伦理、法律和社会等多个视角。其次，我们推出了 TrustGen，这是第一个动态基准评估平台，旨在评估多个维度和模型类型的可信度，包括文本到图像、大型语言模型和视觉语言模型。TrustGen 利用模块化组件——元数据整理、测试用例生成和上下文变化——实现自适应和迭代评估，克服了静态评估方法的局限性。通过使用 TrustGen，我们揭示了可信度的显著进展，同时也识别了持续存在的挑战。最后，我们深入讨论了可信 GenFM 的挑战与未来方向，揭示了可信度复杂且不断发展的本质，突出效用与可信度之间的微妙权衡，并考虑到各种下游应用，识别持续的挑战，并为未来的研究提供战略性路线图。本研究建立了一个全面的框架，为推动 GenAI 中的可信度奠定基础，为将 GenFMs 安全、负责任地集成到关键应用中铺平道路。为了促进社区的进步，我们将发布动态评估工具包。

## SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation
[SongGen: 基于单阶段自回归 Transformer 的文本到歌曲生成](https://arxiv.org/abs/2502.13128)

文本到歌曲生成（Text-to-song generation）任务旨在从文本输入合成包含人声和伴奏的音频。然而，由于该任务涉及复杂的音乐领域，同时训练数据较为稀缺，因此面临诸多挑战。现有方法通常采用多阶段生成流程，使得训练和推理流程较为复杂。为此，本文提出了 SongGen，这是一种完全开源的单阶段自回归 Transformer，专为可控的歌曲生成而设计。该模型支持对多种音乐属性进行细粒度控制，包括歌词、乐器编排的文本描述、音乐流派、情绪和音色。此外，它还提供了一个可选的三秒参考音频片段用于语音克隆。

在统一的自回归框架下，SongGen 提供两种输出模式：
- **混合模式（Mixed mode）**：直接生成包含人声和伴奏的混合音频；
- **双轨模式（Dual-track mode）**：分别合成人声和伴奏，以提升下游应用的灵活性。

我们针对每种模式探索了不同的 Token 策略，从而实现了显著改进，并提供了重要的见解。此外，我们设计了一条自动化的数据预处理流水线，并引入了有效的质量控制机制。

为促进社区参与和推动后续研究，我们将开放模型权重、训练代码、标注数据以及预处理流水线。更多示例可在我们的项目页面查看：https://liuzh-19.github.io/SongGen/ ，代码将在 https://github.com/LiuZH-19/SongGen 上发布。

## RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning
[RAD: 基于大规模 3DGS 强化学习的端到端驾驶策略训练](https://arxiv.org/abs/2502.13144)

当前端到端自动驾驶（AD）算法通常采用模仿学习（IL）范式，但这种方法存在因果混淆（causal confusion）和开环缺陷（open-loop gap）等挑战。为此，本文提出了一种基于 3DGS 的闭环强化学习（RL）训练框架。利用 3DGS 技术，我们构建了一个高度逼真的数字复制环境，使自动驾驶策略能够在状态空间中进行广泛探索，并通过大规模试错学习适应分布外（out-of-distribution）场景。

为提高安全性，我们设计了一套专门的奖励机制，引导策略有效应对安全关键事件，并学习真实世界的因果关系。此外，为了更好地对齐人类驾驶行为，我们在强化学习训练过程中引入了模仿学习作为正则化约束。我们还提出了一个包含多种全新 3DGS 环境的闭环评测基准。

相比 IL 方法，RAD 在大多数闭环指标上表现更优，尤其是碰撞率降低了 3 倍。完整的闭环实验结果展示在：https://hgao-cv.github.io/RAD 。

## MoM: Linear Sequence Modeling with Mixture-of-Memories
[MoM: 混合记忆（MoM）用于线性序列建模](https://arxiv.org/abs/2502.13685)

线性序列建模（Linear sequence modeling）方法（如线性注意力（linear attention）、状态空间建模（state space modeling）和线性 RNN）通过降低训练和推理的计算复杂度，实现了显著的效率提升。然而，这些方法通常将整个输入序列映射到一个固定大小的记忆状态，导致在依赖回忆能力的下游任务上表现不佳。

受神经科学的启发，尤其是大脑在维持稳健的长期记忆的同时减少“记忆干涉（memory interference）”的能力，我们提出了一种新架构——混合记忆（Mixture-of-Memories, MoM）。MoM 采用多个独立的记忆状态，并通过路由网络（router network）动态地将输入 Token 分配到不同的记忆状态。这种方法在提升整体记忆容量的同时，有效减少了记忆干涉，使 MoM 在依赖回忆能力的任务上表现卓越，优于现有的线性序列建模方法。

尽管 MoM 采用了多个记忆状态，但每个记忆状态的计算复杂度仍保持线性，从而使 MoM 在训练过程中保持线性复杂度，在推理阶段保持常数复杂度。实验结果表明，MoM 在下游语言任务，特别是依赖回忆能力的任务上，显著优于当前的线性序列模型，在某些任务上，其性能甚至可与 Transformer 模型相媲美。

代码已开放，访问链接：https://github.com/OpenSparseLLMs/MoM 。MoM 也是 https://github.com/OpenSparseLLMs/Linear-MoE 项目的一部分。

## Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering
[最终答案？测试时扩展计算能力可提升选择性问答性能](https://arxiv.org/abs/2502.13962)

在推理阶段增加计算资源已被证明能够显著提升大语言模型（LLM）在推理基准测试中的表现。然而，现有研究通常假设推理系统应当对所有问题都提供答案，而未考虑模型的置信度及其是否应当始终作答。为了解决这一问题，我们在推理过程中提取置信度分数，并基于阈值对模型响应进行筛选。

实验发现，增加推理阶段的计算预算不仅可以使模型回答更多问题正确，还能提高正确回答的置信度。此外，我们扩展了当前以“零风险响应”为基础的评估范式，探索了允许一定程度风险的评估设定，并提出了一种适用于该设定的评估方案。


## MMTEB: Massive Multilingual Text Embedding Benchmark
[MMTEB: 大规模多语言文本嵌入基准](https://arxiv.org/abs/2502.13595)

文本嵌入（Text Embedding）通常仅在有限的任务集上进行评估，受限于语言、领域及任务多样性。为提供更全面的评估，我们提出 **大规模多语言文本嵌入基准（Massive Multilingual Text Embedding Benchmark, MMTEB）**，这是一项社区驱动的 MTEB 扩展，涵盖 250 多种语言、超过 500 个经过质量控制的评估任务。

MMTEB 进一步引入了多个具有挑战性的全新任务，如指令遵循（instruction following）、长文档检索（long-document retrieval）和代码检索（code retrieval），构成迄今最大规模的多语言嵌入模型评测集合。基于该数据集，我们构建了一系列高度多语言化的基准，并对一组具有代表性的模型进行评测。研究发现，尽管参数规模达数十亿的大语言模型（LLM）在部分语言子集和任务类别上可达到最先进（state-of-the-art）性能，但目前公开可用的最佳模型是 **multilingual-e5-large-instruct**，其参数量仅 5.6 亿。

为提高可访问性并降低计算成本，我们提出了一种基于任务间相关性（inter-task correlation）的新型降采样方法，在保证任务多样性的同时，保持模型排名的一致性。此外，我们优化了检索任务，引入困难负样本（hard negatives）采样技术，创建更紧凑但高效的测试集。这些优化措施使我们得以构建计算需求大幅降低的评测基准。例如，我们提出的 **零样本英语基准（zero-shot English benchmark）** 仅需完整版本的一小部分计算量，却能保持相似的排名顺序。

## LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization
[LongPO: 基于短到长偏好优化的大语言模型长上下文自进化](https://arxiv.org/abs/2502.13922)

大语言模型（LLMs）在预训练和对齐过程中展现出强大的能力。然而，即使在短上下文任务上表现卓越的 LLMs，在长上下文场景中仍可能因缺乏有效对齐而表现欠佳。长上下文对齐面临两大挑战：一是人工标注长上下文数据的难度极高，二是难以平衡短上下文与长上下文任务的性能。

为此，我们提出 **LongPO**，该方法使短上下文 LLMs 能够通过内部迁移短上下文能力至长上下文任务，实现自进化并提升长上下文表现。LongPO 让 LLMs 从自生成的短到长偏好数据中学习，这些数据由相同指令在长上下文输入与其压缩的短上下文输入下分别生成的成对响应构成。这些偏好数据揭示了 LLMs 在短上下文对齐过程中习得的能力，而这些能力在长上下文任务中可能被削弱。此外，LongPO 还引入短到长 KL 约束，以防止长上下文对齐过程中短上下文性能下降。

实验表明，LongPO 在 Mistral-7B-Instruct-v0.2 上（128K 扩展至 512K 上下文长度）能够完全保持短上下文性能，并在长上下文和短上下文任务上均显著超越普通 SFT 和 DPO 方法。LongPO 训练的模型在长上下文基准测试上的表现可媲美甚至超越 GPT-4-128K 等依赖大量长上下文标注和更大参数规模的 LLMs。


## MLGym: A New Framework and Benchmark for Advancing AI Research Agents
[MLGym: 面向 AI 研究智能体的全新框架与基准测试](https://arxiv.org/abs/2502.14499)

我们提出 **Meta MLGym 和 MLGym-Bench**，这是一个专为 AI 研究任务中的 LLM 智能体设计的全新框架和基准测试集。这是首个针对机器学习（ML）任务的 Gym 环境，使得研究者可以利用强化学习（RL）算法训练 AI 研究智能体。

MLGym-Bench 涵盖 13 个来自计算机视觉（CV）、自然语言处理（NLP）、强化学习（RL）和博弈论等领域的多样化 AI 研究任务。研究发现，当前前沿 LLMs 主要通过优化超参数在基准测试上取得一定提升，但尚未展现出生成全新假设、算法或架构的能力，亦未能实现突破性改进。我们开源该框架与基准测试，以促进 AI 研究智能体的发展。

## SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features
[SigLIP 2: 增强语义理解、定位与密集特征的多语言视觉-语言编码器](https://arxiv.org/abs/2502.14786)

我们提出 **SigLIP 2**，一系列继承 SigLIP 成功经验的新型多语言视觉-语言编码器（Vision-Language Encoders, VLE）。在这一版本中，我们将原始的图像-文本训练目标与多个独立发展的技术整合，形成统一的训练策略，包括：
- **基于字幕的预训练（captioning-based pretraining）**
- **自监督损失（self-distillation, masked prediction）**
- **在线数据筛选（online data curation）**

得益于这些改进，SigLIP 2 在所有模型规模下均超越 SigLIP，特别是在零样本分类、图像-文本检索以及视觉-语言模型（VLM）表征迁移等核心任务中。此外，新训练策略在定位（localization）和密集预测（dense prediction）任务上带来了显著提升。

SigLIP 2 还采用了更多样化的数据混合策略，并结合去偏技术（de-biasing techniques），显著提升了多语言理解能力及公平性。我们提供四种规模的模型，以便用户在推理成本与性能之间做出权衡：
- **ViT-B (86M)**
- **ViT-L (303M)**
- **So400m (400M)**
- **ViT-g (1B)**

## SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines
[SuperGPQA: 285 个研究生学科的大规模 LLM 评测基准](https://arxiv.org/abs/2502.14739)

当前大语言模型（LLMs）主要在数学、物理和计算机科学等学科取得突破，但在人类知识体系中仍有超过 200 个专业学科未被充分评估，尤其是轻工业、农业和服务型学科。

为填补这一空白，我们提出 **SuperGPQA**，这是首个覆盖 285 个研究生学科的 LLM 评测基准。该基准采用 **人类-LLM 协作过滤机制（Human-LLM collaborative filtering）**，通过 LLM 生成的回答与专家反馈多轮筛选，以去除琐碎或模棱两可的问题。SuperGPQA 的实验结果揭示了当前 LLM 在多学科领域的显著性能差距，进一步凸显了 LLM 向通用人工智能（AGI）迈进的挑战。

## How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?
[如何在不影响 LLM 的情况下最大化 LoRA 适配器的知识容量？](https://arxiv.org/abs/2502.14502)

大语言模型（LLMs）在许多任务中的表现受到其预训练阶段学到的知识及其存储在模型参数中的信息的限制。低秩适配（LoRA）是一种高效的训练方法，广泛用于 LLM 的更新和特定领域适配。在本研究中，我们探讨了如何利用 LoRA 在不影响已有知识的情况下向 LLM 注入新知识。

在实验中，我们使用 LoRA 对 **Llama-3.1-8B-instruct** 进行微调，并测试了不同规模的新知识输入。结果表明，最佳效果出现在训练数据同时包含已知事实与新知识的情况下。然而，这种方法仍可能对模型产生不利影响，例如模型在外部问答基准测试中的表现下降。此外，当训练数据对某些实体存在偏倚时，模型更容易生成少数过度代表的答案。此外，我们发现微调后的模型在极少数情况下表现出更高的置信度，并拒绝回答问题。这些发现揭示了基于 LoRA 进行知识更新的潜在风险，并强调了训练数据构成和参数调整在平衡新知识整合与模型整体能力方面的重要性。


## S*: Test Time Scaling for Code Generation
[S*: 代码生成中的测试时扩展计算](https://arxiv.org/abs/2502.14382)

在多个领域，提高 LLM 推理阶段的计算资源已被证明能显著提升性能，但代码生成任务在这方面仍然缺乏深入研究，而数学推理领域已进行过广泛探索。在本论文中，我们提出 **S***，一种混合型测试时扩展计算框架，可显著提升代码生成的覆盖范围和选择准确性。

S* 在现有的并行扩展计算（parallel scaling）方法基础上，结合了顺序扩展计算（sequential scaling），进一步突破性能瓶颈。此外，S* 采用了一种新颖的选择机制，它通过自适应生成区分性输入进行两两比较，并结合执行结果信息，以更可靠地识别正确解。我们在 12 个大语言模型（LLMs）和大推理模型（Large Reasoning Models）上进行了评测，主要结果如下：
1. **S*** 在不同模型家族和规模上均能稳定提升性能，使 **3B 规模的模型** 超越 **GPT-4o-mini**。
2. **S*** 使非推理模型超越推理模型——**GPT-4o-mini** 搭载 S* 后，在 **LiveCodeBench** 上比 **o1-preview** 高出 **3.7%**。
3. **S*** 进一步提升了最先进的推理模型表现——**DeepSeek-R1-Distill-Qwen-32B** 在 **LiveCodeBench** 上达到 **85.7%** 的准确率，接近 **o1 (high)** 的 **88.5%**。

代码将在 **https://github.com/NovaSky-AI/SkyThought** 公开发布。

## Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning
[Logic-RL: 基于规则的强化学习提升 LLM 推理能力](https://arxiv.org/abs/2502.14768)

受到 **DeepSeek-R1** 成功的启发，我们探索了**基于规则的强化学习（RL）** 在大规模推理模型中的潜力。为了分析推理过程，我们采用具有**可控复杂度** 且 **可直接验证答案** 的**合成逻辑谜题（synthetic logic puzzles）** 作为训练数据。

我们提出了三项关键改进，使 RL 训练更加稳定且高效：
- **系统提示（system prompt）**，强调模型的思考与回答过程；
- **格式化奖励函数（format reward function）**，惩罚跳过推理步骤的捷径答案；
- **稳定收敛的训练策略**，确保 RL 训练过程的可靠性。

训练后的 7B 规模模型展现出**反思（reflection）、验证（verification）和总结（summarization）** 等**原始逻辑语料库未曾包含的高级推理能力**。令人惊讶的是，仅训练 5000 道逻辑问题后，该模型便能在数学基准 **AIME** 和 **AMC** 上展现出良好的泛化能力。


## Discovering highly efficient low-weight quantum error-correcting codes with reinforcement learning
[通过强化学习优化量子纠错码，提升计算效率](https://arxiv.org/abs/2502.14372)

实现可扩展的容错量子计算需要**高效的量子纠错码（quantum error-correcting codes）**。在优化量子计算的过程中，**测量权重（measurement weight）** 是一个关键因素，它决定了用于提取错误信息的测量复杂度。由于高测量权重会增加实现成本并引入更多误差，优化测量权重成为量子编码设计的核心挑战。

本研究提出了一种**基于强化学习（RL）** 的稳定化码权重优化方法，该方法显著降低了量子纠错码的测量权重，在多个参数范围内远超现有技术水平。例如，我们的方法使**权重 6 的编码方案** 相比现有方案**减少 1 到 2 个数量级的物理量子比特开销**，并使其开销降至可用于近期实验的水平。此外，我们利用 RL 框架分析了**编码参数之间的相互作用**，提供了新的优化策略。

整体而言，我们的研究表明，强化学习在优化量子纠错码方面具有巨大的潜力，有望加速**容错量子计算的实际应用** 进程。
