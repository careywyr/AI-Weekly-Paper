## Enabling Scalable Oversight via Self-Evolving Critic
[通过自我进化的批评者实现可扩展的监督](https://arxiv.org/abs/2501.05727)

尽管大语言模型 (LLMs) 表现卓越，但其发展面临一个关键挑战：在人类评估困难或 LLMs 超越人类的任务中，如何提供有效的反馈。尽管使用 LLMs 进行批评的兴趣日益增长，但当前的方法仍然依赖于人类注释或更强大的模型，这使得在没有外部监督的情况下增强批评能力的问题仍未解决。我们提出了 SCRIT (Self-evolving CRITic)，这是一个能够实现批评能力真正自我进化的框架。从技术上讲，SCRIT 通过训练合成数据进行自我改进，这些数据由基于对比的自我批评者生成，该批评者使用参考解决方案进行逐步批评，并通过自我验证机制确保批评质量，该机制通过纠正结果来确保批评质量。使用 Qwen2.5-72B-Instruct（最强大的 LLMs 之一）实现，SCRIT 在批评纠正和错误识别基准测试中实现了高达 10.3\% 的提升。我们的分析表明，SCRIT 的性能随着数据和模型规模的增加而正向扩展，优于其他方法，并且其自我验证组件对其性能至关重要。

## VideoRAG: Retrieval-Augmented Generation over Video Corpus
[VideoRAG: 视频语料库的检索增强生成](https://arxiv.org/abs/2501.05874)

检索增强生成 (Retrieval-Augmented Generation, RAG) 是一种通过检索与查询相关的外部知识并将其整合到生成过程中，来解决基础模型生成事实错误输出的强大策略。然而，现有的 RAG 方法主要集中在文本信息上，最近的一些进展开始考虑图像，但它们大多忽略了视频，这是一种丰富的多模态知识来源，能够比其他任何模态更有效地表示事件、过程和上下文细节。虽然最近的一些研究探索了在响应生成过程中集成视频的方法，但它们要么预定义与查询相关的视频而不根据查询进行检索，要么将视频转换为文本描述而不利用其多模态的丰富性。为了解决这些问题，我们引入了 VideoRAG，这是一个新颖的框架，不仅根据查询动态检索相关视频，还在输出生成中利用视频的视觉和文本信息。此外，为了实现这一点，我们的方法围绕最近的大视频语言模型 (Large Video Language Models, LVLMs) 的进展展开，这些模型能够直接处理视频内容以进行检索，并将检索到的视频与查询无缝集成。我们通过实验验证了 VideoRAG 的有效性，展示了其优于相关基线方法的表现。

## LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs
[LlamaV-o1: 重新思考大语言模型中的逐步视觉推理](https://arxiv.org/abs/2501.06186)

推理是解决复杂多步问题的基本能力，特别是在需要顺序逐步理解的视觉环境中。现有方法缺乏一个全面的框架来评估视觉推理，并且没有强调逐步问题解决。为此，我们提出了一个全面的框架，通过三个关键贡献来推进大语言模型（LLMs）中的逐步视觉推理。首先，我们引入了一个专门设计用于评估多步推理任务的视觉推理基准。该基准提出了八种不同类别的多样化挑战，从复杂的视觉感知到科学推理，总共包含超过4000个推理步骤，使得能够对大语言模型在多步中进行准确和可解释的视觉推理能力进行稳健评估。其次，我们提出了一种新的指标，以单个步骤的粒度评估视觉推理质量，强调正确性和逻辑一致性。与传统的任务最终准确率指标相比，所提出的指标提供了对推理性能的更深入洞察。第三，我们提出了一种新的多模态视觉推理模型，名为LlamaV-o1，使用多步课程学习方法进行训练，其中任务逐步组织以促进增量技能获取和问题解决。所提出的LlamaV-o1专为多步推理设计，并通过结构化的训练范式逐步学习。大量实验表明，我们的LlamaV-o1优于现有的开源模型，并在与闭源私有模型的比较中表现良好。与最近的Llava-CoT相比，我们的LlamaV-o1在六个基准测试中平均得分为67.3，绝对提升为3.8\%，同时在推理扩展阶段速度提高了5倍。我们的基准、模型和代码都是公开可用的。

## OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints
[OmniManip: 通过以对象为中心的交互原语作为空间约束实现通用机器人操作](https://arxiv.org/abs/2501.03841)

开发能够在非结构化环境中进行操作的通用机器人系统是一个重大挑战。虽然视觉-语言模型（Vision-Language Models, VLM）在高级常识推理方面表现出色，但它们缺乏精确操作任务所需的细粒度三维空间理解能力。通过在机器人数据集上微调VLM以创建视觉-语言-动作模型（Vision-Language-Action Models, VLA）是一个潜在的解决方案，但它受到高数据收集成本和泛化问题的阻碍。为了解决这些挑战，我们提出了一种新颖的以对象为中心的表示方法，弥合了VLM的高级推理与操作所需的低级精度之间的差距。我们的关键见解是，由对象的功能可供性（functional affordances）定义的规范空间（canonical space）提供了一种结构化和语义上有意义的方式来描述交互原语，例如点和方向。这些原语充当桥梁，将VLM的常识推理转化为可操作的三维空间约束。在此背景下，我们引入了一种双闭环、开放词汇的机器人操作系统：一个循环用于通过原语重采样、交互渲染和VLM检查进行高级规划，另一个循环用于通过六维姿态跟踪（6D pose tracking）进行低级执行。这种设计确保了无需VLM微调的鲁棒实时控制。大量实验证明了在各种机器人操作任务中的强大零样本（zero-shot）泛化能力，突显了这种方法在自动化大规模仿真数据生成方面的潜力。

## OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?
[OVO-Bench: 您的视频大语言模型离现实世界的在线视频理解还有多远？](https://arxiv.org/abs/2501.05510)

时间感知能力，即根据问题提出时的时间戳进行动态推理的能力，是离线与在线视频大语言模型的关键区别。离线模型依赖完整视频进行静态事后分析，而在线模型则逐步处理视频流，并根据问题提出时的时间戳动态调整响应。尽管时间感知能力至关重要，但现有基准测试中并未对其进行充分评估。为此，我们提出了OVO-Bench（在线视频基准测试），这是一个强调时间戳对高级在线视频理解能力评估重要性的新型视频基准测试。OVO-Bench评估视频大语言模型在三种场景下对特定时间戳事件进行推理和响应的能力：（1）回溯追踪：回溯过去事件以回答问题；（2）实时理解：在当前时间戳发生时理解和响应事件；（3）前瞻性主动响应：延迟响应，直到获得足够的未来信息以准确回答问题。OVO-Bench包含12个任务，涵盖644个独特视频和约2800个人工策划的精细元注释，这些注释带有精确时间戳。我们结合了自动化生成管道与人工策划，并基于这些高质量样本开发了评估管道，以系统地沿视频时间线查询视频大语言模型。对九个视频大语言模型的评估显示，尽管在传统基准测试上取得了进展，但当前模型在在线视频理解方面仍存在困难，与人类智能体相比存在显著差距。我们希望OVO-Bench能推动视频大语言模型的进展，并激发未来在线视频推理的研究。我们的基准测试和代码可在https://github.com/JoeLeelyf/OVO-Bench访问。

## Migician: Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models
[Migician: 揭示多模态大语言模型中自由形式多图像定位的奥秘](https://arxiv.org/abs/2501.05767)

近年来，多模态大语言模型（MLLMs）的进展显著提升了它们对单张图像的细粒度感知以及对多张图像的综合理解能力。然而，现有的 MLLMs 在复杂的多图像场景中实现精确定位仍然面临挑战。为此，我们首先探索了一种思维链（CoT）框架，该框架将单图像定位与多图像理解相结合。虽然在一定程度上有效，但由于其非端到端的性质，该框架仍然不稳定，并且难以捕捉抽象的视觉信息。因此，我们提出了 Migician，这是第一个能够在多张图像上执行自由形式和精确定位的多图像定位模型。为此，我们构建了 MGrounding-630k 数据集，该数据集包含从现有数据集中提取的多个多图像定位任务的数据，以及新生成的自由形式定位指令跟随数据。此外，我们提出了 MIG-Bench，这是一个专门用于评估多图像定位能力的综合基准。实验结果表明，我们的模型在多图像定位能力上显著优于现有最好的 MLLMs，超出 21.61 个百分点，甚至超过了更大的 70B 模型。我们的代码、模型、数据集和基准均已开源。

## The Lessons of Developing Process Reward Models in Mathematical Reasoning
[数学推理中过程奖励模型开发的启示](https://arxiv.org/abs/2501.07301)

过程奖励模型 (Process Reward Models, PRMs) 是一种在大语言模型 (Large Language Models, LLMs) 数学推理过程中进行过程监督的有前景的方法，旨在识别并减少推理过程中的中间错误。然而，开发有效的 PRMs 面临诸多挑战，尤其是在数据标注和评估方法方面。本文通过大量实验表明，常用的基于蒙特卡罗 (Monte Carlo, MC) 估计的数据合成方法通常表现较差，泛化能力也不如 LLM-as-a-judge 和人工标注方法。MC 估计依赖于完成模型来评估当前步骤的正确性，这导致步骤验证不准确。此外，我们还发现传统 Best-of-N (BoN) 评估策略在 PRMs 中存在潜在偏差：(1) 不可靠的策略模型生成的响应虽然答案正确，但过程存在缺陷，导致 BoN 的评估标准与 PRMs 的过程验证目标不一致。(2) PRMs 对此类响应的接受度导致 BoN 分数虚高。(3) 现有的 PRMs 在最终答案步骤上集中了大量最低分，这表明 BoN 优化的 PRMs 从过程评估转向了结果评估。为了解决这些问题，我们开发了一种共识过滤机制，有效地将 MC 估计与 LLM-as-a-judge 结合，并提出了一种结合响应级别和步骤级别指标的更全面的评估框架。基于这些机制，我们在 BoN 评估和逐步错误识别任务中显著提升了模型性能和数据效率。最后，我们发布了一个新的最先进的 PRM，其性能优于现有的开源替代方案，并为未来构建过程监督模型的研究提供了实用指南。

## Tensor Product Attention Is All You Need
[Tensor Product Attention Is All You Need](https://arxiv.org/abs/2501.06425)

扩展语言模型的输入序列长度通常需要大量的键值（KV）缓存，这会在推理过程中带来显著的内存开销。在本文中，我们提出了 Tensor Product Attention (TPA)，一种新颖的注意力机制，它使用张量分解来紧凑地表示查询、键和值，从而在推理时显著缩小 KV 缓存的大小。通过将这些表示分解为上下文相关的低秩分量（上下文分解）并与 RoPE 无缝集成，TPA 在提升模型质量的同时也提升了内存效率。基于 TPA，我们引入了 Tensor ProducT ATTenTion Transformer (T6)，一种用于序列建模的新模型架构。通过对语言建模任务的广泛实证评估，我们证明了 T6 在困惑度等多个评估指标上超越了包括 MHA、MQA、GQA 和 MLA 在内的标准 Transformer 基线。值得注意的是，TPA 的内存效率使得在固定资源约束下能够处理显著更长的序列，应对了现代语言模型中的一个关键可扩展性挑战。代码可在 https://github.com/tensorgi/T6 获取。

## $\text{Transformer}^2$: Self-adaptive LLMs
[$\text{Transformer}^2$: 自适应大语言模型](https://arxiv.org/abs/2501.06252)

自适应大语言模型 (LLMs) 旨在解决传统微调方法带来的挑战，这些方法通常在计算上非常密集，并且在处理多样化任务时能力是静态不变的。我们引入了 \implname，一种新颖的自适应框架，通过有选择地仅调整其权重矩阵的单一组件部分，实时适应未见过的任务。在推理过程中，\implname 采用了一种两阶段机制：首先，一个调度系统识别任务属性，然后使用强化学习训练的任务特定“专家”向量被动态混合，以获得针对输入提示的针对性响应。我们的方法在参数更少、效率更高的情况下，优于诸如 LoRA 等广泛使用的方法。\implname 展示了在不同 LLM 架构和模态（包括视觉-语言任务）中的广泛适用性。\implname 代表了一个重大的飞跃，为增强 LLMs 的适应性和任务特定性能提供了一个可扩展、高效的解决方案，为真正动态、自组织的 AI 系统铺平了道路。

## BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature

[BIOMEDICA: 一个开放的生物医学图像-描述存档、数据集和从科学文献中衍生的视觉-语言模型](https://arxiv.org/abs/2501.07171)
视觉-语言模型 (Vision-Language Models, VLMs) 的发展依赖于大规模且多样化的多模态数据集。然而，由于缺乏跨生物学和医学领域的标注、公开可访问的数据集，通用生物医学视觉-语言模型的进展受到了限制。现有的工作大多局限于狭窄的领域，未能涵盖科学文献中编码的完整生物医学知识。为了解决这一问题，我们提出了 BIOMEDICA，这是一个可扩展的开源框架，用于提取、标注和序列化处理 PubMed Central 开放访问子集的全部内容，形成一个易于使用且公开可访问的数据集。我们的框架生成了一个包含超过 2400 万独特图像-文本对的综合数据集，这些数据来自超过 600 万篇文章。我们还提供了元数据和专家指导的标注。为了展示该资源的实用性和可访问性，我们发布了 BMCA-CLIP，这是一套通过流式传输在 BIOMEDICA 数据集上持续预训练的 CLIP 模型，无需本地下载 27 TB 的数据。平均而言，我们的模型在 40 个任务中实现了最先进的性能，涵盖病理学、放射学、眼科学、皮肤病学、外科、分子生物学、寄生虫学和细胞生物学等领域，在零样本分类中表现优异，平均提升了 6.56%（在皮肤病学和眼科学中分别高达 29.8% 和 17.5%），并且在图像-文本检索方面表现更强，同时计算资源使用减少了 10 倍。为了促进可重复性和协作，我们向更广泛的研究社区发布了代码库和数据集。

## MinMo: A Multimodal Large Language Model for Seamless Voice Interaction
[MinMo: 用于无缝语音交互的多模态大语言模型](https://arxiv.org/abs/2501.06282)

近年来，大语言模型（LLMs）和多模态语音-文本模型的快速发展为无缝语音交互提供了基础支持，使得实时、自然且类人的对话成为可能。以往的语音交互模型主要分为原生模型和对齐模型。原生模型在一个框架中集成了语音和文本处理，但存在序列长度不一致和预训练不足等问题。对齐模型虽然保留了文本大语言模型的能力，但通常受限于小数据集和对语音任务的狭窄关注。在本研究中，我们提出了 MinMo，一个拥有约 80 亿参数的多模态大语言模型，用于无缝语音交互。我们通过多阶段的语音-文本对齐、文本-语音对齐、语音-语音对齐和双工交互对齐训练，解决了以往对齐多模态模型的主要限制。MinMo 在 140 万小时的多样化语音数据和广泛的语音任务上进行了训练。经过多阶段训练后，MinMo 在各种语音理解和生成基准测试中达到了业界领先的性能，同时保留了文本大语言模型的能力，并且支持全双工对话，即用户与系统之间的双向同时通信。此外，我们提出了一种新颖且简单的语音解码器，在语音生成方面表现优于以往的模型。MinMo 增强的指令跟随能力支持基于用户指令控制语音生成，包括情感、方言和语速等多种细微差别，并能够模仿特定声音。对于 MinMo，语音到文本的延迟约为 100 毫秒，全双工延迟在理论上约为 600 毫秒，实际中约为 800 毫秒。MinMo 项目网页为 https://funaudiollm.github.io/minmo，代码和模型将很快发布。

## VideoAuteur: Towards Long Narrative Video Generation
[VideoAuteur: 迈向长篇叙事视频生成](https://arxiv.org/abs/2501.06173)

近期的视频生成模型在生成几秒钟的高质量视频片段方面取得了显著进展。然而，这些模型在生成长序列时仍面临挑战，难以生成传达清晰且信息丰富的事件，从而限制了其支持连贯叙事的能力。本文提出了一个大规模的烹饪视频数据集，旨在推动烹饪领域的长篇叙事生成。我们分别使用最先进的视觉-语言模型（VLMs）和视频生成模型，验证了该数据集在视觉保真度和文本字幕准确性方面的质量。此外，我们引入了一个长篇叙事视频导演模块，以增强生成视频的视觉和语义连贯性，并强调了视觉嵌入对齐在提升整体视频质量中的重要作用。通过整合文本和图像嵌入的微调技术，我们的方法在生成视觉细节丰富且语义对齐的关键帧方面取得了显著改进。
项目页面: https://videoauteur.github.io/

## O1 Replication Journey -- Part 3: Inference-time Scaling for Medical Reasoning
[O1 复制之旅 —— 第三部分：医学推理中的推理时扩展](https://arxiv.org/abs/2501.06458)

基于我们之前对 O1 复制的研究（第一部分：旅程学习 [Qin et al., 2024] 和第二部分：蒸馏 [Huang et al., 2024]），本研究探讨了大语言模型（LLMs）在医学推理任务中推理时扩展的潜力，涵盖从诊断决策到治疗计划的多个方面。通过对不同复杂度的医学基准（MedQA、Medbullets 和 JAMA Clinical Challenges）进行广泛实验，我们得出了以下关键结论：(1) 增加推理时间确实能够提升模型性能。在仅有 500 个样本的训练集下，我们的模型实现了 6%-11% 的显著性能提升。(2) 任务复杂度与所需推理链的长度直接相关，这进一步证实了在处理复杂问题时扩展思维过程的必要性。(3) 我们的模型生成的鉴别诊断遵循假设演绎法的原则，能够生成一个可能解释患者症状的潜在条件列表，并通过评估证据系统地缩小这些可能性。这些发现表明，推理时扩展与旅程学习在提升 LLMs 的临床推理能力方面具有显著的协同效应。

## MiniMax-01: Scaling Foundation Models with Lightning Attention
[MiniMax-01: Scaling Foundation Models with Lightning Attention](https://arxiv.org/abs/2501.08313)

我们介绍了 MiniMax-01 系列，包括 MiniMax-Text-01 和 MiniMax-VL-01，
它们在处理更长上下文时提供了卓越的能力，同时与顶级模型相当。核心在于闪电注意力（Lightning Attention）及其
高效的扩展。为了最大化计算能力，我们将其与专家混合模型（Mixture of Experts, MoE）结合，
创建了一个包含 32 个专家和 4560 亿总参数的模型，其中每个 Token 激活 459 亿参数。我们
为 MoE 和闪电注意力开发了优化的并行策略和高效的计算-通信重叠技术。
这种方法使我们能够在跨越数百万 Token 的上下文中，对具有数千亿参数的模型进行高效的训练和推理。
MiniMax-Text-01 的上下文长度在训练期间可以达到 100 万 Token，并在推理期间以较低的成本外推到 400 万 Token。
我们的视觉语言模型 MiniMax-VL-01 是通过继续训练 5120 亿视觉语言 Token 构建的。
在标准和内部基准测试中，实验表明我们的模型在性能上与 GPT-4o 和 Claude-3.5-Sonnet 等最先进的模型相当，
同时提供了 20-32 倍更长的上下文长度。我们在 https://github.com/MiniMax-AI 公开发布了 MiniMax-01。

## MangaNinja: Line Art Colorization with Precise Reference Following
[MangaNinja: 精确参考跟随的线稿上色](https://arxiv.org/abs/2501.08332)

源自扩散模型，MangaNinja 专注于参考引导的线稿上色任务。我们设计了两个精心构思的模块，以确保角色细节的精确转录，包括一个补丁混洗模块，用于促进参考彩色图像与目标线稿之间的对应关系学习，以及一个点驱动的控制方案，以实现精细的颜色匹配。在自建基准测试上的实验表明，我们的模型在精确上色方面优于现有解决方案。我们进一步展示了所提出的交互式点控制在处理具有挑战性的案例、跨角色上色、多参考协调方面的潜力，这些是现有算法无法企及的。

## 3DIS-FLUX: simple and efficient multi-instance generation with DiT rendering
[3DIS-FLUX: 使用 DiT 渲染的简单高效多实例生成](https://arxiv.org/abs/2501.05131)

随着文本到图像生成中对可控输出的需求不断增加，多实例生成 (MIG) 领域取得了显著进步，使用户能够指定实例的布局和属性。目前，MIG 中最先进的方法主要基于适配器。然而，这些方法在每当发布更先进的模型时都需要重新训练一个新的适配器，导致资源消耗巨大。为此，一种名为深度驱动的解耦实例合成 (3DIS) 的方法被引入，它将 MIG 解耦为两个不同的阶段：1) 基于深度的场景构建和 2) 使用广泛预训练的深度控制模型进行细节渲染。3DIS 方法仅在场景构建阶段需要进行适配器训练，同时允许各种模型进行无需训练的细节渲染。最初，3DIS 主要采用 U-Net 架构（如 SD1.5、SD2 和 SDXL）的渲染技术，而没有探索最近基于 DiT 的模型（如 FLUX）的潜力。在本文中，我们介绍了 3DIS-FLUX，这是 3DIS 框架的扩展，集成了 FLUX 模型以增强渲染能力。具体来说，我们使用 FLUX.1-Depth-dev 模型进行深度图控制的图像生成，并引入了一个细节渲染器，该渲染器基于布局信息控制 FLUX 的联合注意力机制中的注意力掩码。这种方法使得每个实例的细粒度属性能够被精确渲染。我们的实验结果表明，利用 FLUX 模型的 3DIS-FLUX 在性能和图像质量方面优于使用 SD2 和 SDXL 的原始 3DIS 方法，并且超越了当前最先进的基于适配器的方法。项目页面：
https://limuloo.github.io/3DIS/。

## Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models
[Padding Tone: 对 T2I 模型中填充 Token 的机制分析](https://arxiv.org/abs/2501.06751)

文本到图像（Text-to-image, T2I）扩散模型依赖于编码的提示来指导图像生成过程。通常，这些提示通过在文本编码之前添加填充 Token（Padding Token）扩展到固定长度。尽管这是一种默认做法，但填充 Token 对图像生成过程的影响尚未被研究。在这项工作中，我们首次对填充 Token 在 T2I 模型中的作用进行了深入分析。我们开发了两种因果推断技术，用于分析信息是如何在 T2I 流程的不同组件中的 Token 表示中编码的。利用这些技术，我们研究了填充 Token 在何时以及如何影响图像生成过程。我们的研究结果揭示了三种不同的场景：填充 Token 可能在文本编码期间、扩散过程中影响模型的输出，或者被有效忽略。此外，我们确定了这些场景与模型架构（交叉注意力或自注意力）及其训练过程（冻结或训练的文本编码器）之间的关键关系。这些发现有助于更深入地理解填充 Token 的机制，可能为未来 T2I 系统的模型设计和训练实践提供参考。

## Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks
[Omni-RGPT: 通过 Token Marks 统一图像和视频的区域级理解](https://arxiv.org/abs/2501.08326)

我们提出了 Omni-RGPT，一个多模态大语言模型，旨在实现图像和视频的区域级理解。为了实现跨空间和时间维度的一致区域表示，我们引入了 Token Mark，这是一组在视觉特征空间中标记目标区域的 Token。这些 Token 通过区域提示信息（例如，框或掩码）直接嵌入到空间区域中，并同时融入文本提示中以指定目标，从而建立视觉和文本 Token 之间的直接连接。为了进一步支持无需轨迹的鲁棒视频理解，我们引入了一个辅助任务，通过利用 Token 的一致性来指导 Token Mark，从而确保视频中的稳定区域解释。此外，我们构建了一个大规模的区域级视频指令数据集（RegVID-300k）。Omni-RGPT 在基于图像和视频的常识推理基准上取得了最先进的结果，同时在字幕生成和指代表达理解任务中表现出色。

## Diffusion Adversarial Post-Training for One-Step Video Generation
[用于一步视频生成的扩散对抗后训练](https://arxiv.org/abs/2501.08316)

扩散模型广泛用于图像和视频生成，但其迭代生成过程缓慢且计算成本高。虽然现有的蒸馏方法已经展示了在图像领域实现一步生成的潜力，但它们仍然存在显著的质量损失。在这项工作中，我们提出了在扩散预训练后进行对抗后训练（APT）以实现一步视频生成。为了提高训练稳定性和质量，我们引入了对模型架构和训练过程的多项改进，以及一个近似的 R1 正则化目标。实验表明，我们的对抗后训练模型 Seaweed-APT 可以在单次前向传播步骤中实时生成 2 秒、1280x720、24fps 的视频。此外，我们的模型能够一步生成 1024px 的图像，其质量可与最先进的方法相媲美。

## Towards Best Practices for Open Datasets for LLM Training
[大语言模型训练的最佳开放数据集实践](https://arxiv.org/abs/2501.08365)

许多 AI 公司在未经版权所有者许可的情况下，使用数据训练他们的大语言模型 (LLMs)。这种做法的合法性因地区而异：在欧盟和日本等国家，这在某些限制下是允许的，而在美国，法律环境则更为模糊。无论法律地位如何，创意生产者的担忧已经引发了几起备受关注的版权诉讼，诉讼威胁通常被认为是公司和公共利益相关者减少训练数据集信息共享的原因。这种限制数据信息的趋势阻碍了透明度、问责制和更广泛生态系统中的创新，使研究人员、审计员和受影响的个人难以获取理解 AI 模型所需的信息。

虽然通过在开放获取和公共领域数据上训练语言模型可以缓解这一问题，但在撰写本文时，由于在构建必要语料库方面存在重大的技术和社会学挑战，目前还没有这样的模型（以有意义的规模训练）。这些挑战包括不完整和不可靠的元数据、数字化物理记录的成本和复杂性，以及确保在快速变化的环境中相关性和责任所需的各种法律和技术技能。构建一个未来，使 AI 系统可以在负责任地管理和治理的开放许可数据上进行训练，需要跨法律、技术和政策领域的合作，以及对元数据标准、数字化和培养开放文化的投资。

## MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents
[MMDocIR: 长文档多模态检索的基准评估](https://arxiv.org/abs/2501.08828)

多模态文档检索旨在识别和检索多模态内容，如图表、表格、图表和布局信息。尽管其重要性显著，但目前缺乏一个强有力的基准来有效评估多模态文档检索系统的性能。为此，本文引入了一个名为MMDocIR的新基准，涵盖两个不同的任务：页面级和布局级检索。前者旨在在长文档中定位最相关的页面，而后者则针对特定布局的检测，提供比整页分析更细粒度的分析。布局可以指各种元素，如文本段落、公式、图表、表格或图表。MMDocIR基准包含一个丰富的数据集，包含1,685个问题的专家注释标签和173,843个问题的引导标签，成为推进多模态文档检索训练和评估的关键资源。通过实验，我们发现：(i) 视觉检索器显著优于文本检索器，(ii) MMDocIR训练集可以有效促进多模态文档检索的训练过程，(iii) 利用VLM-text的文本检索器比使用OCR-text的文本检索器表现更好。这些发现表明整合视觉元素在多模态文档检索中的潜在优势。

## Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps
[Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps](https://arxiv.org/abs/2501.09732)

生成模型在多个领域产生了深远影响，这主要得益于它们在训练过程中通过增加数据、计算资源和模型规模进行扩展的能力，这种现象通常被称为扩展定律。最近的研究开始探索大语言模型（LLMs）在推理时的扩展特性，揭示了在推理过程中通过增加计算量可以进一步提升性能。与LLMs不同，扩散模型本身具备通过调整去噪步数来灵活控制推理时计算的能力，尽管性能提升通常在几十步后趋于平缓。在本研究中，我们探索了扩散模型在增加去噪步数之外的推理时扩展行为，并研究了如何通过增加计算量进一步提升生成性能。具体而言，我们提出了一个搜索问题，旨在为扩散采样过程找到更好的噪声。我们围绕两个维度构建了设计空间：用于提供反馈的验证器，以及用于寻找更好噪声候选的算法。通过对类条件和文本条件图像生成基准的广泛实验，我们的研究结果表明，增加推理时计算量可以显著提升扩散模型生成样本的质量。此外，由于图像的复杂性，我们可以根据不同的应用场景，特别选择框架中的组件组合。

## OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking
[OmniThink: 通过思考扩展机器写作中的知识边界](https://arxiv.org/abs/2501.09751)

使用大语言模型进行机器写作通常依赖于基于检索的增强生成。然而，这些方法仍然受限于模型预定义的范围之内，限制了生成具有丰富信息的内容。具体来说，常规的检索信息往往缺乏深度和实用性，并且存在冗余问题，这对生成文章的质量产生了负面影响，导致文章内容浅薄、重复且缺乏原创性。为了解决这些问题，我们提出了 OmniThink，一种模拟人类迭代扩展与反思过程的机器写作框架。OmniThink 的核心思想是模拟学习者在逐步加深对主题理解时的认知行为。实验结果表明，OmniThink 在不影响连贯性、深度等指标的情况下，提高了生成文章的知识密度。人类评估和专家反馈进一步证明了 OmniThink 在生成长篇文章中应对现实世界挑战的潜力。

## Learnings from Scaling Visual Tokenizers for Reconstruction and Generation
[视觉分词器扩展对重建与生成的影响研究](https://arxiv.org/abs/2501.09755)

通过自编码实现的视觉分词技术，能够将像素压缩到潜在空间中，从而赋能最先进的图像和视频生成模型。尽管基于 Transformer 的生成器扩展是近期进展的核心，但分词器组件本身却很少被扩展，这导致自编码器设计选择如何影响其重建目标及下游生成性能的问题尚未得到充分解答。本文旨在探索自编码器扩展的影响，以填补这一空白。为此，我们采用增强的 Vision Transformer 架构（ViTok）替代了传统的卷积主干，并在远超 ImageNet-1K 的大规模图像和视频数据集上训练 ViTok，从而消除了分词器扩展的数据限制。我们首先研究了自编码器瓶颈扩展对重建和生成的影响，发现其与重建高度相关，但与生成的关系更为复杂。接着，我们分别探索了自编码器的编码器和解码器扩展对重建和生成性能的影响。关键发现是，编码器扩展对重建或生成的提升微乎其微，而解码器扩展则显著提升了重建效果，但对生成的影响则因情况而异。基于这些发现，我们设计了 ViTok 这一轻量级自编码器，其在 ImageNet-1K 和 COCO 重建任务（256p 和 512p）中表现优异，与最先进的自编码器相当，同时在 UCF-101 的 16 帧 128p 视频重建任务上超越了现有自编码器，且 FLOPs 减少了 2-5 倍。当与 Diffusion Transformers 结合时，ViTok 在 ImageNet-1K 的图像生成任务中表现出色，并在 UCF-101 的类条件视频生成任务中创下了新的最先进基准。

