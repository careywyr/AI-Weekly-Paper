## MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention
[MiniMax-M1：基于闪电注意力的高效推理计算扩展方案](https://arxiv.org/abs/2506.13585)

我们正式发布MiniMax-M1，这是全球首个开源权重的大规模混合注意力推理模型。该模型采用混合专家(Mixture-of-Experts, MoE)架构与闪电注意力机制相结合的创新设计。M1基于我们此前研发的MiniMax-Text-01模型构建，后者总参数量达4560亿，其中每个token激活45.9亿参数。该模型原生支持100万token的上下文窗口，是DeepSeek R1上下文长度的8倍。其闪电注意力机制能显著提升推理计算效率，使M1特别适合需要处理长序列输入和深度推理的复杂任务。训练方面，我们通过在包含沙盒环境和真实软件工程场景的多样化问题上进行大规模强化学习(RL)来训练模型。除架构本身的RL训练效率优势外，我们还提出了新型RL算法CISPO，该算法通过裁剪重要性采样权重(而非token更新)来提升训练效率，其性能优于其他主流RL算法变体。结合混合注意力架构与CISPO算法，MiniMax-M1在512块H800 GPU上仅用三周就完成了完整RL训练，计算成本仅为53.47万美元。我们发布了40K和80K两种thinking budget版本的模型，其中40K版本是80K训练过程的中间产物。基准测试表明，我们的模型性能与DeepSeek-R1、Qwen3-235B等顶尖开源模型相当甚至更优，尤其在复杂软件工程、工具调用和长上下文处理等任务中表现突出。模型已开源发布：https://github.com/MiniMax-AI/MiniMax-M1。

## MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation
[MultiFinBen：面向金融大语言模型评估的多语言、多模态及难度感知基准](https://arxiv.org/abs/2506.14028)

大语言模型（LLMs）的最新发展推动了金融自然语言处理（NLP）及相关应用的进步。然而现有评估基准仍局限于单语言和单模态场景，且过于依赖简单任务，难以反映真实金融场景的复杂性。本文提出MultiFinBen基准，这是首个专为全球金融领域设计的多语言多模态评估体系，涵盖文本、视觉和音频三种模态，支持单语言、双语及多语言环境下的领域特定任务评估。我们创新性地引入两项任务：PolyFiQA-Easy和PolyFiQA-Expert是首个要求模型处理混合语言输入并进行复杂推理的多语言金融基准；EnglishOCR和SpanishOCR则是首个集成OCR技术的金融问答任务，要求模型从视觉文本型金融文档中提取信息并完成推理。此外，我们设计了动态难度感知的选择机制，构建了紧凑平衡的评估体系，而非简单堆砌现有数据集。通过对22个前沿模型的全面测试发现，即使是最先进的模型，虽然具备通用多模态和多语言能力，在处理金融领域跨语言多模态任务时仍存在显著不足。MultiFinBen已开源发布，旨在推动金融研究和应用向透明化、可复现和包容性方向发展。

## Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning
[科学家的首次考试：通过感知、理解与推理评估 MLLM 的认知能力](https://arxiv.org/abs/2506.10521)

现代科学发现日益依赖于信息密集型科学数据与领域专业知识相结合的复杂多模态推理。基于专家级科学基准构建的科学多模态大语言模型 (Multimodal Large Language Models, MLLMs) 有望在实际科研流程中显著提升这一发现过程。然而，现有科学基准主要针对 MLLM 的知识理解能力进行评估，难以全面衡量其感知与推理能力。为此，我们提出科学家的首次考试 (Scientists' First Exam, SFE) 基准，通过三个相互关联的维度系统评估 MLLM 的科学认知能力：科学信号感知、科学属性理解以及科学比较推理。具体而言，SFE 包含 830 组经专家验证的视觉问答 (VQA) 数据对，涵盖三种问题类型，涉及五个重点学科的 66 个多模态任务。实验结果表明，当前最先进的 GPT-o3 和 InternVL-3 模型在 SFE 上的得分仅为 34.08% 和 26.52%，这表明 MLLM 在科学领域仍有显著提升空间。我们期望 SFE 的研究发现能为 AI 驱动的科学发现提供新的发展思路。

## DeepResearch Bench: A Comprehensive Evaluation Benchmark for Deep Research Agents
[DeepResearch Bench：面向深度研究智能体的综合评测基准](https://arxiv.org/abs/2506.11763)

深度研究智能体是基于大语言模型(LLM)的重要智能体类型。通过自主协调多步骤网络探索、定向检索和高级信息整合，这类智能体能够将海量网络信息转化为具备专业分析水准且引用详实的研究报告，将传统人工案头研究所需的数小时工作量缩减至数分钟。然而，目前尚缺乏系统性评估此类智能体能力的综合基准。为此，我们提出DeepResearch Bench基准，包含100个博士水平的研究任务，每个任务均由22个不同学科领域的专家精心设计。由于深度研究智能体的评估本身具有复杂性和高人力消耗特性，我们提出了两种与专家评估结果高度吻合的创新方法：其一是基于参考报告的自适应标准评估框架，用于量化生成研究报告的质量；其二是通过有效引用数和引用准确率来评估智能体信息检索与收集能力的新框架。我们已在https://github.com/Ayanami0730/deep_research_bench开源DeepResearch Bench及核心评估框架，以促进实用型大语言模型智能体的研发。

## Scaling Test-time Compute for LLM Agents
[大语言模型智能体的测试阶段计算资源扩展](https://arxiv.org/abs/2506.12928)

扩展测试阶段计算资源在提升大语言模型(LLMs)推理能力方面已取得显著成效。本研究首次系统探索了测试阶段扩展方法在语言智能体中的应用，并量化评估了其性能提升效果。具体而言，我们研究了以下扩展策略：(1) 并行采样算法；(2) 序列化修正策略；(3) 验证与结果融合方法；(4) 多样化探索路径策略。通过系统分析与消融实验，我们得出以下结论：1. 扩展测试阶段计算资源可有效提升智能体性能；2. 智能体需要准确掌握反思时机；3. 在各类验证与结果融合方法中，列表式(list-wise)方法表现最优；4. 增加多样化探索路径对智能体任务表现具有显著提升效果。

## Sekai: A Video Dataset towards World Exploration
[Sekai：面向世界探索的视频数据集](https://arxiv.org/abs/2506.15675)

视频生成技术已取得显著进展，有望为交互式世界探索奠定基础。然而现有视频生成数据集存在若干局限性：地理覆盖有限、时长较短、场景静态化，且缺乏探索行为与世界属性的标注。本文提出Sekai（日语"世界"之意），这是一个标注丰富的高质量第一人称视角全球视频数据集，专为世界探索任务设计。数据集包含来自750个城市、100多个国家和地区的5,000余小时步行/无人机视角视频（第一人称视角FPV和无人机航拍UVA）。我们开发了高效的数据处理工具箱，可完成视频采集、预处理及多维度标注（包括地理位置、场景类型、天气状况、人群密度、文字描述和相机轨迹）。实验验证了数据集的质量优势。基于数据子集，我们训练了名为YUME（日语"梦想"之意）的交互式视频世界探索模型。相信Sekai数据集将推动视频生成与世界探索领域的发展，并促进相关应用开发。

## Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback
[Feedback Friction：大语言模型难以完全吸收外部反馈](https://arxiv.org/abs/2506.11930)

最新研究表明，大语言模型 (LLM) 在获得外部反馈时具备改进其输出的能力。然而，这些模型对外部反馈的吸收效率和完整程度仍不明确。在理想情况下，当大语言模型获得近乎完美的完整反馈时，理论上应能完全吸收反馈并将错误答案修正为正确答案。本文通过设计受控实验环境，系统性地研究了大语言模型吸收反馈的能力。实验流程为：针对每个问题，求解模型 (solver model) 首先生成解答，随后由可获取近乎完整标准答案的反馈生成器 (feedback generator) 生成针对性反馈，最后求解模型再次尝试解答。我们在数学推理、知识推理、科学推理等多个领域进行评估，测试模型包括Claude 3.7（包含扩展思考与不包含扩展思考两种模式）等前沿语言模型。值得注意的是，即使在这种近乎理想的实验条件下，求解模型仍持续表现出对反馈的吸收障碍，我们将这种现象定义为"反馈摩擦"。为缓解该问题，我们尝试了基于采样 (sampling-based) 的改进策略，包括渐进式温度提升和显式拒绝先前错误答案等方法，这些策略虽带来一定提升，但仍未能使模型达到预期性能水平。此外，我们通过严格实验排除了模型过度自信和数据熟悉度等潜在影响因素。我们期望通过揭示大语言模型这一特性并排除若干次要因素，能为未来的模型自我改进研究提供参考。

## CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction Following
[CMI-Bench：音乐指令跟随综合评估基准](https://arxiv.org/abs/2506.12285)

音频-文本大语言模型（LLMs）的最新进展为音乐理解与生成领域带来了新的机遇。然而现有基准测试普遍存在范围局限，多采用简化任务或多选评估方式，难以反映真实场景中音乐分析的复杂性。本研究将传统音乐信息检索（MIR）的各类标注数据重构为指令跟随格式，提出CMI-Bench这一综合性音乐指令跟随评估基准，用于系统评估音频-文本LLMs在多样化MIR任务中的表现。涵盖任务包括：流派分类、情感回归、情感标注、乐器分类、音高估计、调性检测、歌词转录、旋律提取、演唱技法识别、演奏技法检测、音乐标注、音乐描述以及节拍跟踪等MIR核心研究课题。与以往基准不同，CMI-Bench采用与当前最优MIR模型一致的标准化评估指标，确保结果与监督学习方法具有直接可比性。我们提供的评估工具包支持所有开源音频-文本LLMs（包括LTU、Qwen-audio、SALMONN、MusiLingo等）。实验结果表明，LLMs与监督模型存在显著性能差距，同时暴露出模型在文化、时代和性别方面的偏差，揭示了当前模型处理MIR任务的优势与不足。CMI-Bench为音乐指令跟随能力评估建立了统一标准，将有力促进具备音乐理解能力的LLMs发展。

