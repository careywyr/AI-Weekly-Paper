## GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models
[GLM-4.5: 智能体、推理与编码（ARC）基础模型](https://arxiv.org/abs/2508.06471)

我们推出 GLM-4.5，这是一个开源的混合专家（MoE）大语言模型，总参数量达 3550 亿，激活参数量为 320 亿，其特色是支持思维模式与直接响应模式的混合推理方法。通过对 23 万亿 Token 的多阶段训练，以及结合专家模型迭代和强化学习的全面后训练，GLM-4.5 在智能体、推理和编码（ARC）任务中表现优异：TAU-Bench 得分 70.1%、AIME 24 得分 91.0%、SWE-bench Verified 得分 64.2%。该模型参数量显著少于多数竞品，在所有评估模型中综合排名第三，智能体基准测试排名第二。我们同时发布了 GLM-4.5（3550 亿参数）及其轻量版 GLM-4.5-Air（1060 亿参数），以推动推理与智能体 AI 系统的研究。代码、模型及更多信息详见 https://github.com/zai-org/GLM-4.5。

## We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning
[We-Math 2.0: 促进视觉化数学推理的多功能数学书系统](https://arxiv.org/abs/2508.10433)

多模态大语言模型 (MLLM) 在各种任务中展现出卓越能力，但在复杂数学推理方面仍存在不足。现有研究主要聚焦于数据集构建与方法优化，通常忽略两个关键因素：全面的知识驱动设计和模型中心的数据空间建模。本文提出 We-Math 2.0，该系统整合了结构化数学知识体系、模型中心的数据空间建模以及基于强化学习 (RL) 的训练范式，全面提升 MLLM 的数学推理能力。We-Math 2.0 的核心贡献包含四个方面：(1) MathBook 知识系统：构建包含 491 个知识点和 1,819 条基本原理的五级层次体系。(2) MathBook-Standard & Pro：开发 MathBook-Standard 数据集，通过双重扩展实现广泛概念覆盖与灵活性；同时定义三维难度空间并为每个问题生成 7 个渐进变体，构建面向鲁棒训练的挑战性数据集 MathBook-Pro。(3) MathBook-RL：提出两阶段 RL 框架，包括：(i) 冷启动微调阶段，使模型与知识导向的思维链推理对齐；(ii) 渐进对齐 RL 阶段，通过平均奖励学习与动态数据调度实现跨难度渐进对齐。(4) MathBookEval：建立涵盖全部 491 个知识点且具有多样化推理步骤分布的综合评估基准。实验表明，MathBook-RL 在四个常用基准测试中性能与现有基线相当，在 MathBookEval 上表现优异，展现出良好的数学推理泛化能力。

## NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale
[NextStep-1：基于连续 Token 的大规模自回归图像生成研究](https://arxiv.org/abs/2508.10711)

当前主流的文生图自回归 (AR) 模型存在两种实现路径：依赖计算量大且资源密集的扩散模型处理连续图像 Token，或采用向量量化 (VQ) 获取存在量化损失的离散 Token。本研究通过 NextStep-1 推进了自回归范式的发展，该模型采用 14B 参数的自回归主干网络配合 157M 参数的流匹配头，基于下一 Token 预测目标，实现了对离散文本 Token 和连续图像 Token 的联合训练。NextStep-1 在文生图任务中达到了自回归模型的当前最优性能，展现出卓越的高保真图像生成能力。此外，该方法在图像编辑任务中同样表现突出，充分证明了其统一框架的强大能力与广泛适用性。为推动开放研究，我们将向社区开源相关代码和模型。

## ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability
[ReasonRank：具备强推理能力的段落排序方法](https://arxiv.org/abs/2508.07050)

基于大语言模型（LLM）的列表式排序已在众多段落排序任务中展现出卓越性能。随着大型推理模型的发展，多项研究表明，推理过程中的分步处理能有效提升列表式排序性能。然而，由于缺乏推理密集型训练数据，现有重排序器在复杂排序场景中表现欠佳，且推理密集型重排序器的排序能力尚未充分开发。本文首先提出自动化推理密集型训练数据合成框架，该框架从多领域采集训练查询与段落，并采用DeepSeek-R1生成高质量训练标签。通过自一致性数据过滤机制确保数据质量。为增强列表式重排序器的推理能力，我们进一步提出两阶段后训练策略：第一阶段为冷启动监督微调（SFT）用于学习推理模式，第二阶段通过强化学习（RL）进一步提升排序能力。在RL阶段，基于列表式排序特性设计了多维度排序奖励函数，其效果优于基于排序指标的奖励函数。大量实验表明，我们训练的推理密集型重排序器\textbf{ReasonRank}显著超越现有基线模型，且相比逐点重排序器Rank1延迟显著降低。\textbf{进一步实验证明，ReasonRank在BRIGHT排行榜\footnote{https://brightbenchmark.github.io/}上取得40.6分的最先进（SOTA）性能。}代码已开源：https://github.com/8421BCD/ReasonRank。

## WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent 
[WebWatcher：视觉-语言深度研究智能体的开创性突破](https://arxiv.org/abs/2508.05748)  

Deep Research 等网络智能体已展现出超越人类水平的认知能力，能够解决高难度的信息获取问题。然而现有研究多集中于纯文本领域，忽略了现实场景中的视觉信息。这使得多模态 Deep Research 面临巨大挑战，因为此类智能体需要比文本型智能体具备更强大的感知、逻辑、知识推理能力，并能操作更复杂的工具。为解决这一局限，我们提出 WebWatcher——一种具备增强型视觉-语言推理能力的多模态深度研究智能体。该系统采用高质量合成多模态数据轨迹 (synthetic multimodal trajectories) 实现高效冷启动训练 (cold start training)，通过多样化工具链进行深度推理，并利用强化学习进一步提升泛化性能。为系统评估多模态智能体能力，我们构建了 BrowseComp-VL 基准测试，该测试采用 BrowseComp 范式 (BrowseComp-style)，要求智能体完成同时涉及视觉与文本信息的复杂信息获取任务。实验表明，在四项高难度 VQA (视觉问答) 基准测试中，WebWatcher 性能显著优于商业闭源基线系统、RAG (检索增强生成) 工作流及开源智能体，为复杂多模态信息获取任务提供了新的解决方案。

## WideSearch: Benchmarking Agentic Broad Info-Seeking
[WideSearch：智能体广域信息检索基准测试](https://arxiv.org/abs/2508.07999)

从专业研究到日常规划，许多任务都受制于大规模信息检索，这类工作重复性高但认知复杂度低。随着大语言模型（LLMs）的快速发展，由LLMs驱动的自动化搜索智能体为解放人类生产力提供了可行方案。然而，由于缺乏合适的评估基准，这些智能体执行"宽上下文(wide-context)"信息收集的可靠性和完整性仍缺乏系统评估。为此，我们提出WideSearch基准，专门用于评估智能体在大规模信息收集任务中的可靠性。该基准包含200个经过人工筛选的问题（中英文各100个），覆盖15个以上不同领域，所有问题均源自真实用户查询。每个任务要求智能体收集可逐个验证的原子信息(atomic information)，并将其组织为结构化输出。通过严格设计的五阶段质量控制管道(pipeline)，我们确保了数据集的难度、完整性和可验证性。我们对10余个前沿搜索智能体系统进行了测试，包括单智能体框架、多智能体架构以及端到端商业系统。测试结果显示，多数系统总体成功率接近0%，最优系统仅达到5%。但经多位人类测试者充分时间交叉验证后，成功率可接近100%。这些结果表明当前搜索智能体在大规模信息检索方面存在显著缺陷，凸显了该领域亟待加强的研究方向。我们的数据集、评估管道和基准结果已开源发布：https://widesearch-seed.github.io/

## A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems  
[自进化 AI 智能体综述：基础模型与终身智能体系统融合的新范式](https://arxiv.org/abs/2508.07407)  

大语言模型的最新进展推动了对复杂现实任务求解 AI 智能体的研究热潮。然而，现有智能体系统多依赖人工设计的配置，这些配置在部署后固定不变，难以适应动态变化的环境。为此，研究者开始探索基于交互数据和环境反馈实现智能体系统自动增强的进化技术。这一新兴研究方向为自进化 AI 智能体奠定了基础，通过融合基础模型的静态能力与终身智能体系统所需的持续适应性，开创了新的技术范式。  

本文系统梳理了自进化智能体系统的关键技术。首先提出统一概念框架，抽象出自进化智能体系统设计的反馈环路。该框架包含四个核心组件：系统输入、智能体系统、环境和优化器，为理解与比较不同技术方案提供基础。基于此框架，我们分类综述了针对智能体系统各组件的自进化技术，并重点分析了生物医学、编程和金融等领域的专用进化策略，这些策略的优化目标与领域约束存在强关联。此外，本文专门探讨了自进化智能体系统的评估方法、安全机制与伦理问题，这些因素对确保系统可靠运行至关重要。  

本综述旨在帮助研究者和开发者系统掌握自进化 AI 智能体的技术脉络，为构建更具适应性、自主性和持续学习能力的终身智能体系统提供理论支撑。

## Matrix-3D: Omnidirectional Explorable 3D World Generation
[Matrix-3D：全向可探索的3D世界生成](https://arxiv.org/abs/2508.08086)

基于单张图像或文本提示生成可探索的3D世界是空间智能的核心基础。近期研究利用视频模型实现了大范围且泛化性强的3D世界生成。然而，现有方法普遍存在生成场景范围受限的问题。本文提出Matrix-3D框架，该框架通过全景表示实现广覆盖的全向可探索3D世界生成，结合了条件视频生成与全景3D重建技术。我们首先训练了一个轨迹引导的全景视频扩散模型，该模型以场景网格渲染为条件，能够生成高质量且几何一致的场景视频。为将全景场景视频转换为3D世界，我们提出了两种独立方法：(1) 前馈型大型全景重建模型，用于快速3D场景重建；(2) 基于优化的方法，用于实现精确且详细的3D场景重建。为支持有效训练，我们还构建了Matrix-Pano数据集，这是首个包含11.6万条带深度和轨迹标注的高质量静态全景视频序列的大规模合成数据集。实验结果表明，所提框架在全景视频生成和3D世界生成方面均达到了最先进性能。更多内容请访问https://matrix-3d.github.io。

## Story2Board: A Training-Free Approach for Expressive Storyboard Generation
[Story2Board：免训练的富有表现力故事板生成方法](https://arxiv.org/abs/2508.09983)

我们提出Story2Board——一个免训练框架，能够根据自然语言生成富有表现力的故事板。现有方法过度聚焦于主体身份，忽略了空间构图、背景演变和叙事节奏等视觉叙事关键要素。为此，我们设计了一个轻量级一致性框架，包含两个核心组件：潜在面板锚定（Latent Panel Anchoring）用于跨面板保持共享角色参照，互惠注意力混合（Reciprocal Attention Value Mixing）则在具有强互惠注意力的token对之间实现视觉特征的软融合。这些机制协同工作，在不改变架构或微调的情况下增强连贯性，使前沿扩散模型能够生成视觉多样且一致的故事板。

为结构化生成过程，我们采用现成语言模型将自由文本故事转换为面板级 grounded prompts。评估方面，我们提出Rich Storyboard Benchmark（RSB）——一套开放域叙事测试集，除一致性外还能评估布局多样性和背景驱动的叙事能力。同时引入新的场景多样性指标，量化故事板间的空间与姿态变化。定性与定量实验及用户研究表明，相比现有基线，Story2Board能生成更具动态性、连贯性和叙事吸引力的故事板。

## Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation
[Omni-Effects: 统一且空间可控的视觉特效生成](https://arxiv.org/abs/2508.07981)

视觉特效 (Visual Effects, VFX) 是现代影视制作的核心视觉增强技术。虽然视频生成模型为 VFX 制作提供了高效解决方案，但现有方法受限于单特效 LoRA 训练范式，仅支持单一特效生成。这一局限性严重制约了需要空间可控复合特效的应用场景——在指定区域同步生成多种特效。然而，构建统一的多特效框架存在两大核心挑战：特效变体间的相互干扰，以及多 VFX 联合训练时的空间控制缺失。为此，我们提出首个支持提示引导生成与空间可控复合特效的统一框架 Omni-Effects，其核心创新包括：(1) 基于 LoRA 的专家混合模型 (LoRA-MoE)，通过专家 LoRA 集群在统一模型中集成多样化特效，有效抑制任务间干扰；(2) 空间感知提示 (Spatial-Aware Prompt, SAP) 将空间掩码信息编码至文本 token，实现像素级空间控制。此外，SAP 中集成的独立信息流 (Independent-Information Flow, IIF) 模块可隔离各特效控制信号，防止特征混淆。为支持研究，我们通过融合图像编辑与首尾帧视频合成 (First-Last Frame-to-Video, FLF2V) 的新型数据采集流程，构建了专业级 VFX 数据集 Omni-VFX，并开发了专项评估框架验证模型性能。大量实验证明，Omni-Effects 能实现精准的空间控制与多样化特效生成，支持用户同步指定特效类型与作用区域。

## Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off
[Voost：面向双向虚拟试穿与反向试穿的统一可扩展扩散Transformer](https://arxiv.org/abs/2508.04825)

虚拟试穿技术旨在生成人物穿着目标服装的真实图像，但精确建模服装与人体间的对应关系仍是核心难题，尤其在处理姿态和外观变化时更为显著。本文提出Voost——基于单一扩散Transformer (Diffusion Transformer) 的统一可扩展框架，可同时学习虚拟试穿及其逆向过程（反向试穿）。通过联合建模这两个任务，Voost实现了服装-人物配对数据的双向监督，并支持生成方向与服装类别的条件生成 (conditional generation)，无需特定任务网络、辅助损失函数或额外标注即可增强服装-人体关系推理能力。此外，我们提出了两项推理阶段技术：用于提升分辨率及掩模变化鲁棒性的注意力温度缩放 (attention temperature scaling)，以及利用任务间双向一致性的自校正采样 (self-corrective sampling)。大量实验表明，Voost在试穿与反向试穿基准测试中均达到当前最优性能，在对齐精度、视觉保真度和泛化能力方面显著优于现有基线方法。

