## EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation
[EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation](https://arxiv.org/abs/2501.01895)

我们介绍了 EnerVerse，一个专门为机器人操作任务设计的具身未来空间生成的综合框架。EnerVerse 无缝结合了卷积和双向注意力机制，用于内部块级空间建模，确保低级别的一致性和连续性。认识到视频数据中固有的冗余性，我们提出了一种稀疏内存上下文结合分块单向生成范式，以实现无限长序列的生成。为了进一步增强机器人能力，我们引入了自由视角 (Free Anchor View, FAV) 空间，它提供了灵活的视角以增强观察和分析。FAV 空间减轻了运动建模的不确定性，消除了受限环境中的物理约束，并显著提高了机器人在各种任务和设置中的泛化能力和适应性。为了解决获取多摄像头观测的高成本和劳动强度问题，我们提出了一个数据引擎流水线，该流水线将生成模型与 4D 高斯泼溅 (4D Gaussian Splatting, 4DGS) 集成在一起。该流水线利用生成模型的强大泛化能力和 4DGS 提供的空间约束，实现了数据质量和多样性的迭代增强，从而创建了一个有效缩小模拟到现实差距的正反馈效应。最后，我们的实验表明，具身未来空间生成先验显著提升了策略预测能力，从而提高了整体性能，特别是在长距离机器人操作任务中。

## VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction
[VITA-1.5: 迈向 GPT-4o 级别的实时视觉与语音交互](https://arxiv.org/abs/2501.01957)

最近的多模态大语言模型 (MLLMs) 通常专注于整合视觉和文本模态，而对语音在增强交互中的作用关注较少。然而，语音在多模态对话系统中扮演着关键角色，并且由于模态之间的根本差异，实现视觉和语音任务的高性能仍然是一个重大挑战。在本文中，我们提出了一种精心设计的多阶段训练方法，逐步训练大语言模型以理解视觉和语音信息，最终实现流畅的视觉和语音交互。我们的方法不仅保留了强大的视觉-语言能力，还实现了高效的语音到语音对话能力，而无需单独的 ASR 和 TTS 模块，显著加速了多模态端到端的响应速度。通过在图像、视频和语音任务的基准测试中与最先进的模型进行比较，我们证明了我们的模型具备强大的视觉和语音能力，能够实现接近实时的视觉和语音交互。

## Virgo: A Preliminary Exploration on Reproducing o1-like MLLM
[Virgo: 关于复现 o1 类多模态大语言模型的初步探索](https://arxiv.org/abs/2501.01904)

最近，基于大语言模型 (LLMs) 的慢速推理系统通过延长推理过程中的思考时间而获得了广泛关注。人们也越来越有兴趣将这种能力应用于多模态大语言模型 (MLLMs)。鉴于 MLLMs 处理不同模态的更复杂的语义数据，从直观上看，实现多模态慢速推理系统更具挑战性。
  为了解决这个问题，在本文中，我们探索了一种直接的方法，通过使用少量长格式文本思考数据微调一个强大的 MLLM，从而得到一个多模态慢速推理系统，Virgo（具有长思考的视觉推理）。我们发现，这些以自然语言表达的长格式推理可以有效地转移到 MLLMs 中。此外，看起来这种文本推理数据在激发 MLLMs 的慢速推理能力方面甚至比视觉推理数据更有效。虽然这项工作尚处于早期阶段，但它表明慢速推理能力从根本上与语言模型组件相关联，可以跨模态或领域转移。这一发现可用于指导开发更强大的慢速推理系统。我们在 https://github.com/RUCAIBox/Virgo 发布了我们的资源。

## STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution
[STAR: 基于文本到视频模型的空间-时间增强用于现实世界视频超分辨率](https://arxiv.org/abs/2501.02976)

图像扩散模型已被用于现实世界视频超分辨率，用于解决基于 GAN 的方法中的过度平滑问题。然而，这些模型难以保持时间一致性，由于它们是在静态图像上训练的，限制了它们有效捕捉时间动态的能力。将文本到视频 (T2V) 模型集成到视频超分辨率中以改进时间建模是直观的。然而，仍然存在两个关键挑战：现实场景中复杂退化导致的伪影，以及由于强大的 T2V 模型（例如 CogVideoX-5B）的强大生成能力导致的保真度下降。为了增强恢复视频的时空质量，我们引入了~\name
（基于 T2V 模型的空间-时间增强用于现实世界视频超分辨率），这是一种利用 T2V 模型用于现实世界视频超分辨率的新方法，实现了逼真的空间细节和强大的时间一致性。具体来说，我们在全局注意力块之前提出了一个局部信息增强模块 (LIEM)，用于丰富局部细节并减轻退化伪影。此外，我们提出了一种动态频率 (DF) 损失来增强保真度，引导模型在扩散过程中关注不同的频率分量。广泛的实验表明，~\name~在合成和现实世界数据集上均优于最先进的方法。

## BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning
[BoostStep: 通过改进单步推理提升大语言模型的数学能力](https://arxiv.org/abs/2501.03226)

最先进的大语言模型 (LLMs) 在解决复杂数学问题时展现出良好的性能，通过分而治之的策略和上下文学习 (ICL) 示例的辅助。然而，它们的性能提升空间受到 ICL 示例中两个关键问题的限制：粒度不匹配和随之而来的负面效应噪声问题。具体来说，LLMs 能够进行分解过程，但在少数征服步骤中的推理不准确，而检索到的基于问题粒度的 ICL 示例有时缺乏特定挑战性推理步骤的相关步骤。此外，这种不匹配可能由于其不相关性而阻碍正确的推理。为了解决这些问题，我们专注于提高每个步骤中的推理质量，并提出了 BoostStep。BoostStep 在步骤粒度上匹配检索和推理的粒度，并通过一种新颖的“首次尝试策略”为每个推理步骤提供高度相关的 ICL 示例。BoostStep 提供了比基于问题粒度的粗粒度策略更相关的示例，显著提高了模型在每个步骤中的推理质量。BoostStep 是一种通用且鲁棒的推理增强方法，不仅提高了独立推理性能，还与蒙特卡洛树搜索方法 (MCTS) 无缝结合，以改进候选生成和决策过程。在定量评估中，它在各种数学基准上将 GPT-4o 和 Qwen2.5-Math-72B 分别提高了 3.6\% 和 2.0\%，与 MCTS 结合使用时提高了 7.5\%。

## Test-time Computing: from System-1 Thinking to System-2 Thinking
[测试时计算：从系统1思维到系统2思维](https://arxiv.org/abs/2501.02497)

o1 模型在复杂推理任务中的卓越表现表明，测试时计算扩展能够进一步释放模型的潜力，从而实现强大的系统2思维。然而，目前关于测试时计算扩展的全面综述仍然较为缺乏。我们将测试时计算的概念追溯到系统1模型。在系统1模型中，测试时计算通过参数更新、输入修改、表示编辑和输出校准等方式应对分布偏移，从而提升模型的鲁棒性和泛化性。而在系统2模型中，测试时计算则通过重复采样、自校正和树搜索等机制增强模型的推理能力，以解决复杂问题。本文按照从系统1思维到系统2思维的发展趋势组织综述，重点探讨测试时计算在从系统1模型到弱系统2模型，再到强系统2模型的转变过程中所起的关键作用。此外，我们还指出了未来可能的研究方向。

## Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction
[Dispider: 通过解耦感知、决策和反应实现视频大语言模型的主动实时交互](https://arxiv.org/abs/2501.03218)

与视频大语言模型的主动实时交互开创了一种新的人机交互范式，模型不仅理解用户意图，还在实时处理流媒体视频的同时进行响应。与离线视频大语言模型不同，离线模型在回答问题之前需要分析整个视频，而主动实时交互则需要三种能力：1）感知：实时视频监控和交互捕捉。2）决策：在适当的情况下发起主动交互。3）反应：与用户进行持续交互。然而，这些期望的能力之间存在固有的冲突。决策和反应需要不同的感知规模和粒度，而自回归解码在反应过程中阻碍了实时感知和决策。为了在和谐系统中统一这些冲突的能力，我们提出了 Dispider，一个解耦感知、决策和反应的系统。Dispider 具有一个轻量级的主动流媒体视频处理模块，用于跟踪视频流并识别交互的最佳时机。一旦交互被触发，异步交互模块会提供详细的响应，同时处理模块继续监控视频。我们的解耦和异步设计确保了及时、上下文相关且计算高效的响应，使 Dispider 成为长时间视频流主动实时交互的理想解决方案。实验表明，Dispider 不仅在传统视频问答任务中保持了强大的性能，还在流媒体场景响应中显著超越了之前的在线模型，从而验证了我们架构的有效性。代码和模型发布于 https://github.com/Mark12Ding/Dispider。

## REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models
[REINFORCE++: 一种简单高效的大语言模型对齐方法](https://arxiv.org/abs/2501.03262)

基于人类反馈的强化学习 (Reinforcement Learning from Human Feedback, RLHF) 已成为将大语言模型与人类偏好对齐的关键方法，通过诸如近端策略优化 (Proximal Policy Optimization, PPO)、直接偏好优化 (Direct Preference Optimization, DPO)、REINFORCE Leave One-Out (RLOO)、ReMax 和组相对策略优化 (Group Relative Policy Optimization, GRPO) 等方法，见证了算法的快速演进。我们提出了 REINFORCE++，这是经典 REINFORCE 算法的增强版本，它结合了 PPO 的关键优化技术，同时消除了对评论家网络的需求。REINFORCE++ 实现了三个主要目标：(1) 简单性 (2) 增强的训练稳定性，以及 (3) 减少的计算开销。通过广泛的实证评估，我们证明了 REINFORCE++ 相较于 GRPO 表现出更优的稳定性，并且在保持相当性能的同时，比 PPO 具有更高的计算效率。实现代码可在 https://github.com/OpenRLHF/OpenRLHF 获取。

## Cosmos World Foundation Model Platform for Physical AI
[Cosmos 世界基础模型平台：面向物理 AI](https://arxiv.org/abs/2501.03575)

物理 AI 首先需要进行数字化训练，这包括其自身的数字孪生体（策略模型）和世界的数字孪生体（世界模型）。本文介绍了 Cosmos 世界基础模型平台，旨在帮助开发者为其物理 AI 系统构建定制化的世界模型。我们将世界基础模型定义为一种通用世界模型，可通过微调转化为下游应用的定制模型。该平台包含视频管理管道、预训练的世界基础模型、训练后示例以及视频 Tokenizer。为了助力物理 AI 开发者解决社会中的关键问题，我们开源了该平台，并通过 https://github.com/NVIDIA/Cosmos 提供开放权重的模型及宽松的许可证。

## LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token
[LLaVA-Mini：使用单一视觉 Token 的高效图像和视频大模态模型](https://arxiv.org/abs/2501.03895)

随着 GPT-4o 等实时大模态模型 (LMMs) 的出现，人们对高效 LMMs 产生了极大的兴趣。LMM 框架通常将视觉输入编码为视觉 Token（连续表示），并将这些视觉 Token 与文本指令一起集成到大语言模型 (LLMs) 的上下文中。然而，大规模参数和大量上下文 Token（主要是视觉 Token）导致了巨大的计算开销。以往关于高效 LMMs 的研究主要集中在用更小的模型替换 LLM 骨干上，而忽略了 Token 数量的关键问题。在本文中，我们介绍了 LLaVA-Mini，一种具有最少视觉 Token 的高效 LMM。为了在保留视觉信息的同时实现视觉 Token 的高压缩比，我们首先分析了 LMMs 如何理解视觉 Token，发现大多数视觉 Token 仅在 LLM 骨干的早期层中起关键作用，它们主要将视觉信息融合到文本 Token 中。基于这一发现，LLaVA-Mini 引入了模态预融合，提前将视觉信息融合到文本 Token 中，从而促进将输入到 LLM 骨干的视觉 Token 极端压缩为一个 Token。LLaVA-Mini 是一种统一的大模态模型，能够高效地支持图像、高分辨率图像和视频的理解。在 11 个基于图像和 7 个基于视频的基准测试中的实验表明，LLaVA-Mini 仅使用 1 个视觉 Token 而非 576 个，就优于 LLaVA-v1.5。效率分析显示，LLaVA-Mini 可以将 FLOPs 减少 77%，在 40 毫秒内提供低延迟响应，并在 24GB 内存的 GPU 硬件上处理超过 10,000 帧视频。

## MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models
[MotionBench: 基准测试与改进视觉语言模型的细粒度视频运动理解能力](https://arxiv.org/abs/2501.02955)

近年来，视觉语言模型（VLMs）在视频理解方面取得了显著进展。然而，一项关键能力——细粒度运动理解能力——在当前的基准测试中尚未充分探索。为了解决这一差距，我们提出了 MotionBench，一个全面的评估基准，旨在评估视频理解模型的细粒度运动理解能力。MotionBench 通过六种主要类别的运动相关问题类型评估模型的运动级感知，并包括从不同来源收集的数据，确保广泛覆盖现实世界的视频内容。实验结果表明，现有的 VLMs 在理解细粒度运动方面表现不佳。为了增强 VLM 在有限的大语言模型序列长度内感知细粒度运动的能力，我们进行了广泛的实验，回顾了为视频特征压缩优化的 VLM 架构，并提出了一种新颖且高效的 Through-Encoder (TE) 融合方法。实验表明，更高的帧率输入和 TE 融合在运动理解方面带来了提升，但仍存在很大的改进空间。我们的基准旨在指导和激励开发更强大的视频理解模型，强调细粒度运动理解能力的重要性。项目页面：https://motion-bench.github.io 。

## Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos
[Sa2VA: 将 SAM2 与 LLaVA 结合以实现图像和视频的密集基础理解](https://arxiv.org/abs/2501.04001)

这项工作提出了 Sa2VA，这是第一个用于密集基础理解的统一模型，适用于图像和视频。与现有的多模态大语言模型不同，后者通常局限于特定的模态和任务，Sa2VA 支持广泛的图像和视频任务，包括引用分割和对话，仅需极少的一次性指令调优。Sa2VA 将 SAM-2（一个基础视频分割模型）与 LLaVA（一个先进的视觉-语言模型）结合，并将文本、图像和视频统一到一个共享的大语言模型 Token 空间中。通过使用大语言模型，Sa2VA 生成指令 Token，这些 Token 指导 SAM-2 生成精确的掩码，从而实现对静态和动态视觉内容的基于基础的多模态理解。此外，我们引入了 Ref-SAV，这是一个自动标注的数据集，包含超过 72k 个复杂视频场景中的对象表达，旨在提升模型性能。我们还手动验证了 Ref-SAV 数据集中的 2k 个视频对象，以在复杂环境中对引用视频对象分割进行基准测试。实验表明，Sa2VA 在多个任务中达到了最先进的水平，特别是在引用视频对象分割方面，突显了其在复杂现实应用中的潜力。

## rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking
[rStar-Math: 小型大语言模型可以通过自我进化的深度推理掌握数学推理](https://arxiv.org/abs/2501.04519)

我们提出了 rStar-Math，以证明小型大语言模型 (SLMs) 可以在不依赖从更优模型蒸馏的情况下，媲美甚至超越 OpenAI o1 的数学推理能力。rStar-Math 通过蒙特卡洛树搜索算法 (MCTS) 进行“深度推理”来实现这一目标，其中数学策略 SLM 在基于 SLM 的过程奖励模型 (Process Reward Model, PRM) 的指导下执行测试时搜索。rStar-Math 引入了三项创新来解决训练这两个 SLM 的挑战：(1) 一种新颖的代码增强的链式思维 (Chain-of-Thought, CoT) 数据合成方法，通过广泛的 MCTS 模拟生成逐步验证的推理轨迹，用于训练策略 SLM；(2) 一种新颖的过程奖励模型训练方法，避免了简单的逐步骤评分标注，从而产生更有效的过程偏好模型 (PPM)；(3) 一种自我进化策略，其中策略 SLM 和 PPM 从零开始构建，并通过迭代进化来提高推理能力。通过 4 轮自我进化，针对 747k 个数学问题生成了数百万个合成解，rStar-Math 将 SLM 的数学推理能力提升到了业界领先水平。在 MATH 基准测试中，它将 Qwen2.5-Math-7B 从 58.8% 提升到 90.0%，将 Phi3-mini-3.8B 从 41.4% 提升到 86.4%，分别超越了 o1-preview 4.5% 和 0.9%。在美国数学奥林匹克竞赛 (AIME) 中，rStar-Math 平均解决了 53.3% (8/15) 的问题，跻身于最优秀的高中数学学生前 20%。代码和数据将在 https://github.com/microsoft/rStar 上提供。

## Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Though
[大语言模型中的系统 2 推理：通过元思维链学习如何思考](https://arxiv.org/abs/2501.04682)

我们提出了一种新的框架——元思维链（Meta-CoT），它通过显式建模生成特定思维链所需的底层推理过程，扩展了传统的思维链（CoT）方法。我们展示了当前最先进模型的经验证据，这些模型的行为与上下文搜索一致，并探索了通过过程监督、合成数据生成和搜索算法生成元思维链的方法。此外，我们详细描述了一个训练模型生成元思维链的具体流程，该流程结合了指令调优、线性化搜索轨迹以及强化学习后训练。最后，我们讨论了开放的研究问题，包括扩展法则、验证器的作用以及发现新型推理算法的潜力。这项工作为在大语言模型中实现元思维链提供了理论和实践指导，为人工智能中更强大且类人的推理能力奠定了基础。

## Agent Laboratory: Using LLM Agents as Research Assistants
[Agent Laboratory: 使用大语言模型智能体作为研究助手](https://arxiv.org/abs/2501.04227)

历史上，科学发现是一个耗时且成本高昂的过程，从构想到最终成果需要大量的时间和资源。为了加速科学发现，降低研究成本，并提高研究质量，我们引入了 Agent Laboratory，这是一个基于大语言模型的自主框架，能够完成整个研究过程。该框架接受人类输入的研究想法，并通过三个阶段——文献综述、实验和报告撰写，以生成完整的研究成果，包括代码库和研究报告，同时允许用户在每个阶段提供反馈和指导。我们部署了 Agent Laboratory，并采用了多种最先进的大语言模型，邀请了多位研究人员通过参与调查评估其质量，提供人类反馈来指导研究过程，并对最终论文进行评估。我们发现：(1) 由 o1-preview 驱动的 Agent Laboratory 产生了最佳的研究成果；(2) 生成的机器学习代码能够与现有方法相比实现最先进的性能；(3) 人类在每个阶段的参与和反馈显著提高了研究的整体质量；(4) 与之前的自主研究方法相比，Agent Laboratory 显著降低了研究费用，实现了 84% 的成本降低。我们希望 Agent Laboratory 能够让研究人员将更多精力投入到创造性构思中，而不是繁琐的编码和写作，最终加速科学发现。

## Search-o1: Agentic Search-Enhanced Large Reasoning Models
[Search-o1: 基于智能体搜索增强的大推理模型](https://arxiv.org/abs/2501.05366)

像 OpenAI-o1 这样的大推理模型 (Large Reasoning Models, LRMs) 已经通过大规模强化学习展示了令人印象深刻的多步推理能力。然而，它们的扩展推理过程经常受到知识缺失的影响，导致频繁的不确定性和潜在的错误。为了解决这一限制，我们引入了 Search-o1，一个通过基于智能体的检索增强生成 (Retrieval-Augmented Generation, RAG) 机制和用于精炼检索文档的 "Reason-in-Documents"（文档内推理）模块来增强 LRMs 的框架。Search-o1 将智能体搜索工作流集成到推理过程中，使 LRMs 在遇到不确定的知识点时能够动态检索外部知识。此外，由于检索文档的冗长性，我们设计了一个独立的 "Reason-in-Documents" 模块，在将检索到的信息融入推理链之前对其进行深入分析，从而最小化干扰并保持连贯的推理流程。在科学、数学和编码等复杂推理任务以及六个开放域 QA 基准上的广泛实验证明了 Search-o1 的强大性能。这种方法增强了 LRMs 在复杂推理任务中的可信度和适用性，为更可靠和多功能智能系统铺平了道路。代码可在 https://github.com/sunnynexus/Search-o1 获取。

## URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics
[URSA: 理解与验证多模态数学中的思维链推理](https://arxiv.org/abs/2501.04686)

思维链（Chain-of-thought, CoT）推理已广泛应用于大语言模型（Large Language Models, LLMs）的数学推理中。最近，对 CoT 轨迹引入的导数过程监督引发了关于在测试时增强扩展能力的讨论，从而提升了这些模型的潜力。然而，在多模态数学推理中，高质量 CoT 训练数据的稀缺阻碍了现有模型实现高精度的 CoT 推理，并限制了测试时推理潜力的实现。在本工作中，我们提出了一种三模块合成策略，集成了 CoT 蒸馏、轨迹格式重写和格式统一。这产生了一个高质量的多模态数学 CoT 推理指令微调数据集，MMathCoT-1M。我们在多个多模态数学基准上全面验证了训练的 URSA-7B 模型的最先进（State-of-the-Art, SOTA）性能。对于测试时的扩展，我们引入了一种数据合成策略，自动生成过程注释数据集，称为 DualMath-1.1M，专注于解释和逻辑。通过在 DualMath-1.1M 上进一步训练 URSA-7B，我们从 CoT 推理能力过渡到强大的监督能力。训练后的 URSA-RM-7B 作为验证器，有效提升了 URSA-7B 在测试时的性能。URSA-RM-7B 还展示了出色的分布外（Out-of-Distribution, OOD）验证能力，展示了其泛化能力。模型权重、训练数据和代码将开源。

## LLM4SR: A Survey on Large Language Models for Scientific Research
[LLM4SR: 大语言模型在科学研究中的综述](https://arxiv.org/abs/2501.04306)

近年来，大语言模型（Large Language Models, LLMs）的快速发展已经深刻改变了科学研究的格局，在研究周期的各个阶段提供了前所未有的支持。本文首次系统地综述了大语言模型如何革新科学研究过程。我们分析了大语言模型在研究的四个关键阶段中的独特作用：假设发现、实验规划与实施、科学写作和同行评审。我们的综述全面展示了针对特定任务的方法论和评估基准。通过识别当前面临的挑战并提出未来的研究方向，本综述不仅强调了大语言模型的变革潜力，还旨在激励和指导研究人员和实践者利用大语言模型推动科学探究。资源可在以下仓库获取：https://github.com/du-nlp-lab/LLM4SR

## The GAN is dead; long live the GAN! A Modern GAN Baseline
[GAN 已死；GAN 万岁！现代 GAN 基线](https://arxiv.org/abs/2501.05441)

普遍认为 GAN 难以训练，并且文献中的 GAN 架构充满了经验性技巧。我们反驳了这一观点，并以更系统的方式构建了一个现代 GAN 基线。首先，我们推导了一个性能优越的正则化相对论 GAN 损失函数，解决了之前通过一系列临时性技巧处理的模式崩溃和非收敛问题。我们从数学上分析了该损失函数，并证明其具有局部收敛性，这与大多数现有的相对论损失函数不同。其次，我们的新损失函数使我们能够摒弃所有临时性技巧，并用现代架构替换常见 GAN 中使用的过时的骨干网络。以 StyleGAN2 为例，我们提出了一个简化和现代化的路线图，从而产生了一个新的极简基线——R3GAN。尽管方法简单，但我们的方法在 FFHQ、ImageNet、CIFAR 和 Stacked MNIST 数据集上超越了 StyleGAN2，并与最先进的 GAN 和扩散模型相比表现优于。

