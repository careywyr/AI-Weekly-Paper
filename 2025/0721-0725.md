## GUI-G^2: Gaussian Reward Modeling for GUI Grounding
[GUI-G^2：基于高斯奖励建模的图形用户界面定位](https://arxiv.org/abs/2507.15846)

图形用户界面 (Graphical User Interface, GUI) 定位技术将自然语言指令映射到精确的界面坐标以实现自主交互。现有强化学习方法采用二元奖励机制，将界面元素视为非此即彼的点击目标，这种稀疏反馈信号无法体现空间交互的连续性特征。受人类点击行为（天然形成以目标元素为中心的高斯分布）启发，我们提出GUI高斯定位奖励框架 (GUI-G^2)，该框架将GUI元素建模为界面平面上的连续高斯概率分布。GUI-G^2包含两个协同作用的机制：高斯点奖励通过基于元素中心的指数衰减概率分布实现精确定位，覆盖奖励则通过计算预测高斯分布与目标区域的重叠面积来评估空间匹配度。针对不同尺寸的界面元素，我们开发了自适应方差机制，可根据元素尺寸动态调整奖励分布。该框架将GUI定位问题从稀疏二元分类转化为基于连续概率密度的优化问题，利用高斯分布产生的丰富梯度信号引导模型收敛至最优交互位置。在ScreenSpot、ScreenSpot-v2和ScreenSpot-Pro基准测试上的实验表明，GUI-G^2显著超越现有最佳方法UI-TARS-72B，其中在ScreenSpot-Pro上取得24.7%的性能提升。分析表明，连续概率建模对界面变化具有更强的鲁棒性，对未知布局也展现出优异的泛化能力，为GUI交互任务中的空间推理建立了新范式。

## MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization
[MiroMind-M1：基于上下文感知多阶段策略优化的数学推理开源突破](https://arxiv.org/abs/2507.14683)

大语言模型近期实现了从文本生成到跨领域高级推理的能力跃迁，由此诞生了推理语言模型。数学推理因其需要精确的多步逻辑和抽象推理能力，成为具有代表性的基准测试任务，这些能力可泛化至其他领域。尽管 GPT-o3 等闭源 RLMs 展现出卓越的推理能力，但其闭源特性制约了透明度和可复现性。虽然众多开源项目试图弥补这一差距，但多数因缺失关键资源（如数据集和详细训练配置）而开放性不足，影响了可复现性。为推动 RLM 开发的透明度，我们推出基于 Qwen-2.5 主干模型的 MiroMind-M1 全开源系列，其性能达到或超越现有开源 RLMs。具体采用两阶段训练：先在包含 719K 数学推理问题及验证 CoT 轨迹的精选语料库上进行监督微调（SFT），再对 62K 高难度可验证问题进行强化学习验证（RLVR）。为提高 RLVR 过程的鲁棒性和效率，我们提出上下文感知多阶段策略优化算法，该算法融合长度渐进式训练与自适应重复惩罚机制，实现上下文感知的强化学习训练。在 AIME24、AIME25 和 MATH 基准测试中，我们的 Qwen-2.5 架构 7B/32B 开源模型取得了领先或具有竞争力的性能，并展现出更优的 Token 效率。为保障可复现性，我们完整开源：模型（MiroMind-M1-SFT-7B、MiroMind-M1-RL-7B、MiroMind-M1-RL-32B）、数据集（MiroMind-M1-SFT-719K、MiroMind-M1-RL-62K）及全部训练评估配置，期待这些资源能推动后续研究并促进社区发展。

## Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning
[突破上下文限制：面向长跨度推理的潜意识线程架构](https://arxiv.org/abs/2507.16784)

为突破大语言模型 (LLMs) 在推理准确性和效率上的上下文窗口限制，我们提出了线程推理模型 (TIM) —— 一个专为递归式与分解式问题求解训练的大语言模型系列，以及 TIMRUN 推理运行时环境。TIM 模型部署在 TIMRUN 运行时上，可在单一 LLM 推理过程中支持近乎无限的工作记忆容量和多跳工具调用，有效解决了输出长度限制、位置编码约束和 GPU 显存瓶颈等问题。该系统的核心创新在于将自然语言建模为具有长度和深度维度的推理树结构，而非传统线性序列。这些推理树由任务节点构成，每个节点包含思考过程、递归子任务以及基于 Schroeder 等人 (2025) 提出的理论框架得出的结论。在生成过程中，系统通过基于规则的子任务剪枝机制动态维护工作记忆，仅保留最相关上下文 Token 的键-值状态，从而实现位置编码和 GPU 显存页面的全局复用。实验结果表明，即使在 GPU 显存中管理高达 90% 的键-值缓存时，本系统仍能保持高推理吞吐量。此外，系统在数学推理任务中表现出色，并能有效处理需要长跨度推理和多跳工具调用的信息检索挑战。

## nablaNABLA: Neighborhood Adaptive Block-Level Attention
[nablaNABLA：邻域自适应块级注意力](https://arxiv.org/abs/2507.13546)

Transformer 架构的最新进展在视频生成任务中取得了显著成功。然而，全注意力机制的二次计算复杂度仍是关键瓶颈，尤其对于高分辨率和长时视频序列。本文提出 NABLA——一种新型邻域自适应块级注意力机制，可动态适应视频扩散 Transformer (DiTs) 中的稀疏模式。该方法通过采用基于自适应稀疏阈值的块级注意力，在维持生成质量的同时显著降低计算开销。本方案无需定制底层算子设计，且能与 PyTorch 的 Flex Attention 算子无缝集成。实验表明，相比基线方法，NABLA 实现最高 2.7 倍的训练与推理加速，几乎不影响关键定量指标（CLIP 分数、VBench 分数、人工评估分数）和视觉质量损失。代码与模型权重已开源：https://github.com/gen-ai-team/Wan2.1-NABLA

## The Invisible Leash: Why RLVR May Not Escape Its Origin
[无形的束缚：为何RLVR可能无法突破其起源](https://arxiv.org/abs/2507.14843)

大语言模型推理能力的最新进展表明，可验证奖励强化学习 (Reinforcement Learning with Verifiable Rewards, RLVR) 是增强AI能力的一种有前景方法，尤其在解决复杂逻辑任务方面表现突出。然而，RLVR究竟能否真正扩展模型的推理边界，还是仅通过放大基础模型已知的高奖励输出来提高精度，这一问题尚未明确。本研究通过理论和实证分析，揭示了RLVR的潜在局限性。首先，我们提出新的理论观点：RLVR受限于基础模型的支持集——无法对初始概率为零的解决方案进行采样——其本质是一种保守的重加权机制，可能阻碍完全原创解决方案的发现。我们还发现了熵与奖励之间的权衡关系：虽然RLVR能稳定提升精度，但会逐渐缩小探索范围，可能遗漏正确但低概率的解决方案。大量实验数据表明，尽管RLVR能持续提高pass@1指标，但在更大采样预算下，经验支持集的收缩幅度通常超过其扩展幅度，导致基础模型原本能够生成的正确答案无法被恢复。有趣的是，研究还发现RLVR虽然有时会增加token级别的熵（使每个生成步骤的不确定性增大），但答案级别的熵却会降低，这意味着那些看似不确定的生成路径最终会收敛到更小的答案集合。这些发现共同揭示了RLVR在拓展推理能力方面的固有局限。要突破这种隐性的约束，未来可能需要开发新的算法，例如引入显式探索机制，或采用混合策略将概率质量分配到低概率解空间区域。

## Group Sequence Policy Optimization
[序列组策略优化](https://arxiv.org/abs/2507.18071)

本文提出序列组策略优化（Group Sequence Policy Optimization，GSPO），这是一种稳定、高效且高性能的强化学习算法，专为训练大语言模型设计。不同于以往采用Token级重要性比例的方法，GSPO根据序列似然来定义重要性比例，并进行序列级的裁剪、奖励和优化。实验表明，相较于GRPO算法，GSPO在训练效率和性能上表现更优，能有效稳定混合专家（Mixture-of-Experts，MoE）的强化学习训练过程，同时具备简化强化学习基础设施设计的潜力。GSPO的这些优势显著提升了最新Qwen3模型的性能。

## Yume: An Interactive World Generation Model
[Yume：交互式世界生成模型](https://arxiv.org/abs/2507.17744)

Yume 通过图像、文本或视频输入构建交互式、高动态且逼真的虚拟世界，支持通过外设或神经信号进行探索与控制。本报告展示了 \method 的预览版，该系统能够基于输入图像生成动态世界，并支持键盘交互探索。为实现高保真交互式视频世界生成，我们设计了包含四大核心组件的框架：相机运动量化、视频生成架构、高级采样器和模型加速。首先，通过量化相机运动实现稳定训练和基于键盘的友好交互；其次，提出带记忆模块的掩码视频扩散 Transformer (MVDT)，以自回归方式实现无限视频生成；随后，采样器引入免训练抗伪影机制 (AAM) 和基于随机微分方程的时间旅行采样 (TTS-SDE)，以提升视觉质量与控制精度；此外，通过对抗性蒸馏与缓存机制的协同优化实现模型加速。基于高质量世界探索数据集 \sekai 训练的 \method 在多样化场景与应用中表现优异。所有数据、代码库及模型权重详见 https://github.com/stdstu12/YUME。Yume 将按月迭代更新以实现既定目标。项目主页：https://stdstu12.github.io/YUME-Project/。

## The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs  
[面具背后的魔鬼：扩散大语言模型新现的安全漏洞](https://arxiv.org/abs/2507.11097)  

扩散大语言模型 (Diffusion-based LLMs, dLLMs) 近期成为自回归大语言模型的重要替代方案，其并行解码与双向建模特性显著提升了推理速度与交互能力。尽管在代码生成和文本填充任务中表现优异，我们发现该类模型存在本质性安全隐患：现有对齐机制无法有效防御上下文感知型掩码输入对抗提示，导致新型攻击面暴露。为此，我们提出首个针对 dLLMs 安全弱点的系统性越狱攻击框架 DIJA。该框架通过构建对抗性交错掩码-文本提示，利用 dLLMs 特有的双向建模与并行解码机制：双向建模会强制模型为掩码片段生成上下文一致的输出（即使内容有害），而并行解码则削弱了模型对不安全内容的动态过滤与拒绝采样能力。这种现象导致经对齐调优的 dLLMs 仍会生成有害内容，即便提示中明确包含有害行为指令。实验表明，DIJA 在 JailbreakBench 基准上的评估者 ASR 较现有最优基线 ReNeLLM 提升 78.5%，StrongREJECT 分数提高 37.7 分，且在 Dream-Instruct 数据集实现 100% 关键词 ASR——所有攻击均无需修改或隐藏提示中的有害内容。这些发现揭示了 dLLM 架构中未被重视的安全威胁，亟需重新审视此类新兴语言模型的安全对齐方案。代码已开源：https://github.com/ZichenWen1/DIJA。

## Pixels, Patterns, but No Poetry: To See The World like Humans
[像素、模式，但无诗意：像人类一样看世界](https://arxiv.org/abs/2507.16863)  

在多模态大语言模型 (Multimodal Large Language Models, MLLMs) 中实现类人感知与推理能力，仍是人工智能领域的核心挑战。尽管近期研究主要聚焦于提升 MLLMs 的推理能力，但一个根本性问题始终存在：多模态大语言模型能否真正实现人类式的世界感知？本文将研究重心从推理转向感知，通过构建图灵视觉测试 (Turing Eye Test, TET) —— 一项包含四项诊断任务的感知导向型挑战基准，评估 MLLMs 对人类可直觉处理的合成图像的表现。研究发现：当前最先进的 MLLMs 在人类可轻松完成的感知任务中出现了系统性失效。针对先前基准有效的上下文学习和语言主干训练均无法提升任务表现，而微调视觉编码器 (vision tower) 却能实现快速适应。这表明本基准的挑战性源于视觉编码器的泛化能力不足，而非语言主干的知识与推理缺陷 —— 这揭示了当前 MLLMs 与人类感知间的关键差距。本文发布了 TET 任务集的代表性子集，后续工作将引入更多样化的任务与方法以增强视觉泛化能力。

## Step-Audio 2 Technical Report  
[Step-Audio 2 技术报告](https://arxiv.org/abs/2507.16632)  

本文提出 Step-Audio 2，一种面向工业级应用的端到端多模态大语言模型，专注于音频理解与语音对话任务。该模型通过融合潜在音频编码器与推理导向的强化学习（RL），在自动语音识别（ASR）和音频理解任务中展现出优异性能。为实现真正的端到端语音对话，Step-Audio 2 将离散音频 Token 生成纳入语言建模框架，显著增强了对超语言信息（如语音风格和情感特征）的响应能力。为高效利用真实数据中的文本与声学知识，模型集成检索增强生成（RAG）技术，支持调用外部工具（如网络搜索以缓解生成幻觉，音频搜索实现音色切换）。基于数百万小时语音与音频数据的训练，Step-Audio 2 在多样化对话场景中具备智能交互与情感表达能力。评估显示，相较于其他开源及商业方案，该模型在多项音频理解与对话基准测试中均达到当前最优性能。更多信息详见 [https://github.com/stepfun-ai/Step-Audio2](https://github.com/stepfun-ai/Step-Audio2)。

## A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models
[应对俄语语音生成模型音系与韵律挑战的数据中心框架](https://arxiv.org/abs/2507.13563)

俄语语音合成存在若干独特挑战，包括元音弱化、辅音清音化、可变重音模式、同形异义词歧义及非自然语调。本文提出Balalaika数据集，包含超过2,000小时录音室级质量的俄语语音数据，并附带完整文本标注（含标点符号和重音标记）。实验表明，基于Balalaika训练的模型在语音合成与增强任务上的表现显著优于现有数据集训练的模型。我们详细阐述了数据集构建流程、标注方法和对比评估结果。

## KIMI K2: OPEN AGENT INTELLIGENCE - Technical Report of KIMI K2
[KIMI K2 技术报告](https://github.com/MoonshotAI/Kimi-K2/blob/main/tech_report.pdf)

我们发布了 Kimi K2，这是一款采用专家混合架构（Mixture-of-Experts, MoE）的大语言模型，具备 1 万亿总参数量，其中每次推理激活的参数为 320 亿。我们提出了一种改进的优化器 MuonClip，它在 Muon 的基础上引入了全新的 QK-clip 技术，有效解决训练过程中的不稳定性问题，同时保留了 Muon 在 Token 使用上的高效特性。在 MuonClip 优化器的支持下，K2 在 15.5 万亿个 Token 上完成了预训练，训练过程中未出现 loss 的突增。在后训练阶段，K2 经过多阶段的处理流程，重点包括一个大规模的 AI 智能体数据合成流水线，以及一个联合强化学习（Reinforcement Learning, RL）阶段，模型通过与真实与合成环境的交互，不断提升智能能力。

在开源非推理模型中，Kimi K2 展现了最先进的性能，特别是在 AI 智能体能力方面表现突出。具体来看，K2 在 Tau2-Bench 上得分为 66.1，在 ACEBench（英文）上为 76.5，在 SWE-Bench Verified 上为 65.8，在 SWE-Bench Multilingual 上为 47.3，全面超越多数开源和闭源的基线模型。在编程、数学与逻辑推理任务中，K2 同样展现出强大能力，在 LiveCodeBench v6 上得分为 53.7，AIME 2025 上为 49.5，GPQA-Diamond 上为 75.1，OJBench 上为 27.1，以上成绩均在未使用复杂推理机制的前提下取得。综上所述，Kimi K2 是当前开源大语言模型中表现最强的一员，尤其适用于软件工程与 AI 智能体任务。我们已开放基础模型与后训练模型的检查点，以推动智能体智能领域的研究与应用。