## Less is More: Recursive Reasoning with Tiny Networks
[少即是多：微型网络的递归推理](https://arxiv.org/abs/2510.04871)

分层推理模型 (HRM) 是一种创新方法，它采用两个小型神经网络，以不同的递归频率运行。这种受生物学启发的技术，在数独、迷宫和 ARC-AGI 等复杂谜题任务中，表现优于大语言模型 (LLMs)，且仅需小型模型（2700万参数）和少量数据（约1000个样本）进行训练。HRM 在利用小型网络解决难题方面前景广阔，但其机制尚未被充分理解，且可能存在优化空间。我们提出微型递归模型 (TRM)，这是一种更简化的递归推理方法，仅使用一个2层微型网络，就实现了比 HRM 更高的泛化性能。TRM 仅含700万参数，在 ARC-AGI-1 上测试准确率达到45%，在 ARC-AGI-2 上为8%，优于大多数大语言模型（如 Deepseek R1、o3-mini、Gemini 2.5 Pro），而参数量不足它们的0.01%。

## Agent Learning via Early Experience
[智能体通过早期经验学习](https://arxiv.org/abs/2510.08558)

语言智能体的长期目标是通过自身经验进行学习与改进，最终在复杂的现实任务中超越人类。然而，在许多环境中，利用强化学习从经验数据训练智能体仍面临困难：这些环境或缺乏可验证奖励（如网站交互），或需进行低效的长程轨迹展开（如多轮工具调用）。因此，当前多数智能体依赖对专家数据的监督微调，这种方法难以扩展且泛化性能较差。该局限源于专家演示的本质——它们仅涵盖有限场景，使智能体接触的环境多样性不足。为解决此问题，我们提出一种折中范式“早期经验”：即由智能体自身行动产生的交互数据，其中后续状态可作为无奖励信号的监督信息。在此范式下，我们探索两种数据使用策略：（1）隐式世界建模——利用收集的状态使策略适应环境动态；（2）自我反思——智能体通过分析次优行动来提升推理与决策能力。我们在八个异构环境及多个模型系列中开展评估，结果表明该方法能持续提升任务效能与跨领域泛化能力，印证了早期经验的价值。此外，在具可验证奖励的环境中，早期经验为后续强化学习奠定了坚实基础，可作为模仿学习与全经验驱动智能体之间的有效桥梁。

## Apriel-1.5-15b-Thinker  
[Apriel-1.5-15b-Thinker](https://arxiv.org/abs/2510.01141)  

我们推出 Apriel-1.5-15B-Thinker，这是一个拥有 150 亿参数的开放权重多模态推理模型，通过精心的训练设计而非单纯扩大规模实现了前沿级性能。基于 Pixtral-12B，我们采用三阶段渐进方法：(1) 深度扩展以增强推理能力而无需重新预训练；(2) 分阶段持续预训练：先建立基础文本与视觉理解能力，再通过针对空间结构、组合理解和细粒度感知的定向合成数据生成来强化视觉推理；(3) 在涵盖数学、编程、科学和工具使用的精选指令-响应对上进行高质量纯文本监督微调，并包含显式推理轨迹。值得注意的是，该模型无需强化学习或偏好优化即获得有竞争力的结果，这凸显了我们以数据为中心的持续预训练方法的独立贡献。在 Artificial Analysis Intelligence Index 评估中，Apriel-1.5-15B-Thinker 取得 52 分，与 DeepSeek-R1-0528 表现相当，但计算资源消耗显著更低。在十项图像基准测试中，其性能平均与 Gemini-2.5-Flash 和 Claude Sonnet-3.7 相差不超过 5 分，这对于受单 GPU 部署限制的模型而言具有重要意义。我们的结果表明，经过精心设计的中期训练能够在无需大规模扩展的情况下显著弥补能力差距，让基础设施有限的组织也能实现前沿级多模态推理。我们依据 MIT 许可证公开模型检查点、完整训练方案和评估协议，以促进开源研究发展。

## MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization
[MM-HELIX：基于整体平台与自适应混合策略优化的多模态长链反思推理增强](https://arxiv.org/abs/2510.08540)

当前多模态大语言模型 (MLLMs) 虽在数学与逻辑等推理任务中表现优异，但其长链反思推理能力——解决复杂现实问题的关键前提——仍待深入探索。本研究首先通过大规模实证评估该能力，基于精心设计的数据合成引擎构建了MM-HELIX多模态基准，包含42类挑战性合成任务的1,260个样本，均需迭代思维与逆向推理。基准测试表明，现有MLLMs在长链反思推理中存在明显能力不足。为此，我们生成训练后数据并探索其学习范式：首先开发步骤诱导响应生成流水线，构建包含10万条高质量反思推理轨迹的大规模指令微调数据集MM-HELIX-100K。针对标准强化学习在复杂任务中因奖励稀疏与监督微调后灾难性遗忘而失效的问题，提出自适应混合策略优化 (AHPO)，该创新训练策略将离线监督与在线优化动态融合至单一阶段，使模型能在奖励稀疏时学习专家数据，熟练后自主探索。在Qwen2.5-VL-7B基线上的实验表明，本方法在MM-HELIX基准上准确率提升18.6%，在通用数学与逻辑任务中平均性能提升5.7%，展现出强大泛化能力。本研究证实MLLMs的反思推理能力可被有效学习与泛化，为开发更强大的多模态大语言模型奠定基础。

## Paper2Video: Automatic Video Generation from Scientific Papers
[Paper2Video：从科学论文自动生成视频](https://arxiv.org/abs/2510.05096)

学术演示视频已成为研究传播的关键媒介，但其制作过程仍高度耗时费力，通常需要花费数小时进行幻灯片设计、录制和编辑，才能产出一段2至10分钟的短视频。与普通视频不同，演示视频生成面临独特挑战：输入源为研究论文，包含密集的多模态信息（文本、图表、表格），且需协调多个同步组件，如幻灯片、字幕、语音和真人讲述者。为解决这些问题，我们提出了Paper2Video基准数据集，首次收录了101篇研究论文及其对应的作者自制演示视频、幻灯片和演讲者元数据。我们还设计了四项定制评估指标——元相似度、PresentArena、PresentQuiz和IP Memory——用于量化视频向观众传递论文信息的效果。在此基础上，我们开发了PaperTalker，这是首个面向学术演示视频生成的多智能体框架。该框架通过创新性的树搜索视觉选择、光标定位、字幕生成、语音合成和虚拟人像渲染技术，将幻灯片生成与布局优化有效整合，并采用逐页并行生成策略以提升效率。在Paper2Video数据集上的实验表明，本方法生成的演示视频相较于现有基线具有更高的信息保真度和丰富度，为实现自动化、即用型学术视频生成迈出了实质性一步。我们的数据集、智能体及代码公开于：https://github.com/showlab/Paper2Video。

## Cache-to-Cache: Direct Semantic Communication Between Large Language Models
[Cache-to-Cache：大语言模型间的直接语义通信](https://arxiv.org/abs/2510.03215)

多LLM系统通过整合多样化大语言模型的互补优势，实现了单一模型无法达成的性能提升与效率优化。现有架构中，LLM间采用文本进行通信，这要求将内部表示转换为输出token序列。该过程不仅导致丰富语义信息的丢失，还会引入逐token生成的延迟。基于这些局限性，我们探讨：LLM能否实现超越文本的通信？Oracle实验证明，在保持缓存大小不变的情况下，增强KV-Cache的语义可提升响应质量，表明KV-Cache可作为模型间通信的有效媒介。为此，我们提出Cache-to-Cache (C2C) 这一LLM间直接语义通信的新范式。C2C通过神经网络将源模型的KV-cache投影并融合至目标模型的KV-cache，实现直接语义传递。采用可学习门控机制筛选可从缓存通信中获益的目标层。相较于文本通信，C2C能同时利用两模型的深层专业语义，且无需生成显式中间文本。实验结果表明，C2C相比单一模型平均准确率提升8.5-10.5%，较文本通信范式进一步提高约3.0-5.0%，同时延迟平均降低至1/2（速度提升2.0倍）。代码已开源：https://github.com/thu-nics/C2C。

## In-the-Flow Agentic System Optimization for Effective Planning and Tool Use
[在线流程智能体系统优化实现高效规划与工具调用](https://arxiv.org/abs/2510.05592)

结果导向的强化学习推动了大语言模型 (LLM) 推理能力的发展，但现有工具增强方法通常训练单一整体策略，在完整上下文环境中交替生成推理思路和调用工具。这种方法在长时域任务和多样化工具集场景下扩展性较差，且对新场景的泛化能力较弱。智能体系统通过将任务分解至专用模块提供了有前景的替代方案，然而大多数系统仍保持无需训练状态，或采用与多轮交互实时动态分离的离线训练。我们提出 AgentFlow——一个可训练的在线流程智能体框架，通过动态演进的记忆机制协调四个核心模块（规划器、执行器、验证器、生成器），并在多轮交互循环中直接优化规划器。为实现实时环境中的在线策略训练，我们提出基于流程的分组精炼策略优化 (Flow-GRPO)，该方法将多轮优化问题转化为一系列易处理的单轮策略更新，有效解决长时域稀疏奖励的信用分配难题。通过向每个交互轮次广播单一可验证的轨迹级结果，使局部规划器决策与全局成功目标保持一致，并利用分组归一化的优势函数稳定训练过程。在十项基准测试中，基于70亿参数骨干网络的AgentFlow显著超越最优基线方法，在搜索任务上平均准确率提升14.9%，智能体任务提升14.0%，数学任务提升14.5%，科学任务提升4.1%，甚至优于GPT-4o等更大规模的专有模型。深入分析证实了在线流程优化的优势，展现出更精准的规划能力、更可靠的工具调用性能，以及随模型规模与推理轮次增加而持续提升的可扩展性。

## Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer
[Ming-UniVision：基于统一连续分词器的联合图像理解与生成](https://arxiv.org/abs/2510.06590)

视觉分词技术仍是实现自回归范式下视觉理解与生成统一的核心挑战。现有方法通常采用离散潜在空间的分词器，以对齐大语言模型的 Token，但量化误差会限制语义表达能力，并削弱视觉语言理解性能。为此，我们提出 MingTok——一个具有连续潜在空间的视觉分词器系列，用于统一的自回归生成与理解任务。理解任务倾向于判别性高维特征，而生成任务则更适合紧凑的低层编码。为平衡这一矛盾，MingTok 采用包含低层编码、语义增强和视觉重建的三阶段序列架构。基于此架构，Ming-UniVision 无需任务专用的视觉表示，即可将多种视觉语言任务统一于单一自回归预测范式。通过将理解与生成共同形式化为共享连续空间中的下一 Token 预测，本方法可无缝支持多轮上下文相关任务，例如迭代式理解、生成与编辑。实证研究表明，采用统一连续视觉表示能有效调和理解与生成任务对分词器的矛盾要求，从而在两类任务中均达到最先进性能。我们希望本研究能推动连续域中的统一视觉分词技术发展。相关推理代码与模型权重已开源，以促进社区研究。

## Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs
[Fathom-DeepResearch：解锁小语言模型 (SLM) 的长视野信息检索与合成](https://arxiv.org/abs/2509.24107)

工具集成推理已成为实现 AI 智能体 (AI Agent) 应用的关键方向。其中，深度研究智能体 (DeepResearch Agent) 因其在复杂、开放式信息检索任务上的卓越表现而备受关注。我们推出 Fathom-DeepResearch，该系统由两个专用模型构成：首先是 Fathom-Search-4B，这是一个基于 Qwen3-4B 训练的深度搜索 (DeepSearch) 模型，通过实时网络搜索与定向网页查询优化了基于证据的探索流程。其训练融合了三项技术创新：(i) DUETQA——通过多智能体自博弈生成的 5,000 样本数据集，强制模型严格依赖网络搜索并实现异构信息源锚定；(ii) RAPO——作为 GRPO 的零开销扩展，通过课程剪枝、奖励感知优势缩放及逐提示重放缓冲区，实现了带可验证奖励的多轮强化学习稳定训练；(iii) 可调控的步骤级奖励机制，依据认知行为与边际效用对每个工具调用进行分类，从而显式控制搜索轨迹的广度、深度与时间范围。这些改进使得工具调用在必要时可稳定扩展至超过 20 次。其次是 Fathom-Synthesizer-4B，该模型基于 Qwen3-4B 训练，能将多轮深度搜索轨迹转化为结构化的高引用密度深度研究报告，实现信息的全面整合。在深度搜索基准（SimpleQA、FRAMES、WebWalker、Seal0、MuSiQue）与 DeepResearch-Bench 上的评估表明，本系统在开放权重类别中达到了最优性能，同时在 HLE、AIME-25、GPQA-Diamond 及 MedQA 等多样化推理任务中展现出强大的泛化能力。

## MemMamba: Rethinking Memory Patterns in State Space Model
[MemMamba：重新思考状态空间模型中的记忆模式](https://arxiv.org/abs/2510.03279)

随着数据量的爆炸式增长，长序列建模在自然语言处理和生物信息学等任务中的重要性日益凸显。然而，现有方法在效率与内存之间存在固有权衡：循环神经网络因梯度消失和爆炸问题而难以扩展；Transformers 虽能建模全局依赖，但受二次复杂度限制。近期，选择性状态空间模型（如 Mamba）展现出 O(n) 时间复杂度和 O(1) 循环推理的高效特性，但其长程记忆会呈指数级衰减。本文通过数学推导与信息论分析，系统揭示了 Mamba 的记忆衰减机制，并回答了核心问题：Mamba 的长程记忆本质及其信息保留机制。为量化关键信息损失，我们进一步提出水平-垂直记忆保真度指标，用于度量层内与层间的记忆退化。受人类阅读长文档时提炼关键信息的启发，我们提出 MemMamba——一种集成状态汇总机制与跨层、跨 Token 注意力的新型架构框架，在保持线性复杂度的同时有效缓解长程遗忘问题。在 PG19 和密码检索等长序列基准测试中，MemMamba 较现有 Mamba 变体及 Transformers 实现显著提升，推理效率加速 48%。理论与实验结果表明，MemMamba 突破了复杂度-内存权衡瓶颈，为超长序列建模提供了新范式。

## Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models
[智能体上下文工程：为自改进语言模型演化上下文](https://arxiv.org/abs/2510.04618)

大语言模型 (LLM) 应用（如智能体和领域特定推理系统）日益依赖上下文自适应技术——通过添加指令、策略或证据来修改输入，而非更新模型权重。现有方法虽提升了可用性，但常存在两大缺陷：一是简洁性偏见，即为追求简明摘要而丢失关键领域洞察；二是上下文崩溃，即迭代重写过程会随时间推移不断侵蚀细节信息。基于动态备忘单 (Dynamic Cheatsheet) 提出的自适应记忆机制，我们引入 ACE（智能体上下文工程）框架，该框架将上下文视为持续演化的策略手册，通过模块化的生成、反思与管理流程，实现策略的持续积累、优化和组织。ACE 采用结构化增量更新机制，既能保留细粒度知识，又可适配长上下文模型，从而有效避免上下文崩溃。在智能体与领域特定基准测试中，ACE 对离线上下文（如系统提示）和在线上下文（如智能体记忆）均实现优化，其性能持续超越强基线模型：在智能体任务上提升10.6%，在金融领域任务上提升8.6%，同时显著降低了自适应延迟与部署成本。值得注意的是，ACE 无需标注监督即可实现有效自适应，仅依靠自然执行反馈进行优化。在 AppWorld 排行榜中，尽管采用规模较小的开源模型，ACE 在整体平均指标上追平排名最高的生产级智能体，并在难度更高的测试挑战集上实现反超。这些结果表明，通过构建全面且持续演进的上下文系统，能够以较低开销实现可扩展、高效率且具备自改进能力的大语言模型系统。

## TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning
[TaTToo：面向表格推理测试时扩展的工具基础思维过程奖励模型](https://arxiv.org/abs/2510.06217)

过程奖励模型 (PRMs) 近期已成为增强大型推理模型 (LRMs) 推理能力的强大框架, 尤其在测试时扩展 (TTS) 场景中。然而, 其在表格推理领域中监督大型推理模型的潜力仍未得到充分探索。通过详细实证分析, 我们发现现有 PRMs 虽广泛用于监督纯文本推理步骤, 但在处理表格特有操作 (如子表检索和模式交互) 时存在困难, 导致关键性能瓶颈。为突破此限制, 我们提出 TaTToo——一种新型表格基础 PRM 框架, 具备以下特性: (i) 显式推理表格推理步骤, (ii) 集成工具验证以提供精准奖励监督。具体实现中, 我们首先设计可扩展数据构建流程, 通过融合表格验证原理与工具执行, 生成超过 6 万条高质量步骤级标注数据。基于此数据, 采用双阶段范式训练 TaTToo: 通过冷启动监督微调学习工具使用推理模式, 继而采用工具基础奖励塑造的强化学习使模型适配表格验证机制。我们对新设计 PRM 引发的策略改进进行全面评估: 在涵盖数值推理、事实核查与数据分析的 5 项挑战性表格推理基准测试中, TaTToo 在推理阶段将下游策略大型推理模型性能提升 30.9%, 仅以 80 亿参数即超越 Qwen-2.5-Math-PRM-720 亿等强基准模型, 并在多样化测试时扩展 (TTS) 策略中展现卓越泛化能力。

## VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning
[VideoCanvas：通过上下文条件化从任意时空补丁实现统一视频补全](https://arxiv.org/abs/2510.08555)

我们提出了任意时空视频补全任务，该任务根据用户指定的任意补丁生成视频，这些补丁可置于任意空间位置和时间点，类似于在视频画布上进行绘制。这种灵活框架自然地将多种现有可控视频生成任务——包括首帧图像转视频、修复、扩展和插值——统一到单一连贯的范式下。然而，实现该愿景面临现代潜在视频扩散模型的一个根本性挑战：因果VAE引入的时间模糊性问题，即多个像素帧被压缩为单一潜在表示，导致在结构上难以实现精确的帧级条件控制。为此，我们推出VideoCanvas，这一创新框架将上下文条件化（ICC）范式应用于此类细粒度控制任务，且无需新增参数。我们提出一种混合条件化策略，以解耦空间与时间控制：空间布局通过零填充处理，而时间对齐则借助时间RoPE插值技术实现，该技术为每个条件在潜在序列中分配连续分数位置。这消除了VAE的时间模糊性，并在参数冻结的骨干网络上实现了像素帧级感知控制。为评估这一新能力，我们构建了VideoCanvasBench，这是首个针对任意时空视频补全的基准测试，涵盖场景内保真度与场景间创造性。实验结果表明，VideoCanvas显著优于现有条件化方法，在灵活统一的视频生成领域确立了新的技术标杆。

## UniVideo: Unified Understanding, Generation, and Editing for Videos
[UniVideo：视频的统一理解、生成与编辑](https://arxiv.org/abs/2510.08377)

统一多模态模型在多模态内容生成与编辑领域已展现出优异性能，但目前主要局限于图像领域。本文提出UniVideo——一种通用框架，将统一建模能力扩展至视频领域。该框架采用双流架构：通过多模态大语言模型 (MLLM) 实现指令理解，结合多模态DiT (MMDiT) 完成视频生成。该设计既能精准解析复杂多模态指令，又可保持视觉一致性。基于此架构，UniVideo将多种视频生成与编辑任务统一整合至单一多模态指令范式，并进行联合训练。大量实验表明，在文本/图像到视频生成、上下文视频生成及上下文视频编辑任务中，UniVideo的性能达到或超越了当前最先进的专用模型。值得注意的是，其统一架构实现了两类泛化能力：首先支持任务组合（如将编辑与风格迁移相结合），通过单一指令整合多项功能；其次即使未经过自由形式视频编辑的显式训练，也能将大规模图像编辑数据中学到的能力迁移至该场景，成功处理未见指令（如视频人物绿幕抠像、材质替换等）。除核心功能外，UniVideo还支持基于视觉提示的视频生成，由MLLM解析视觉提示并引导MMDiT完成合成。为促进该领域发展，我们将公开模型与代码。

## Large Reasoning Models Learn Better Alignment from Flawed Thinking
[大型推理模型从缺陷推理中学习更好的对齐](https://arxiv.org/abs/2510.00938)

大型推理模型 (LRMs) 通过生成结构化的思维链 (CoT) 来“思考”，然后输出最终答案，但它们仍缺乏对安全对齐进行批判性推理的能力，当有缺陷的前提被引入其思维过程时，容易产生偏见。我们提出 RECAP (通过反对齐预填充的鲁棒安全对齐)，一种基于原则的强化学习 (RL) 后训练方法，旨在明确教导模型推翻有缺陷的推理轨迹，并转向生成安全且有益的响应。RECAP 在合成生成的反对齐 CoT 预填充与标准提示的混合数据上进行训练，无需额外训练成本或对标准基于人类反馈的强化学习 (RLHF) 进行修改，显著提升了安全性和抗越狱鲁棒性，减少了过度拒绝行为，同时保留了核心推理能力——所有这些改进均在维持推理令牌预算的前提下实现。广泛分析表明，经 RECAP 训练的模型更频繁地进行自我反思，并在自适应攻击下保持鲁棒性，即使面临多次试图推翻其推理的尝试，仍能维持安全性。

## DreamOmni2: Multimodal Instruction-based Editing and Generation
[DreamOmni2：基于多模态指令的编辑与生成](https://arxiv.org/abs/2510.06679)

近期，基于指令的图像编辑与主体驱动生成技术取得显著进展，引发广泛关注，但这两类任务在满足实际用户需求方面仍存在局限。基于指令的编辑仅依赖语言指令，往往难以捕捉具体编辑细节，因此需要参考图像；而主体驱动生成则局限于组合具体物体或人物，忽略了更广泛的抽象概念。为解决这些问题，我们提出两项新任务：基于多模态指令的编辑与生成。这些任务同时支持文本和图像指令，并将范围扩展至具体与抽象概念，从而大幅提升其实用性。我们推出 DreamOmni2，旨在解决两大挑战：数据构建与模型框架设计。数据合成流程包含三个步骤：(1) 采用特征混合方法为抽象和具体概念生成提取数据；(2) 利用编辑和提取模型生成基于多模态指令的编辑训练数据；(3) 进一步应用提取模型创建基于多模态指令的编辑训练数据。在框架设计上，为处理多图像输入，我们提出索引编码与位置编码偏移方案，帮助模型区分图像并防止像素信息混淆；同时引入视觉语言模型 (VLM) 与生成/编辑模型的联合训练，以更好地解析复杂指令。此外，我们为这两项新任务建立了全面基准测试，以推动其发展。实验表明，DreamOmni2 取得了显著成果，模型与代码将公开。

## Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding
[Lumina-DiMOO：面向多模态生成与理解的全能扩散大语言模型](https://arxiv.org/abs/2510.06308)

我们推出 Lumina-DiMOO——一个开源基础模型，能够实现无缝的多模态生成与理解。该模型采用完全离散扩散建模方法处理多种模态的输入输出，从而与现有统一模型形成显著区别。这种创新方案使 Lumina-DiMOO 相比之前的自回归 (AR) 或混合 AR-扩散范式具有更高采样效率，并能灵活支持广泛的多模态任务，包括：文本到图像生成、图像到图像生成（如图像编辑、主题驱动生成和图像修复等）以及图像理解。Lumina-DiMOO 在多项基准测试中达到业界最优性能，超越现有开源统一多模态模型。为推动多模态与离散扩散模型研究的进展，我们已向社区公开代码和模型检查点。项目页面：https://synbol.github.io/Lumina-DiMOO

## From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning 
[从“是什么”到“为什么”：一个基于证据的化学反应条件推理多智能体系统](https://arxiv.org/abs/2509.23768)  

化学反应推荐旨在为化学反应选择合适的反应条件参数，这对加速化学科学发展具有关键作用。随着大语言模型 (LLM) 的快速发展，利用其推理与规划能力进行反应条件推荐的兴趣日益增长。尽管现有方法已取得一定成功，但它们很少阐明推荐反应条件的内在依据，从而限制了其在高风险科学工作流程中的应用价值。本研究提出 ChemMAS，一个多智能体系统，它将条件预测重构为基于证据的推理任务。ChemMAS 将任务分解为机制基础分析、多通道召回、约束感知的智能体辩论和原理聚合四个阶段。每个决策均以化学知识和检索到的先例为基础，提供可解释的理由。实验表明，ChemMAS 在 Top-1 准确率上较领域特定基线提升 20-35%，并优于通用大语言模型 10-15%，同时输出可证伪、人类可信的推理过程，为科学发现中的可解释 AI 确立了新范式。

