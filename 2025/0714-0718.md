## A Survey of Context Engineering for Large Language Models 
[大语言模型上下文工程研究综述](https://arxiv.org/abs/2507.13334)  

大语言模型 (LLM) 的推理性能本质上取决于其所接收的上下文信息。本综述系统性地提出了上下文工程 (Context Engineering) 这一新兴学科领域，它不仅超越了简单的提示设计 (prompt design) ，更涵盖了对大语言模型信息载体的系统性优化。我们建立了一个完整的分类体系，将上下文工程分解为核心基础组件及其在智能系统中的高级实现。首先剖析三大基础组件：上下文检索与生成、上下文处理以及上下文管理。进而探讨这些组件的体系化集成方案：检索增强生成 (RAG) 、记忆系统与工具集成推理，以及多智能体系统。基于对 1300 余篇文献的系统性分析，本综述不仅绘制了该领域的技术发展路线图，更揭示了一个关键研究矛盾：模型能力存在本质性不对称。尽管当前模型在先进上下文工程的加持下展现出对复杂上下文惊人的理解能力，但其生成同等复杂度长文本输出的能力却存在显著缺陷。弥合这一差距将成为未来研究的核心方向。最终，本综述为研究者与工程师推进上下文感知 AI 的发展提供了统一的理论框架。

## Test-Time Scaling with Reflective Generative Model
[基于反射生成模型的测试时缩放机制](https://arxiv.org/abs/2507.01951)

我们提出首个反射生成模型MetaStone-S1，通过新型反射生成架构实现与OpenAI o3-mini的性能对标。该架构聚焦高质量推理轨迹选择，包含两项创新：1) 策略与过程奖励模型的统一接口：共享主干网络并采用任务专用头部分别处理推理轨迹预测和评分，轨迹评分模块仅引入53M额外参数；2) 消除过程级标注依赖：提出自监督过程奖励模型，可直接基于结果奖励学习高质量推理轨迹选择。该反射生成架构使MetaStone-S1原生支持测试时缩放，我们根据可控推理步长提供三种计算强度模式（低、中、高）。实验表明，MetaStone-S1仅用32B参数量即达到与OpenAI o3-mini系列相当的性能。为促进研究发展，我们已将MetaStone-S1开源至https://github.com/MetaStone-AI/MetaStone-S1。

## Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination
[推理还是记忆？数据污染导致的强化学习结果不可靠性](https://arxiv.org/abs/2507.10532)

大语言模型(LLMs)的推理能力始终是研究领域的重点方向。近期研究通过强化学习(RL)进一步提升了模型性能，多项新方法报告称在极少或无需外部监督的情况下即可实现显著改进。值得注意的是，部分研究甚至发现随机或错误的奖励信号也能提高推理表现。然而，这些突破主要出现在Qwen2.5模型系列中，并在MATH-500、AMC和AIME等知名基准测试上获得验证，而Llama等其他模型则无法复现类似效果，这一现象需要深入研究。我们的分析表明，虽然Qwen2.5展现出强大的数学推理能力，但其基于大规模网络文本的预训练过程使其容易受到常见基准测试中数据污染的影响，导致相关测试结果可能缺乏可靠性。为此，我们开发了一个生成器，能够创建任意长度和难度的全合成算术问题，由此构建了一个名为RandomCalculation的无污染数据集。基于这些无数据泄漏的数据集，我们证实只有准确的奖励信号能持续提升模型性能，而噪声信号或错误信号则无效。我们建议在无污染的基准测试和多样化模型架构上评估RL方法，以确保研究结论的可信性。

## Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning
[Open Vision Reasoner：迁移语言认知行为用于视觉推理](https://arxiv.org/abs/2507.05255)

大语言模型 (LLMs) 的卓越推理能力源自其通过可验证奖励强化所获得的认知行为。本研究探索如何将这一机制迁移至多模态大语言模型 (MLLMs) 以实现高级视觉推理。我们基于 Qwen2.5-VL-7B 提出了两阶段训练范式：首先进行大规模语言冷启动微调，随后实施近 1,000 步的多模态强化学习 (RL)，其训练规模超越了此前所有开源工作。这项开创性研究揭示了三个关键发现：(1) 由于语言心理意象的作用，行为迁移在冷启动阶段即早期显现；(2) 冷启动阶段会广泛学习视觉行为特征，而 RL 阶段则能有效筛选并放大优质模式；(3) 迁移过程会优先保留高价值行为，如视觉反思能力。最终得到的 Open-Vision-Reasoner (OVR) 模型在多项推理基准测试中达到领先水平，包括 MATH500 (95.3%)、MathVision (51.8%) 和 MathVerse (54.6%)。我们公开了模型、数据及训练过程，以推动开发性能更强、行为一致的多模态推理系统。

## NeuralOS: Towards Simulating Operating Systems via Neural Generative Models  
[NeuralOS：基于神经生成模型的操作系统仿真](https://arxiv.org/abs/2507.08800)  

我们提出 NeuralOS 神经框架，该框架通过直接预测用户输入（如鼠标移动、点击和键盘事件）对应的屏幕帧来实现操作系统图形用户界面（GUI）的仿真。NeuralOS 整合了状态跟踪循环神经网络（RNN）与基于扩散的神经渲染器，后者负责生成屏幕图像。模型训练采用大规模 Ubuntu XFCE 操作记录数据集，包含随机交互和 AI 智能体执行的拟真交互。实验表明，NeuralOS 能够生成逼真的 GUI 操作序列，精确捕捉鼠标交互行为，并稳定预测应用程序启动等状态转换。尽管对键盘细粒度交互的精确建模仍存在挑战，但 NeuralOS 为构建未来自适应生成式神经人机交互系统奠定了基础。

##  Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs 
[迈向智能化的深度推理 RAG：大语言模型中 RAG-推理系统综述](https://arxiv.org/abs/2507.09477)  

检索增强生成 (Retrieval-Augmented Generation, RAG) 通过引入外部知识增强了大语言模型 (Large Language Models, LLMs) 的事实性，但在多步推理任务上仍有不足；而纯推理方法则易出现幻觉或事实依据错误。本文从推理-检索协同的视角整合这两种范式。首先系统分析高级推理技术如何优化 RAG 各阶段 (推理增强型 RAG)；进而阐述不同类型检索知识如何补充缺失前提并扩展复杂推理的上下文 (RAG 增强型推理)；最后重点探讨新兴的协同 RAG-推理框架——(智能化) 大语言模型通过搜索与推理的迭代交织，在知识密集型基准测试中实现了最先进性能。我们系统梳理了相关方法、数据集与开放挑战，并展望未来研究方向：构建更高效、多模态自适应、可信且以人为本的深度 RAG-推理系统。资源合集详见 https://github.com/DavidZWZ/Awesome-RAG-Reasoning。

## Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Image Generation
[视觉基础模型作为自回归图像生成的高效视觉Tokenizers](https://arxiv.org/abs/2507.08441)

基于预训练视觉基础模型(传统用于视觉理解任务)的强大表征能力，我们探索了一个新方向：直接在此类模型上构建图像tokenizer，该方向目前研究尚不充分。具体实现中，我们采用冻结的视觉基础模型作为tokenizer的编码器。为提升性能，我们引入了两个关键组件：(1) 区域自适应量化框架，能够减少规则2D网格上预训练特征的冗余；(2) 语义重建目标，通过将tokenizer输出与基础模型表征对齐来保持语义保真度。基于这些设计，我们提出的图像tokenizer VFMTok在图像重建和生成质量上取得显著提升，同时提高了token效率。该方法进一步提升了自回归(AR)生成性能——在ImageNet基准测试中达到2.07的gFID，模型收敛速度加快三倍，并且无需使用分类器无关引导(CFG)即可实现高保真类别条件合成。代码将开源以促进社区发展。

## VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning
[VisionThink：基于强化学习的智能高效视觉语言模型](https://arxiv.org/abs/2507.13348)  

近期视觉语言模型（Vision Language Models, VLMs）通过大幅增加视觉 Token 数量提升了性能，这些视觉 Token 的数量通常远超文本 Token。然而我们发现，大多数实际场景并不需要如此庞大的视觉 Token。虽然在部分 OCR 相关任务中性能会显著下降，但模型在仅使用 1/4 分辨率时，仍能在绝大多数通用 VQA 任务中保持准确性能。为此，我们提出动态调整样本分辨率的解决方案，并构建了新型视觉 Token 压缩范式 VisionThink。该模型从低分辨率图像出发，智能判断当前分辨率是否足以解决问题；若不足，则输出特殊 Token 请求更高分辨率图像。与现有采用固定剪枝比率或阈值压缩 Token 的高效 VLM 方法不同，VisionThink 能够按需压缩 Token。这使得模型在 OCR 相关任务中展现出优异的细粒度视觉理解能力，同时在简单任务上显著减少视觉 Token 消耗。我们采用强化学习框架，提出 LLM-as-Judge 策略成功实现 RL 在通用 VQA 任务中的应用，并通过精心设计的奖励函数与惩罚机制，实现了稳定的图像分辨率切换频率控制。大量实验验证了该方法在性能、效率和实用性方面的优势。代码已开源：https://github.com/dvlab-research/VisionThink。

## Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation
[混合递归：学习动态递归深度以实现自适应 Token 级计算](https://arxiv.org/abs/2507.10524)

扩展语言模型能够展现出强大的能力，但随之而来的计算和内存需求使得训练和部署成本高。现有的效率优化工作通常集中于参数共享或自适应计算，而如何同时实现两者仍是一个未解决的问题。我们提出了混合递归 (Mixture-of-Recursions, MoR)，这是一个统一的框架，将这两个效率方面结合在一个递归 Transformer 中。MoR 通过跨递归步骤重用共享的层堆栈来实现参数效率，同时轻量级路由器通过动态为单个 Token 分配不同的递归深度来实现 Token 级的自适应计算。这使得 MoR 能够仅对当前递归深度下活跃的 Token 进行二次注意力计算，并通过选择性缓存仅它们的键值对来进一步提高内存访问效率。除了这些核心机制外，我们还提出了一种 KV 共享变体，该变体重用第一次递归的 KV 对，专为降低预填充延迟和减少内存占用而设计。在从 1.35 亿到 17 亿参数的模型规模范围内，MoR 构建了新的帕累托前沿：在相同训练 FLOPs 和较小模型规模下，它显著降低验证困惑度并提升少样本准确率，同时与普通和现有的递归基线相比提供了更高的吞吐量。这些收益表明，MoR 是一条无需承担大模型成本即可实现大模型质量的有效途径。

## CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive Neural Rendering
[CLiFT：面向计算高效与自适应神经渲染的压缩光场Token](https://arxiv.org/abs/2507.08776)

本文提出了一种神经渲染方法，将场景表示为"压缩光场Token(CLiFTs)"，保留了场景丰富的外观(appearance)和几何信息。CLiFT通过压缩Token实现计算高效的渲染，同时能够动态调整Token数量来表征场景或使用单一训练网络渲染新视角。具体实现包含：给定输入图像集，多视角编码器结合相机位姿完成图像Token化；在潜在空间中使用K均值算法选取缩减后的光线作为聚类中心；多视角"冷凝器"将所有Token信息压缩至中心Token以构建CLiFTs。测试阶段根据目标视角和计算预算(CLiFT数量)，系统收集指定数量的邻近Token，并采用计算自适应渲染器合成新视角。在RealEstate10K和DL3DV数据集上的实验从定量和定性角度验证了该方法，在渲染质量相当的情况下实现显著数据压缩，获得最高综合渲染评分，同时提供数据规模、渲染质量与速度的灵活权衡。

## Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities
[Gemini 2.5: 以先进推理、多模态、长上下文和下一代智能体能力突破边界](https://arxiv.org/abs/2507.06261)

在本报告中，我们介绍 Gemini 2.X 模型家族：Gemini 2.5 Pro 和 Gemini 2.5 Flash，以及早期版本 Gemini 2.0 Flash 和 Flash-Lite。Gemini 2.5 Pro 是我们当前最强大的模型，在尖端编码和推理基准测试中实现了最先进(SoTA)性能。除了卓越的编码和推理能力外，Gemini 2.5 Pro 还是一款具备多模态理解能力的认知模型，可处理长达3小时的视频内容。其独特的长上下文、多模态和推理能力相结合，能够实现新型智能体工作流。Gemini 2.5 Flash 以较低的计算资源和延迟要求提供出色的推理能力，而 Gemini 2.0 Flash 和 Flash-Lite 则在低延迟和低成本条件下保持高性能。整体而言，Gemini 2.X 模型系列覆盖了模型能力与成本的完整帕累托边界，使开发者能够探索复杂智能体问题求解的极限。

## SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation  
[SpeakerVid-5M：面向视听双人交互式人类生成的大规模高质量数据集](https://arxiv.org/abs/2507.09862)  

大规模模型的快速发展推动了数字人领域的重大突破。这些先进方法为虚拟形象驱动和渲染提供了高保真解决方案，使学术界将目光投向下一个重大挑战：视听双人交互式虚拟人。为促进这一新兴领域的研究，我们推出SpeakerVid-5M数据集——首个专为视听双人交互式虚拟人生成设计的大规模高质量数据集。该数据集总时长超过8,743小时，包含超过520万段人物肖像视频片段，涵盖多种尺度和交互类型，包括单人说话、倾听及双人对话。值得注意的是，该数据集沿两个关键维度构建：交互类型和数据质量。首先，根据交互场景分为四种类型（对话分支、单分支、倾听分支和多轮分支）；其次，划分为大规模预训练子集和经过筛选的高质量监督微调（SFT）子集。这种双重结构可支持各类2D虚拟人任务。此外，我们提供了基于该数据训练的自回归（AR）视频聊天基线模型，同时配套开发了专用评估指标集和测试数据VidChatBench，作为未来研究的基准。数据集及相应数据处理代码将公开。项目页面：https://dorniwang.github.io/SpeakerVid-5M/

