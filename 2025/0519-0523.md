## Qwen3 Technical Report  
[Qwen3 技术报告](https://arxiv.org/abs/2505.09388)  

本文介绍 Qwen 模型家族的最新版本 Qwen3。该系列包含基于密集架构和混合专家（Mixture-of-Expert, MoE）架构的大语言模型（LLMs），参数量级覆盖0.6亿至2350亿，在性能、效率及多语言能力方面实现显著突破。  

Qwen3 的核心创新在于将思考模式（支持复杂多步推理）与非思考模式（实现快速上下文响应）整合至统一框架，用户无需在聊天优化模型（如 GPT-4o）与专用推理模型（如 QwQ-32B）间手动切换，系统可根据查询内容或对话模板自动选择最优模式。同时引入的思考预算机制支持推理过程中动态分配计算资源，实现任务复杂度与响应延迟的智能平衡。  

通过迁移旗舰模型知识，我们在保证性能竞争力的前提下显著降低了小规模模型的训练成本。实验评估显示，Qwen3 在代码生成、数学推理、AI 智能体任务等基准测试中达到业界最优水平，性能可匹敌更大规模的 MoE 模型和商业闭源模型。相较于前代 Qwen2.5，其支持语言从29种扩展至119种语言及方言，通过增强跨语言理解与生成能力提升全球适用性。所有模型均基于 Apache 2.0 协议开源，以促进可复现研究及社区生态发展。

## Emerging Properties in Unified Multimodal Pretraining
[统一多模态预训练中的新兴特性](https://arxiv.org/abs/2505.14683)

多模态理解与生成的统一已在尖端专有系统中展现出卓越性能。本研究提出了BAGEL——一个原生支持多模态理解与生成的开源基础模型。BAGEL采用纯解码器架构，基于从海量交错文本、图像、视频及网络数据中精选的数万亿token进行预训练。当使用这种多样化多模态交错数据进行扩展时，BAGEL在复杂多模态推理任务中展现出新兴能力。实验表明，该模型在标准基准测试的多模态生成和理解任务上显著优于现有开源统一模型，同时具备高级多模态推理能力，包括自由形态图像编辑、未来帧预测、三维操作和场景导航等。为推动多模态研究发展，我们公开了关键技术发现、预训练细节、数据构建规范，并开源了模型代码与检查点。项目主页：https://bagel-ai.org/

## Chain-of-Model Learning for Language Model
[大语言模型的链式模型学习](https://arxiv.org/abs/2505.11820)

本文提出了一种新颖的学习范式——链式模型（Chain-of-Model，CoM），该范式以链式结构将因果关系融入各层隐藏状态，显著提升了模型训练的扩展效率和部署时的推理灵活性。我们提出链式表示（Chain-of-Representation，CoR）概念，将每层隐藏状态表示为多个子表示（即链）在隐藏维度上的组合。在每层中，输出表示的每个链仅能访问输入表示中位于其前的所有链。基于CoM框架构建的模型可通过逐步增加新链来扩展规模，并通过调整链数量实现弹性推理的多尺寸子模型。基于此原理，我们设计了链式语言模型（Chain-of-Language-Model，CoLM），将CoM思想融入Transformer架构的每一层。基于CoLM，我们进一步提出CoLM-Air，通过KV共享机制在首链计算所有键值并全局共享，该设计具备额外的可扩展特性，如实现无缝语言模型切换和预填充加速等功能。实验表明，CoLM系列模型在保持与标准Transformer相当性能的同时，提供了渐进式扩展提升训练效率、多尺寸弹性推理等优势，为大语言模型构建提供了新思路。代码将发布于：https://github.com/microsoft/CoLM。

## NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification 
[NovelSeek：当智能体成为科学家——构建从假设到验证的闭环系统](https://arxiv.org/abs/2505.16938)  

人工智能 (AI) 正在加速科学研究范式的变革，不仅提升了研究效率，更推动了创新突破。本文提出 NovelSeek，这是一个统一的闭环多智能体框架，可在多学科领域实现自主科学研究 (Autonomous Scientific Research, ASR)，帮助研究人员以前所未有的速度与精度解决复杂科学问题。NovelSeek 具备三大核心优势：1) 可扩展性：该框架已在 12 项科研任务中验证其普适性，能生成创新方案以优化基准代码性能；2) 交互性：通过自动化端到端流程中的人类专家反馈接口与多智能体交互机制，支持领域专家知识的无缝融合；3) 高效性：相较于人工研究，NovelSeek 在多个科学领域以极低时间成本取得显著性能提升。例如：在反应产率预测任务中，仅耗时 12 小时便将准确率从 27.6% 提升至 35.4%；在增强子活性预测中，处理 4 小时即实现准确率从 0.52 跃升至 0.79；在 2D 语义分割任务中，30 小时内就将精度从 78.8% 提高到 81.0%。

## Web-Shepherd: Advancing PRMs for Reinforcing Web Agents
[Web-Shepherd: 强化 Web 智能体的过程奖励模型进阶](https://arxiv.org/abs/2505.15277)

Web 导航是一个独特的领域，既能自动化现实生活中的诸多重复性任务，又因其需要超越典型多模态大语言模型 (MLLM) 任务的长跨度序列决策而具有挑战性。然而，迄今为止仍缺乏可在训练和测试阶段通用的专用 Web 导航奖励模型。尽管速度和成本效益至关重要，但先前工作都采用 MLLM 作为奖励模型，这对实际部署造成了严重制约。为此，我们提出首个过程奖励模型 (PRM) Web-Shepherd，能够实现步骤级别的 Web 导航轨迹评估。为实现这一目标，我们首先构建了 WebPRM Collection——一个包含 4 万组步骤级别偏好数据对、涵盖多领域多难度级别的带标注检查项清单的大规模数据集。同时，我们推出了首个 PRM 评估的元基准 WebRewardBench。实验表明，在 WebRewardBench 上，我们的 Web-Shepherd 相比 GPT-4o 实现了约 30 个百分点的准确率提升。在 WebArena-lite 测试中，当采用 GPT-4o-mini 作为策略模型、Web-Shepherd 作为验证器时，相比使用 GPT-4o-mini 作为验证器的方案，性能提升达 10.9 个百分点，同时成本减少 90%。我们的模型、数据集和代码已公开于 LINK。

## MMaDA: Multimodal Large Diffusion Language Models
[MMaDA：多模态大规模扩散语言模型](https://arxiv.org/abs/2505.15809)

我们提出MMaDA，这是一种新型多模态扩散基础模型系列，旨在文本推理、多模态理解和文本到图像生成等多个领域实现卓越性能。该方法的三大核心创新在于：(i) MMaDA采用具有共享概率公式和模态无关设计的统一扩散架构，无需特定模态组件即可实现跨数据类型的无缝集成与处理。(ii) 通过混合长思维链(CoT)微调策略构建跨模态的统一CoT格式，该策略通过对齐文本与视觉领域的推理过程，为最终强化学习(RL)阶段实现冷启动训练，从而显著提升模型处理复杂任务的初始能力。(iii) 我们提出UniGRPO算法，这是专为扩散基础模型设计的基于策略梯度的统一RL方法，通过多样化奖励建模统一了推理与生成任务的训练后优化，确保性能持续提升。

实验表明，作为统一多模态基础模型的MMaDA-8B展现出卓越的泛化能力：在文本推理任务上超越LLaMA-3-7B和Qwen2-7B，多模态理解性能优于Show-o和SEED-X，文本到图像生成质量超过SDXL和Janus。这些成果验证了MMaDA在统一扩散架构中有效缩小预训练与训练后阶段差距的能力，为未来研究提供了完整框架。项目代码与训练模型已开源：https://github.com/Gen-Verse/MMaDA

## AdaptThink: Reasoning Models Can Learn When to Think
[AdaptThink：推理模型的自适应思考决策](https://arxiv.org/abs/2505.13417)

近期，大语言模型通过类人深度推理机制在多项任务中展现出卓越性能。然而，冗长的推理过程显著增加了计算开销，使效率成为关键瓶颈。本研究首先证实，对于相对简单的任务，NoThinking（即跳过推理步骤直接输出结果）在性能和效率上均更具优势。基于此发现，我们提出AdaptThink——一种新型强化学习（RL）算法，用于训练推理模型根据问题难度自适应选择最优推理模式。该框架包含两个核心组件：(1) 约束优化目标，在保持整体性能的前提下激励模型采用NoThinking模式；(2) 重要性采样策略，通过在策略(on-policy)训练中平衡Thinking与NoThinking样本，实现模型冷启动，并支持训练过程中对两种推理模式的持续探索与利用。实验表明，AdaptThink在显著降低推理成本的同时进一步提升了模型性能。在三个数学数据集上，该方案使DeepSeek-R1-Distill-Qwen-1.5B的平均响应长度缩短53%，准确率提升2.4%，充分证明了自适应推理模式选择对于优化推理质量与效率平衡的价值。代码和模型已开源：https://github.com/THU-KEG/AdaptThink。

## Scaling Law for Quantization-Aware Training
[量化感知训练的缩放定律](https://arxiv.org/abs/2505.14302)

大语言模型 (LLMs) 需要消耗大量计算和内存资源，带来了部署挑战。量化感知训练 (QAT) 通过降低模型精度同时保持性能来应对这些挑战。然而，QAT 的缩放行为（尤其是 4 位精度 (W4A4) 下的表现）尚未被充分理解。现有 QAT 缩放定律往往忽略训练 token 数量和量化粒度等关键因素，从而限制了其适用性。本文提出了一种统一的 QAT 缩放定律，将量化误差建模为模型规模、训练数据量和量化组尺寸的函数。通过 268 次 QAT 实验，我们发现量化误差随模型规模增大而减小，但随训练 token 数量增加和量化粒度增大而上升。为探究 W4A4 量化误差来源，我们将其分解为权重和激活分量。这两个分量均遵循 W4A4 量化误差的总体趋势，但具有不同敏感性：具体表现为权重量化误差随训练 token 数量增加而更快上升。进一步分析表明，FC2 层中由异常值引发的激活量化误差是 W4A4 QAT 量化误差的主要瓶颈。通过采用混合精度量化解决该瓶颈，我们证明权重和激活量化误差可收敛至相近水平。此外，随着训练数据量增加，权重量化误差最终会超过激活量化误差，这表明在此类场景下降低权重量化误差同样关键。这些发现为 QAT 研究的改进与发展提供了重要洞见。

## SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training
[SageAttention3：面向推理的微缩放 FP4 注意力机制及 8 比特训练探索](https://arxiv.org/abs/2505.11594)  

注意力机制因其平方时间复杂度，效率问题至关重要。我们通过两项关键创新提升其效率：首先，基于 Blackwell GPU 的新型 FP4 Tensor Core 加速注意力计算，在 RTX5090 上实现 1038 TOPS 算力，较该平台最快 FlashAttention 提速 5 倍。实验证明 FP4 注意力可直接加速各类模型的推理。其次，我们首次将低比特注意力应用于训练领域。现有工作（如 FlashAttention3 和 SageAttention）仅针对推理优化，但大模型训练效率同样关键。为此我们设计了一种精确高效的 8 比特注意力，同时支持前向和反向传播。实验表明该方案在微调任务中保持无损精度，但预训练任务收敛速度较慢。代码发布于 https://github.com/thu-ml/SageAttention。

## Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models
[推理能力扩展与控制失效：大语言模型指令遵循能力评估](https://arxiv.org/abs/2505.14810)

指令遵循能力是大语言模型 (LLMs) 与用户意图对齐的关键。尽管当前以推理为核心的模型在复杂数学问题上表现出色，但其遵循自然语言指令的能力仍缺乏深入研究。本研究提出 MathIF 基准，专门用于评估数学推理任务中的指令遵循能力。实证分析表明，增强推理能力与保持可控性之间存在固有矛盾：推理能力越强的模型往往越难遵循用户指令。我们发现，基于蒸馏长思维链调优或采用推理导向强化学习训练的模型，其指令遵循性能会随生成长度增加而下降。实验证明，即使简单干预也能部分恢复模型的指令遵循行为，但会损害推理性能。这些发现揭示了当前大语言模型训练范式的根本矛盾，表明我们需要开发更具指令感知能力的推理模型。相关代码和数据已发布于 https://github.com/TingchenFu/MathIF。

## AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning
[AdaCoT：基于强化学习的帕累托最优自适应思维链触发方法](https://arxiv.org/abs/2505.11896)

大语言模型 (LLMs) 展现出卓越能力，但在复杂推理任务中仍面临挑战。尽管思维链 (Chain-of-Thought, CoT) 提示能显著提升推理性能，但其无条件地为所有查询生成长推理步骤会导致高昂计算成本和效率损失，尤其对简单输入更为明显。为解决这一问题，我们提出 AdaCoT (自适应思维链) 框架，使大语言模型能自适应决策 CoT 触发时机。AdaCoT 将自适应推理建模为帕累托优化问题，平衡模型性能与 CoT 调用成本 (频率及计算开销)。我们采用基于强化学习 (Reinforcement Learning, RL) 的方法，通过近端策略优化 (Proximal Policy Optimization, PPO) 动态调节惩罚系数来控制 CoT 触发决策边界，使模型能根据隐式查询复杂度判断 CoT 需求。关键技术贡献是选择性损失掩码 (Selective Loss Masking, SLM)，用于防止多阶段 RL 训练中决策边界崩溃，确保自适应触发机制的鲁棒性。实验显示，AdaCoT 成功实现了帕累托最优，对无需复杂推理的查询显著降低 CoT 使用率。例如在生产流量测试集上，AdaCoT 将 CoT 触发率降至 3.18%，平均响应 Token 数量减少 69.06%，同时保持复杂任务的高性能。

## MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly
[MMLongBench：长上下文视觉-语言模型评测基准的全面构建](https://arxiv.org/abs/2505.10610)

随着大视觉-语言模型 (Large Vision-Language Models) 上下文窗口的快速扩展，长上下文视觉-语言模型 (Long-Context Vision-Language Models, LCVLMs) 应运而生，这类模型能单次前向处理数百张图像与交错排列的文本 token (Text Tokens)。本文提出首个覆盖多元化长上下文视觉-语言任务的评测基准 MMLongBench，该基准包含 13,331 个样本，涵盖视觉检索增强生成 (Visual Retrieval-Augmented Generation, Visual RAG) 、多样本上下文学习 (Many-Shot In-Context Learning) 等五大任务类别，并覆盖自然图像与合成图像等多种图像类型。通过融合视觉块 (Vision Patches) 与文本 token 的跨模态分词 (Tokenization) 方案，所有样本均以五种标准化输入长度（8K-128K token）呈现，以评估模型对输入长度的鲁棒性。基于对 46 个闭源与开源 LCVLM 的系统评测，我们得出以下结论：(1) 单一任务性能不能有效反映整体长上下文能力；(2) 现有模型在长上下文视觉-语言任务中普遍存在不足，表明该领域仍有显著提升空间；(3) 具有更强推理能力的模型通常展现更优的长上下文性能。MMLongBench 通过广泛的任务覆盖、多样化的图像类型及严格的长度控制，为诊断和推动下一代 LCVLM 发展提供了关键基础。

## GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning  
[GuardReasoner-VL：基于强化推理的视觉语言模型安全防护框架](https://arxiv.org/abs/2505.11049)  

针对视觉语言模型（Visual Language Models, VLM）的安全性问题，本文提出新型推理防护模型GuardReasoner-VL。其核心创新在于通过在线强化学习（Reinforcement Learning, RL）机制，驱动防护模型在内容审核前执行多步推理决策。具体实现包含三个阶段：首先构建多模态推理数据集GuardReasoner-VLTrain（包含12.3万样本、63.1万推理步骤，覆盖文本/图像/图文混合输入）；随后通过监督微调（Supervised Fine-Tuning, SFT）初始化模型推理能力；最终采用在线RL优化审核推理性能。关键技术包括：(1) 通过拒绝采样与安全感知数据拼接策略提升样本多样性与难度；(2) 动态调整剪裁参数以平衡训练早期的探索性与后期的稳定性；(3) 设计综合准确率、格式合规性与计算成本的token长度感知奖励函数。实验表明，该模型F1分数平均领先次优方案19.27%。项目已开源3B/7B参数模型及完整资源：https://github.com/yueliu1999/GuardReasoner-VL/

## UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning
[UniVG-R1: 基于强化学习的推理引导通用视觉定位](https://arxiv.org/abs/2505.14231)

传统视觉定位方法主要针对基于简单文本描述的单图像场景。然而，当应用于包含隐含语义和复杂指令的现实场景时，特别是在多图像场景下，现有方法面临重大挑战，这主要源于跨模态上下文推理能力的不足。本研究致力于解决更具实用性的通用视觉定位任务，提出UniVG-R1——一种基于推理引导的多模态大语言模型(Multimodal Large Language Model, MLLM)，通过强化学习(Reinforcement Learning, RL)与冷启动数据的结合来增强推理能力。具体而言，我们首先构建了高质量的思维链(Chain-of-Thought, CoT)定位数据集，其中标注了细粒度推理链，通过监督微调使模型遵循正确的推理路径。随后实施基于规则的强化学习，激励模型识别正确推理链以提升其推理能力。此外，我们发现因简单样本占比过高导致的难度偏差问题，并提出基于难度感知的权重调整策略来进一步提升性能。实验结果表明，UniVG-R1在MIG-Bench基准上实现了最先进性能，较之前方法提升9.1%。该模型还展现出强大的泛化能力，在四个图像视频推理定位基准测试中，零样本性能平均提升23.4%。项目页面详见：https://amap-ml.github.io/UniVG-R1-page/。

## Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning
[Tool-Star：基于强化学习的多工具协同推理大语言模型](https://arxiv.org/abs/2505.16410)

近年来，大语言模型 (LLMs) 借助大规模强化学习 (Reinforcement Learning, RL) 展现出卓越的推理能力。然而，如何运用 RL 算法实现 LLMs 的高效多工具协同推理仍是一个待解决的难题。本文提出 Tool-Star——一个基于 RL 的框架，旨在使 LLMs 能够在分步推理过程中自主调用多个外部工具。该框架整合了六类工具，并在数据合成与训练环节采用了系统性设计。针对工具使用数据稀缺的问题，我们提出通用化的工具集成推理数据合成流程，通过结合工具集成提示 (tool-integrated prompting) 和基于提示的采样 (hint-based sampling)，实现自动化、可扩展的工具使用轨迹生成。后续通过质量归一化和难度感知分类流程，过滤低质量样本并将数据集按难度梯度组织。此外，我们设计了两阶段训练框架以提升多工具协同推理能力：(1) 冷启动微调 (cold-start fine-tuning)，通过工具调用反馈引导 LLMs 探索推理模式；(2) 采用分层奖励机制的自批判多工具 RL 算法，深化对奖励机制的理解并优化工具协作效率。在超过 10 个高难度推理基准测试上的实验分析验证了 Tool-Star 的优越性能。项目代码已开源：https://github.com/dongguanting/Tool-Star。

## Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective
[扩散模型与自回归语言模型：文本嵌入视角](https://arxiv.org/abs/2505.15045)

大语言模型嵌入模型通过大规模预训练与微调，已在文档检索等通用文本嵌入任务上超越基于 BERT 和 T5 的模型。然而，LLM 嵌入的核心局限在于其自回归预训练采用单向注意力机制，这与文本嵌入任务的双向特性存在本质冲突。为此，我们提出采用扩散语言模型进行文本嵌入，因其固有的双向架构及近期在推理任务上匹配甚至超越 LLM 的表现。我们首次对扩散语言嵌入模型进行了系统性研究，实验表明：该模型在长文档检索任务中性能较 LLM 嵌入模型提升 20%，在推理密集型检索中提升 8%，在指令跟随检索中提升 2%，并在传统文本嵌入基准上表现优异。分析证实，双向注意力机制对长文本和复杂文本的全局上下文编码具有关键作用。

## Visual Planning: Let's Think Only with Images
[视觉规划：让我们仅用图像思考](https://arxiv.org/abs/2505.11409)

随着大语言模型 (LLMs) 及其多模态扩展 (MLLMs) 的发展，机器在多样化任务中的推理能力得到了显著提升。然而，这些模型即使面对视觉信息时，仍主要采用纯文本作为表达和结构化推理的载体。本文提出，在处理涉及空间与几何信息的任务时，语言未必是最自然或最高效的推理模态。基于此，我们提出了一种新型范式——视觉规划 (Visual Planning)，该范式通过完全独立于文本的纯视觉表征进行规划。具体而言，规划过程由编码视觉推理步骤的图像序列实现，其机制类似于人类通过草图或心理意象预演未来行动。我们开发了基于强化学习的视觉规划框架 (VPRL)，结合 GRPO 算法对大型视觉模型进行后训练优化，在 FrozenLake、Maze 和 MiniBehavior 等典型视觉导航任务中实现了显著的规划性能提升。实验表明，视觉规划范式在纯文本推理的所有对比方案中表现最优。这些成果证实了视觉规划作为语言推理替代方案的可行性与优势，为需要直观图像推理的任务提供了新的技术路径。

## Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction
[Delta Attention：基于 Delta 校正的快速准确稀疏注意力推理](https://arxiv.org/abs/2505.11254)

Transformer 的注意力机制具有平方级复杂度 (quadratic complexity)，在处理长序列时会导致高昂的推理成本和延迟。然而，注意力矩阵通常具有稀疏性，这意味着大部分计算项可以从计算中省略以实现高效推理。虽然稀疏注意力推理方法能够显著降低计算负担，但往往伴随着严重的性能下降问题。我们发现这种性能下降的主要原因在于：稀疏计算会引发注意力输出的分布偏移 (distributional shift)。这种偏移会导致解码阶段的查询无法与预填充阶段的关键键值良好对齐，从而影响模型性能。我们提出了一种简单而新颖的分布偏移校正方法，能够使稀疏注意力输出的分布逼近标准二次注意力的分布特性。该方法可兼容任何稀疏注意力方法，在 131K RULER 基准测试中平均提升 36 个百分点 (percentage points) 的性能表现。当应用于带 sink token 的滑动窗口注意力方法时，可恢复标准二次注意力 88% 的准确率，且仅引入极小额外开销。我们的方法能保持 98.5% 的稀疏度 (相对于完整二次注意力)，在处理 100 万 token 的预填充序列时，其推理速度可达 Flash Attention 2 的 32 倍。

## Thinkless: LLM Learns When to Think
[Thinkless：大语言模型学习何时思考](https://arxiv.org/abs/2505.13379)

具有长链式思维推理(chain-of-thought reasoning)能力的推理语言模型，在需要复杂逻辑推断的任务中已展现出卓越性能。然而，对所有查询均采用详尽推理(elaborate reasoning)往往会导致显著的计算效率低下，特别是当许多问题可采用直接解法时。这引出了一个开放性问题：大语言模型能否学会何时思考？为此，我们提出Thinkless框架，该可学习框架使大语言模型能够根据任务复杂度和模型能力，自适应选择简短响应(<short>)或深度推理(<think>)。Thinkless采用强化学习范式进行训练，其核心是解耦分组相对策略优化(Decoupled Group Relative Policy Optimization, DeGRPO)算法。该算法将混合推理的学习目标分解为：(1) 控制token损失，用于决策推理模式选择；(2) 响应损失，用于提高生成答案的准确性。这种解耦设计实现了对各目标贡献的细粒度控制，既稳定了训练过程，又有效避免了原始GRPO中出现的性能塌陷问题。实证结果表明，在Minerva Algebra、MATH-500和GSM8K等多个基准测试中，Thinkless能够将长链式思维的使用量降低50%-90%，显著提升了推理语言模型的效率。代码已开源：https://github.com/VainF/Thinkless

