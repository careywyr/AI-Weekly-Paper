## Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning
[反思、重试、奖励：通过强化学习实现大语言模型的自我提升](https://arxiv.org/abs/2505.24726)

我们提出了一种基于自我反思和强化学习的大语言模型性能提升方法。当模型回答错误时，通过激励其生成更高质量的反思内容，我们证明即使无法合成训练数据且仅能获得二元反馈信号，模型解决复杂可验证任务的能力仍能得到显著提升。该框架包含两个阶段：(1) 任务失败时，模型需生成分析先前尝试的反思性文本；(2) 模型在获得反思内容后重新尝试解决该任务。若重试成功，则对反思阶段生成的Token（词元）给予奖励。实验结果显示，该方法在不同架构模型上均取得显著效果提升，其中数学方程编写任务提升达34.7%，函数调用任务提升18.1%。特别值得注意的是，经过微调的中小规模模型（15亿至70亿参数）表现优于同架构下参数规模大10倍的基准模型。这一创新范式为开发具备有限反馈条件下自我提升能力的语言模型提供了新思路，有望推动构建更实用可靠的大语言模型系统。

## Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning
[超越80/20法则：高熵少数Token驱动大语言模型推理的有效强化学习](https://arxiv.org/abs/2506.01939)

强化学习与可验证奖励（RLVR）已成为提升大语言模型（LLMs）推理能力的有效方法，但其工作机制尚未被充分理解。本研究首次从Token熵分布模式的新视角对RLVR进行探索性分析，系统性地揭示了不同Token对推理性能的影响机制。通过分析思维链式（CoT）推理中的Token熵值分布，我们发现仅有少量Token呈现高熵特征，这些Token作为关键决策点引导模型进入不同的推理路径。进一步研究RLVR训练过程中熵值分布的演化规律表明，RLVR主要保留了基础模型的熵分布特征，其优化重点集中于高熵Token的熵值调节。这一发现凸显了高熵Token（即决策Token）在RLVR中的核心作用。我们通过将策略梯度更新限定于决策Token来优化RLVR，并得出了一个超越传统80/20法则的结论：仅使用20%的Token子集，在Qwen3-8B基础模型上即可达到与全梯度更新相当的性能，而在Qwen3-32B（AIME'25提升11.04分，AIME'24提升7.71分）和Qwen3-14B（AIME'25提升4.79分，AIME'24提升5.21分）基础模型上显著优于全梯度更新，展现出显著的扩展性优势。与之形成鲜明对比的是，仅对80%最低熵Token进行训练会导致性能明显下降。这些结果表明RLVR的有效性主要源于对决定推理方向的高熵Token的优化。总体而言，我们的研究揭示了通过Token熵分布视角理解RLVR工作机制的可能性，并证明了利用高熵少数Token优化RLVR以进一步提升大语言模型推理性能的技术潜力。

## ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models
[ProRL：持续强化学习拓展大语言模型的推理边界](https://arxiv.org/abs/2505.24864)

近期以推理为核心的语言模型研究进展表明，强化学习（RL）是一种极具前景的方法，可实现模型与可验证奖励信号的对齐。然而学界仍存在争议：RL究竟是真正拓展了模型的推理能力，还是仅放大了基础模型分布中已有的高奖励输出模式；同时，持续增加RL计算量是否能稳定提升推理性能。本研究通过实证表明，持续强化学习（ProRL）训练能够发现基础模型即便经过大量采样也无法获得的新型推理策略，从而挑战了现有假设。我们提出ProRL这一新型训练方法，其整合了KL散度控制、参考策略重置以及多样化任务集。实验分析显示，经过RL训练的模型在各类pass@k评估中均显著优于基础模型，包括那些基础模型无论如何尝试都完全失败的场景。研究还发现，推理边界的拓展程度与基础模型的任务胜任度及训练时长呈强相关性，说明RL能够随时间推移探索并填充解空间的新区域。这些发现为理解RL如何实质性拓展语言模型推理边界提供了新见解，并为长周期RL推理研究奠定了基础。我们公开模型权重以促进后续研究：
https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B

## AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time
[AlphaOne：测试时推理模型的快慢思考机制](https://arxiv.org/abs/2505.24863)

本文提出 AlphaOne ($\alpha$1) 框架，该框架可在测试时对大推理模型 (LRMs) 的推理过程进行动态调节。$\alpha$1 首先定义 $\alpha$ 时刻，即采用统一参数 $\alpha$ 的缩放思考阶段。在此预 $\alpha$ 阶段，系统通过将推理转换 Token 的插入建模为伯努利随机过程，实现慢速思考的动态切换。当达到 $\alpha$ 时刻后，系统会确定地输出思考结束 Token 来终止慢速思考，转而进入快速推理和高效答案生成阶段。该方法通过实现灵活的慢速-快速推理调节，统一并扩展了现有的单调缩放方法。在数学、编程和科学领域的多项挑战性基准测试中，大量实验数据验证了 $\alpha$1 的卓越推理能力和效率。项目页面：https://alphaone-project.github.io/

## SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics
[SmolVLA：用于经济高效机器人的视觉-语言-动作模型](https://arxiv.org/abs/2506.01844)

基于大规模多模态数据集预训练的视觉语言模型（VLMs）编码了丰富的视觉与语言知识，为机器人技术提供了强大基础。不同于从头训练机器人策略，近期研究将VLMs适配为视觉-语言-动作（VLA）模型，支持自然语言驱动的感知与控制。然而现有VLA模型通常规模庞大（参数量常达数十亿），导致训练成本高昂且实际部署受限。此外，这些模型依赖学术与工业数据集，忽略了经济型机器人平台社区收集数据日益丰富的优势。本文提出SmolVLA——一种小型、高效且社区驱动的VLA模型，在保持竞争力的同时显著降低训练与推理成本。SmolVLA设计为单GPU可训练，并支持消费级GPU甚至CPU部署。通过引入异步推理架构，将感知/动作预测与动作执行解耦，配合分块动作生成技术实现更高控制速率。尽管模型紧凑，SmolVLA性能可比肩10倍规模VLA模型。我们在仿真与真实机器人基准测试中对SmolVLA进行全面评估，并开源全部代码、预训练模型及训练数据。

## Time Blindness: Why Video-Language Models Can't See What Humans Can?
[时间感知缺陷：为何视频语言模型无法理解人类可识别的时间模式？](https://arxiv.org/abs/2505.24867)

视觉语言模型 (Vision-Language Models, VLMs) 的最新进展在视频时空关系理解方面取得了显著突破。然而当空间信息被遮蔽时，这些模型难以捕捉纯时间维度的模式特征。我们提出 $\textbf{SpookyBench}$ 基准测试集，其中信息仅通过噪声化帧序列的时间编码呈现，模拟了从生物信号传导到隐蔽通信等多种自然现象。值得注意的是，人类观察者能以超过98%的准确率识别这些序列中的形状、文本和模式，而当前最先进的VLMs准确率却为0%。这一性能差距揭示了关键缺陷：模型过度依赖单帧空间特征，且缺乏从时间维度提取语义信息的能力。此外，在空间信噪比 (Signal-to-Noise Ratio, SNR) 较低的数据集上训练时，模型的时间理解能力退化速度远超人类感知水平，尤其在需要细粒度时间推理 (fine-grained temporal reasoning) 的任务中表现更差。解决这一局限需要开发能够分离空间依赖与时间处理的新型架构或训练范式。我们的系统分析表明，该问题在不同规模与架构的模型中普遍存在。我们发布SpookyBench基准集，旨在推动时间模式识别研究，弥合人类与机器在视频理解方面的差距。数据集与代码已发布于项目网站：https://timeblindness.github.io/。

## MiMo-VL Technical Report
[MiMo-VL 技术报告](https://arxiv.org/abs/2506.03569)

我们开源了 MiMo-VL-7B-SFT 和 MiMo-VL-7B-RL 这两个强大的视觉语言模型，在通用视觉理解和多模态推理任务中均实现了最先进的性能。MiMo-VL-7B-RL 在 40 项评测任务中的 35 项上超越 Qwen2.5-VL-7B，并在 OlympiadBench 基准测试中获得 59.4 分，性能优于参数规模达 780 亿的模型。在 GUI 定位任务中，该模型以 56.1 分的成绩在 OSWorld-G 上创下新纪录，甚至超越了 UI-TARS 等专用模型。我们的训练方法结合了四阶段预训练（2.4 万亿 token）与混合策略强化学习 (Mixed On-policy Reinforcement Learning, MORL)，整合了多种奖励信号。研究发现，在预训练阶段引入具有长推理链 (Chain-of-Thought) 的高质量推理数据至关重要，同时混合强化学习 (mixed RL) 尽管面临多领域同步优化的挑战，仍能带来显著收益。我们还发布了一个包含 50 多项任务的完整评测集，以提升研究可复现性并推动领域发展。模型检查点与完整评测集已开源：https://github.com/XiaomiMiMo/MiMo-VL。

## REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards
[REASONING GYM：基于可验证奖励的强化学习推理环境](https://arxiv.org/abs/2505.24760)

我们推出 Reasoning Gym (RG)，这是一个为强化学习设计的推理环境库，其核心特征是可验证的奖励机制。该库包含超过 100 个数据生成器和验证器，覆盖代数、算术、计算科学、认知科学、几何学、图论、逻辑学以及多种常见游戏等多个领域。与多数传统推理数据集不同，RG 的关键创新在于能够生成复杂度可调的近乎无限训练数据。这种过程化生成 (procedural generation) 方法支持跨多难度级别的连续性评估。实验结果表明，RG 能有效支持推理模型的性能评估和强化学习训练。

## UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation
[UniWorld：面向统一视觉理解与生成的高分辨率语义编码器](https://arxiv.org/abs/2506.03147)

虽然现有统一模型在视觉语言理解与文本到图像生成任务中表现优异，但其图像感知与操作能力仍存在局限——而这些能力正是实际应用场景中日益关键的需求。近期，OpenAI 推出的 GPT-4o-Image 模型展现了卓越的全方位图像感知与操作能力，引起业界广泛关注。通过精心设计的实验，我们发现 GPT-4o-Image 可能采用语义编码器而非传统 VAE（变分自编码器）进行特征提取，尽管后者通常被视为图像操作任务的核心组件。基于这一发现，我们提出 UniWorld-V1 框架：该统一生成框架通过融合多模态大语言模型与对比语义编码器提取的语义特征构建而成。仅需 270 万训练样本，UniWorld-V1 即可在图像理解、生成、操作及感知等多元任务中实现卓越性能。我们已完整开源 UniWorld-V1 框架，包含模型权重、训练评估脚本及数据集，以推动研究复现与后续探索。

## VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in Multi-Agent Environments
[VS-Bench：评估多智能体环境中视觉语言模型的战略推理与决策能力](https://arxiv.org/abs/2506.02387)

视觉语言模型（Vision Language Models，VLMs）的最新进展已将其能力扩展到交互式智能体任务场景，但现有基准仍局限于单智能体或纯文本环境。相比之下，现实场景通常涉及多个智能体在丰富的视觉和语言环境中交互，面临多模态观察（multimodal observations）和战略交互的双重挑战。为弥合这一差距，我们提出了视觉战略基准（Visual Strategic Bench，VS-Bench），这是一个多模态基准，用于评估多智能体环境中VLMs的战略推理和决策能力。VS-Bench包含八个视觉基元环境，涵盖合作、竞争和混合动机交互，旨在评估智能体预测他人未来行动和优化长期目标的能力。我们考虑了两个互补的评估维度，包括通过下一动作预测准确率（next-action prediction accuracy）进行战略推理的离线评估，以及通过标准化回合回报（normalized episode return）进行决策的在线评估。对14个领先视觉语言模型（VLM）的广泛实验表明，当前模型与最优性能存在显著差距，最佳模型的预测准确率为47.8%，标准化回报为24.3%。我们进一步对VLM智能体的多模态观察、测试时扩展、社会行为和失败案例进行了深入分析。通过标准化评估并突出现有模型的局限性，我们期望VS-Bench成为未来战略多模态智能体研究的基础。代码和数据可在https://vs-bench.github.io获取。

## SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis
[SynthRL: 基于可验证数据合成的视觉推理能力扩展](https://arxiv.org/abs/2506.02096)

采用可验证奖励强化学习 (RLVR) 训练的视觉语言模型 (VLMs) 在测试阶段计算效率扩展方面取得了显著进展。本研究探索了合成强化学习数据对 RLVR 的改进潜力，提出了 \textbf{SynthRL}——一种面向推理任务的强化学习训练自动数据扩展方案，具有可扩展性和质量保证。SynthRL 包含三个核心环节：(1) 筛选符合目标分布的种子问题，(2) 在保持原答案的前提下生成更具挑战性的问题变体，(3) 通过验证环节确保问题正确性并提升难度级别。实验结果表明，当应用于 MMK12 数据集时，SynthRL 能从约 8,000 个种子样本中合成 3,300 多个可验证的高难度问题。使用合成数据训练的模型在五个跨领域视觉数学推理基准测试中均取得稳定提升，显著优于仅使用种子数据训练的基线模型。特别值得注意的是，在最具挑战性的评估样本上性能提升尤为显著，这证明 SynthRL 能有效激发更深层、更复杂的推理能力。

## CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs
[CSVQA: 面向视觉语言模型 STEM 推理能力评估的中文多模态基准](https://arxiv.org/abs/2505.24120)

视觉语言模型 (Vision-Language Models, VLMs) 在多模态理解领域已取得显著进展，但其科学推理能力仍缺乏系统评估。现有主流多模态基准主要针对通用图像理解或文本驱动的推理任务，未能涵盖需要融合领域专业知识与视觉证据分析的典型科学场景。为此，我们提出 CSVQA 基准，该基准通过基于 STEM 领域的视觉问答任务，专门用于评估模型的科学推理能力。该数据集包含 1,378 个精心设计的问答对，覆盖多个 STEM 学科，每个问题均要求模型具备领域知识、视觉证据整合能力以及高阶推理技能。相较于现有基准，CSVQA 更加注重真实科学场景和复杂推理过程。我们还设计了一套严谨的评估方案，通过人工标注的解释性文本，系统验证模型预测是否具有合理的中间推理过程。对 15 个主流 VLMs 的测试结果表明，各模型性能存在显著差距，即使表现最佳的商用模型准确率也仅为 49.6%。这一结果凸显了提升 VLMs 科学推理能力的紧迫性。CSVQA 数据集已发布于 https://huggingface.co/datasets/Skywork/CSVQA。

## Large Language Models for Data Synthesis
[面向数据合成的大语言模型](https://arxiv.org/abs/2505.14752)

生成能准确反映现实世界分布统计特性的合成数据是数据建模的核心挑战。传统方法通常基于强参数假设或人工结构设计，难以处理高维或异构数据。大语言模型 (LLMs) 的最新进展表明，它们可作为现实世界分布的灵活高维先验。然而在数据合成应用中，标准LLM采样方法存在效率低下、受限于固定上下文长度、且难以保证统计一致性的问题。为此，我们提出LLMSynthor框架，通过分布反馈将LLM转化为结构感知的模拟器。该框架将LLM视为建模高阶依赖关系的非参数copula模拟器，并采用LLM提案采样方法生成基础提案分布，无需拒绝采样即可提升效率。通过在统计特征空间最小化差异，该迭代合成流程能同步实现真实数据与合成数据的对齐，并逐步发现和优化潜在生成结构。我们在隐私敏感型领域（如电子商务、人口统计和移动轨迹）的异构数据集上进行了实验验证，涵盖结构化和非结构化数据格式。结果表明，LLMSynthor生成的合成数据具有高统计保真度、实际应用价值和跨数据集适应能力，可广泛应用于经济学、社会科学、城市规划等多个领域。

