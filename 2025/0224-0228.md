## LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers
[LLM-Microscope: 揭示 Transformer 上下文记忆中标点符号的隐藏作用](https://arxiv.org/abs/2502.15007)

我们介绍了一些方法来量化大语言模型 (LLMs) 如何编码和存储上下文信息，揭示了通常被视为次要的 Token（例如，限定词、标点符号）携带了令人惊讶的丰富上下文信息。值得注意的是，移除这些 Token——尤其是停用词、冠词和逗号——即使仅移除不相关的 Token，也会持续降低在 MMLU 和 BABILong-4k 上的性能。我们的分析还显示了上下文化和线性之间的强相关性，其中线性衡量了从一层嵌入到下一层的转换能够通过单一线性映射近似的程度。这些发现强调了填充 Token 在维护上下文中的隐藏重要性。为了进一步探索，我们提出了 LLM-Microscope，这是一个开源工具包，用于评估 Token 级别的非线性、评估上下文记忆、可视化中间层的贡献（通过改进的 Logit Lens），并测量表示的内在维度。该工具包揭示了看似微不足道的 Token 如何对长距离上下文理解至关重要。

## SurveyX: Academic Survey Automation via Large Language Models
[SurveyX: 基于大语言模型的学术调查自动化](https://arxiv.org/abs/2502.14776)

大语言模型 (Large Language Models, LLMs) 已经展示了卓越的理解能力和广泛的知识库，这表明 LLMs 可以作为自动化调查生成的高效工具。然而，最近与自动化调查生成相关的研究仍然受到一些关键限制的约束，如有限的上下文长度、缺失深入的内容讨论以及缺乏系统性的评估框架。受人类写作过程的启发，我们提出了 SurveyX，一个高效且组织良好的自动化调查生成系统，将调查撰写过程分解为两个阶段：准备阶段和生成阶段。通过创新性地引入在线参考检索、一种称为 AttributeTree 的预处理方法以及重新优化过程，SurveyX 显著提高了调查撰写的效率。实验验证结果表明，SurveyX 在内容质量（提升了 0.259）和引用质量（提升了 1.76）方面优于现有的自动化调查生成系统，在多个评估维度上逼近人类专家的表现。SurveyX 生成的调查示例可在 www.surveyx.cn 上查看。

## Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model
[Mol-LLaMA: 大分子语言模型中的分子通用理解](https://arxiv.org/abs/2502.13449)

理解分子是理解生物体和推动药物发现的关键，这需要跨化学和生物学的跨学科知识。尽管大分子语言模型在解释分子结构方面取得了显著成功，但其指令数据集主要依赖于任务导向数据集的特定知识，未能全面涵盖分子的基本特征，限制了其作为通用分子助手的能力。为解决这一问题，我们提出了 Mol-LLaMA，这是一个通过多模态指令调优掌握分子通用知识的大分子语言模型。为此，我们设计了涵盖分子基本特征的关键数据类型，并整合了分子结构中的核心知识。此外，为了提升对分子特征的理解，我们引入了一个模块，该模块整合了来自不同分子编码器的互补信息，充分利用了不同分子表示的独特优势。实验结果表明，Mol-LLaMA 能够理解分子的通用特征，并生成对用户查询的相关响应，同时提供详细解释，展现了其作为分子分析通用助手的潜力。

## MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction
[MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction](https://arxiv.org/abs/2502.11663)

能够根据动作预测环境变化的世界模型对于具备强泛化能力的自动驾驶模型至关重要。现有的驾驶世界模型主要基于视频预测模型。尽管这些模型能够利用先进的基于扩散的生成器生成高保真视频序列，但它们受到预测时长和整体泛化能力的限制。在本文中，我们尝试通过将生成损失与 MAE 风格的特征级上下文学习相结合来解决这一问题。具体而言，我们通过以下三个关键设计来实现这一目标：(1) 一个更具扩展性的 Diffusion Transformer (DiT) 结构，通过额外的掩码构建任务进行训练。(2) 我们设计了与扩散相关的掩码 Token (Mask Token)，以处理掩码重建与生成扩散过程之间的模糊关系。(3) 我们将掩码构建任务扩展到时空域，通过使用行级掩码进行移位自注意力，而非 MAE 中的掩码自注意力。然后，我们采用行级跨视图模块来确保与这种掩码设计的一致性。基于上述改进，我们提出了 MaskGWM：一种具有视频掩码重建功能的可泛化驾驶世界模型。我们的模型包含两个变体：MaskGWM-long，专注于长时预测，以及 MaskGWM-mview，致力于多视图生成。在标准基准上的综合实验验证了所提出方法的有效性，包括 Nuscene 数据集的正常验证、OpenDV-2K 数据集的长时滚动预测以及 Waymo 数据集的零样本验证。这些数据集上的定量指标表明，我们的方法显著改进了当前最先进的驾驶世界模型。

## PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data
[PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data](https://arxiv.org/abs/2502.14397)

我们提出了 PhotoDoodle，这是一种新颖的图像编辑框架，旨在帮助艺术家在照片上叠加装饰元素，从而实现照片涂鸦。照片涂鸦的难点在于插入的元素必须与背景无缝融合，这要求元素与背景在混合效果、透视关系和上下文一致性上达到逼真的效果。此外，背景必须保持原样不被扭曲，同时还需要从有限的训练数据中高效捕捉艺术家的独特风格。这些要求是之前主要关注全局风格迁移或区域修复的方法所未能解决的。PhotoDoodle 采用了两阶段训练策略：首先，我们使用大规模数据训练了一个通用图像编辑模型 OmniEditor；随后，我们通过 EditLoRA 对模型进行微调，使用艺术家精心挑选的小规模前后图像对数据集，以捕捉独特的编辑风格和技术。为了提升生成结果的一致性，我们引入了位置编码重用机制。此外，我们还发布了一个包含六种高质量风格的 PhotoDoodle 数据集。大量实验表明，我们的方法在定制图像编辑中表现出卓越的性能和鲁棒性，为艺术创作开辟了新的可能性。

## VLM²-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues
[VLM²-Bench: 深入探讨视觉语言模型如何隐式链接显式匹配的视觉线索](https://arxiv.org/abs/2502.12084)

视觉匹配线索的链接是日常生活中的一项关键能力，例如根据线索在多张照片中识别出同一个人，即使不知道他们是谁。尽管视觉语言模型 (VLMs) 拥有广泛的知识，但它们是否能够执行这一基本任务仍然在很大程度上未被探索。为了解决这个问题，我们引入了 VLM²-Bench，这是一个旨在评估 VLMs 是否能够视觉上链接匹配线索的基准，包含 9 个子任务和超过 3,000 个测试案例。通过对八个开源 VLMs 和 GPT-4o 的综合评估，以及对各种语言侧和视觉侧提示方法的进一步分析，我们总结了八项关键发现。我们识别出模型在链接视觉线索能力上的关键挑战，突出了一个显著的性能差距，即 GPT-4o 甚至落后于人类 34.80%。基于这些发现，我们建议 (i) 增强核心视觉能力以提高适应性并减少对先验知识的依赖，(ii) 建立更清晰的原则，将基于语言的推理整合到以视觉为中心的任务中，以防止不必要的偏见，以及 (iii) 将视觉-文本训练范式转向培养模型独立构建和推断视觉线索之间关系的能力。

## SIFT: Grounding LLM Reasoning in Contexts via Stickers
[SIFT: 通过上下文锚定大语言模型推理](https://arxiv.org/abs/2502.14922)

本文指出，在大语言模型 (LLM) 的推理过程中，上下文的误解可能是一个重要问题，从小型模型如 Llama3.2-3B-Instruct 到前沿模型如 DeepSeek-R1 都存在这一问题。例如，在短语“每公斤 10 美元”中，大语言模型可能无法识别“每”意味着“每个”，从而导致计算错误。我们引入了一种新颖的训练后优化方法，称为**坚持事实 (SIFT)** 来解决这一问题。SIFT 利用增加的推理时间计算，将大语言模型的推理锚定在上下文中。SIFT 的核心是 *上下文锚点*，它由模型自身生成，用于明确强调上下文中的关键信息。在给定精心设计的上下文锚点后，SIFT 生成两个预测——一个来自原始查询，另一个来自增加了上下文锚点的查询。如果它们不同，上下文锚点将通过 *前向优化*（以更好地将提取的事实与查询对齐）和 *逆向生成*（以符合模型的固有倾向）进行顺序细化，以获得更忠实的推理结果。通过对多种模型（从 3B 到 100B+）和基准测试（例如 GSM8K、MATH-500）的研究，揭示了性能的持续提升。值得注意的是，SIFT 将 DeepSeek-R1 在 AIME2024 上的单次通过准确率 (pass@1 accuracy) 从 78.33% 提高到 **85.67**%，在开源社区中建立了新的最先进水平。代码可在 https://github.com/zhijie-group/SIFT 获取。

## LightThinker: Thinking Step-by-Step Compression
[LightThinker: 逐步推理压缩](https://arxiv.org/abs/2502.15589)

大语言模型在复杂推理任务中表现出色，但其效率受到生成大量 Token 所需的内存和计算成本的限制。在本文中，我们提出了 LightThinker，这是一种新颖的方法，使大语言模型能够在推理过程中动态压缩中间思考。受人类认知过程的启发，LightThinker 将冗长的推理步骤压缩为紧凑表示，并丢弃原始推理链，从而显著减少存储在上下文窗口中的 Token 数量。我们通过数据构建训练模型何时以及如何执行压缩，将隐藏状态映射到简化的要点 Token，并创建专门的注意力掩码来实现这一目标。此外，我们引入了依赖度指标，通过测量生成过程中对历史 Token 的依赖程度来量化压缩程度。在四个数据集和两个模型上的多项实验表明，LightThinker 减少了峰值内存使用和推理时间，同时保持了具有竞争力的准确性。我们的工作为提高大语言模型在复杂推理任务中的效率而不牺牲性能提供了新的方向。代码将在 https://github.com/zjunlp/LightThinker 上发布。

## VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing
[VideoGrain: 调制时空注意力以实现多粒度视频编辑](https://arxiv.org/abs/2502.17258)

近年来，扩散模型（diffusion models）的进展显著提升了视频生成和编辑的能力。然而，多粒度视频编辑（multi-grained video editing），包括类级别、实例级别和部分级别的修改，仍然是一个巨大的挑战。多粒度编辑的主要困难包括文本到区域控制的语义不对齐以及扩散模型内部的特征耦合。为了解决这些困难，我们提出了 VideoGrain，一种零样本（zero-shot）方法，通过调制时空（交叉注意力和自注意力）机制来实现对视频内容的细粒度控制。我们通过增强每个局部提示（local prompt）对其对应空间解耦区域的注意力，同时最小化与交叉注意力中无关区域的交互，来增强文本到区域的控制。此外，我们通过增加区域内感知和减少区域间干扰来改进自注意力中的特征分离。广泛的实验结果表明，我们的方法在现实场景中实现了最先进的性能。我们的代码、数据和演示可在 https://knightyxp.github.io/VideoGrain_project_page/ 获取。

## Thus Spake Long-Context Large Language Model
[长上下文大语言模型的发展与挑战](https://arxiv.org/abs/2502.17129)

长上下文是自然语言处理（NLP）中的一个重要主题，贯穿于NLP架构的发展，并使大语言模型（LLMs）具备了类似人类的终身学习潜力。然而，追求长上下文伴随着许多挑战。尽管如此，长上下文仍然是LLMs的核心竞争优势。在过去的两年中，LLMs的上下文长度实现了突破性的增长，达到了数百万个Token。此外，长上下文LLMs的研究已经从长度外推扩展至对架构、基础设施、训练和评估技术的全面关注。

受到交响诗《查拉图斯特拉如是说》的启发，我们将LLM扩展上下文的旅程与人类试图超越其有限性的尝试进行了类比。在本调查中，我们将展示LLM在长上下文的巨大需求与接受其最终有限性的事实之间的权衡。为此，我们从四个角度提供了长上下文LLMs生命周期的全局图景：架构、基础设施、训练和评估，呈现了长上下文技术的全貌。在本调查的最后，我们将列出当前长上下文LLMs面临的10个未解问题。我们希望本调查能够作为长上下文LLMs研究的系统性介绍。

## Slamming: Training a Speech Language Model on One GPU in a Day
[Slam：在一天内用一块 GPU 训练语音语言模型](https://arxiv.org/abs/2502.15814)

我们介绍了 Slam，一种在 24 小时内使用单块学术 GPU 训练高质量语音语言模型 (SLMs) 的方法。我们通过对模型初始化和架构、合成数据、使用合成数据进行偏好优化以及微调所有其他组件的实验分析来实现这一点。我们通过实验证明，这种训练方法在更多计算资源下也能很好地扩展，以一小部分计算开销获得与领先 SLMs 相当的结果。我们希望这些见解能使 SLM 训练和研究更加容易。在 SLM 扩展规律的背景下，我们的结果远远超过了预测的计算最优性能，为 SLM 的可行性提供了乐观的展望。请参阅代码、数据、模型、样本 - https://pages.cs.huji.ac.il/adiyoss-lab/slamming 。

## DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks
[DICEPTION: 一种用于视觉感知任务的通用扩散模型](https://arxiv.org/abs/2502.17157)

我们的主要目标是创建一个良好的通用感知模型，能够在计算资源和训练数据的限制下处理多个任务。为了实现这一目标，我们借助在数十亿张图像上预训练的文本到图像扩散模型。我们的评估结果表明，DICEPTION 有效地处理了多个感知任务，性能与最先进模型相当。我们仅使用 SAM-vit-h 数据的 0.06%（例如，600K 对 1B 像素级注释图像）就达到了与其相当的结果。受 Wang 等人的启发，DICEPTION 使用颜色编码来表示各种感知任务的输出；我们展示了为不同实例分配随机颜色的策略在实体分割和语义分割中都非常有效。将各种感知任务统一为条件图像生成问题，使我们能够充分利用预训练的文本到图像模型。因此，与从头开始训练的传统模型相比，DICEPTION 可以以低几个数量级的成本高效训练。当我们的模型适应其他任务时，仅需对少至 50 张图像和 1% 的参数进行微调。DICEPTION 为视觉通用模型提供了宝贵的见解和更有前景的解决方案。

## Audio-FLAN: A Preliminary Release
[Audio-FLAN: 初步发布](https://arxiv.org/abs/2502.16584)

近年来，音频 Token 化技术的进步显著提升了大语言模型（LLMs）中音频能力的集成。然而，音频理解和生成通常被视为独立的任务，这阻碍了真正统一的音频-语言模型的发展。尽管指令微调在提高文本和视觉领域的泛化能力和零样本学习性能方面取得了显著成功，但其在音频领域的应用仍然鲜有探索。主要障碍之一是缺乏统一音频理解和生成的综合数据集。为此，我们推出了 Audio-FLAN，这是一个大规模的指令微调数据集，涵盖了语音、音乐和声音领域的 80 种任务，包含 1 亿多个实例。Audio-FLAN 为统一的音频-语言模型奠定了基础，这些模型能够以零样本的方式无缝处理各种音频领域的理解（如转录、理解）和生成（如语音、音乐、声音）任务。Audio-FLAN 数据集可在 HuggingFace 和 GitHub 上获取，并将持续更新。

## OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference
[OmniAlign-V: 迈向增强多模态大语言模型与人类偏好的对齐](https://arxiv.org/abs/2502.18411)

近期开源多模态大语言模型 (MLLMs) 的进展主要集中在增强基础能力上，但在人类偏好对齐方面仍存在显著差距。本文介绍了 OmniAlign-V，一个包含 20 万高质量训练样本的综合数据集，这些样本包含多样化的图像、复杂的问题和多种响应格式，旨在提高 MLLMs 与人类偏好的对齐。我们还提出了 MM-AlignBench，一个专门设计用于评估 MLLMs 与人类价值观对齐的人工标注基准。实验结果表明，使用监督微调 (SFT) 或直接偏好优化 (DPO) 对 MLLMs 进行微调，显著提升了与人类偏好的对齐，同时在标准 VQA 基准上保持或提升了性能，保留了其基础能力。我们的数据集、基准、代码和检查点已在 https://github.com/PhoenixZ810/OmniAlign-V 发布。

## SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution
[SWE-RL: 通过开源软件演化中的强化学习提升大语言模型的推理能力](https://arxiv.org/abs/2502.18449)

最近的 DeepSeek-R1 发布展示了强化学习 (RL) 在增强大语言模型 (LLMs) 的通用推理能力方面的巨大潜力。虽然 DeepSeek-R1 和其他后续工作主要集中在将 RL 应用于竞赛编程和数学问题上，但本文介绍了 SWE-RL，这是第一个将基于 RL 的大语言模型推理扩展到现实世界软件工程的方法。利用轻量级的基于规则的奖励 (例如，真实解决方案与 LLM 生成解之间的相似度得分)，SWE-RL 使 LLMs 能够通过从大量的开源软件演化数据中学习来自主恢复开发者的推理过程和解决方案 —— 这些数据记录了软件的整个生命周期，包括其代码版本快照、代码更改以及问题和拉取请求等事件。在 Llama 3 的基础上训练，我们得到的推理模型 Llama3-SWE-RL-70B 在 SWE-bench Verified 上达到了 41.0% 的解决成功率 —— 这是一个经过人工验证的真实世界 GitHub 问题集合。据我们所知，这是迄今为止中等规模 (<100B) LLMs 报告的最佳性能，甚至可以与 GPT-4o 等领先的专有 LLMs 相媲美。令人惊讶的是，尽管仅在软件演化数据上执行 RL，Llama3-SWE-RL 甚至展现出了通用的推理能力。例如，它在五个领域外任务上表现出改进的结果，即函数编码、库使用、代码推理、数学和一般语言理解，而监督学习微调的基线甚至平均导致性能下降。总体而言，SWE-RL 开辟了一条通过在大规模软件工程数据上进行强化学习来提高 LLMs 推理能力的新方向。

## SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference
[SpargeAttn: 精确稀疏注意力加速任何模型推理](https://arxiv.org/abs/2502.18137)

高效的注意力机制实现对于大模型至关重要，因为其计算复杂度为二次方。幸运的是，注意力机制通常具有稀疏性，即注意力图中的许多值接近于零，这使得可以省略相应的计算。许多研究已经利用这种稀疏模式来加速注意力计算。然而，大多数现有工作都集中在通过利用注意力图的特定稀疏模式来优化特定模型中的注意力计算。一个既能保证加速又能保证多样化模型端到端性能的通用稀疏注意力机制仍然难以实现。在本文中，我们提出了 SpargeAttn，一种适用于任何模型的通用稀疏和量化注意力机制。我们的方法采用了两阶段的在线筛选机制：在第一阶段，我们快速准确地预测注意力图，从而能够跳过注意力计算中的部分矩阵乘法。在第二阶段，我们设计了一个在线 softmax 感知筛选器，它不会引入额外开销，并且进一步跳过部分矩阵乘法。实验表明，我们的方法显著加速了包括语言、图像和视频生成在内的多样化模型，同时保持了端到端的性能指标。代码可在 https://github.com/thu-ml/SpargeAttn 获取。

## ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation
[ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation](https://arxiv.org/abs/2502.18364)

多层图像生成是一项基础任务，它使用户能够隔离、选择和编辑特定的图像层，从而彻底改变了与生成模型的交互方式。在本文中，我们介绍了匿名区域 Transformer (ART)，它基于全局文本提示和匿名区域布局，直接生成可变多层透明图像。受图式理论启发（该理论认为知识是以框架（图式）组织的，使人们能够通过将新信息与先验知识联系起来来解释和学习新信息），这种匿名区域布局允许生成模型自主决定哪些视觉 Token (Token) 应该与哪些文本 Token 对齐，这与之前主导的图像生成任务的语义布局形成对比。此外，分层区域裁剪机制仅选择属于每个匿名区域的视觉 Token，显著减少了注意力计算成本，并能够高效生成具有大量不同层（例如 50+）的图像。与全注意力方法相比，我们的方法速度超过 12 倍，并且表现出更少的层冲突。此外，我们提出了一种高质量的多层透明图像自动编码器，支持联合编码和解码可变多层图像的透明度。通过实现精确控制和可扩展的层生成，ART 为交互式内容创作建立了一个新范式。

## KV-Edit: Training-Free Image Editing for Precise Background Preservation
[KV-Edit: 用于精确背景保持的无训练图像编辑](https://arxiv.org/abs/2502.17363)

在图像编辑任务中，背景一致性仍然是一个关键挑战。尽管已有大量研究进展，现有方法仍需在保持与原始图像的相似性和生成与目标对齐的内容之间进行权衡。为此，我们提出了 KV-Edit，一种无需训练的方法，通过利用 DiTs 中的 KV 缓存来保持背景一致性。该方法保留背景 Token 而非重新生成，从而避免了复杂的机制或高成本的训练需求，最终生成的新内容能够无缝集成到用户指定区域的背景中。我们进一步分析了编辑过程中 KV 缓存的内存消耗，并通过无反转方法将空间复杂度优化为 O(1)。该方法与任何基于 DiT 的生成模型兼容，且无需额外训练。实验结果表明，KV-Edit 在背景和图像质量方面显著优于现有方法，甚至超越了基于训练的方法。项目网页地址为 https://xilluill.github.io/projectpages/KV-Edit。

## GHOST 2.0: generative high-fidelity one shot transfer of heads
[GHOST 2.0: 生成式高保真一次性头部交换](https://arxiv.org/abs/2502.18417)

尽管面部交换任务最近在研究界引起了广泛关注，但头部交换的相关问题仍然未被充分探索。除了肤色转移外，头部交换还带来了额外的挑战，例如在合成过程中需要保留整个头部的结构信息，并填补头部与背景之间的间隙。在本文中，我们提出了 GHOST 2.0 来解决这些问题，它由两个针对特定问题的模块组成。首先，我们引入了增强的 Aligner 模型用于头部重演，该模型在多个尺度上保留身份信息，并对极端姿态变化具有鲁棒性。其次，我们使用了一个 Blender 模块，通过转移肤色和修复不匹配区域，将重演后的头部无缝集成到目标背景中。这两个模块在相应任务上都优于基准模型，使得在头部交换任务中达到了最先进的结果。我们还解决了复杂情况，例如源图像和目标图像之间发型差异较大的情况。代码可在 https://github.com/ai-forever/ghost-2.0 获取。

## Kanana: Compute-efficient Bilingual Language Models
[Kanana: 计算高效的双语大语言模型](https://arxiv.org/abs/2502.18934)

我们介绍了 Kanana，一系列在韩语中表现超越性且在英语中具有竞争力的双语大语言模型。Kanana 的计算成本显著低于类似规模的先进模型。报告详细介绍了在预训练期间采用的技术，以实现计算效率高且具有竞争力的模型，包括高质量数据过滤、分阶段训练、深度扩展技术、剪枝与蒸馏技术。此外，报告还概述了在 Kanana 模型的后训练中使用的方法，包括监督微调技术和偏好优化技术，旨在增强其与用户无缝交互的能力。最后，报告详细阐述了用于语言模型适应特定场景的可行方案，例如嵌入技术、检索增强生成技术及函数调用技术。Kanana 模型系列涵盖从 2.1B 到 32.5B 参数，其中 2.1B 模型（基础、指令、嵌入）已公开发布，以促进韩语大语言模型的研究。

## TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding
[TheoremExplainAgent: 面向大语言模型定理理解的多模态解释](https://arxiv.org/abs/2502.19400)

理解特定领域的定理通常不仅依赖于文本推理；通过结构化的视觉解释进行有效沟通对于深入理解至关重要。尽管大语言模型 (LLMs) 在基于文本的定理推理中表现出色，但它们在生成连贯且具有教学意义的视觉解释方面仍面临挑战。在这项工作中，我们提出了 TheoremExplainAgent，这是一种利用 Manim 动画生成长篇定理解释视频（超过 5 分钟）的代理方法。为了系统评估多模态定理解释，我们构建了 TheoremExplainBench，这是一个涵盖多个 STEM 学科的 240 个定理的基准，并提供了 5 个自动化评估指标。我们的结果表明，代理规划对于生成详细的长篇视频至关重要，o3-mini 代理的成功率为 93.8%，总体得分为 0.77。然而，我们的定量和定性研究表明，大多数生成的视频在视觉元素布局方面存在一些小问题。此外，多模态解释揭示了基于文本的解释未能揭示的更深层次推理缺陷，突显了多模态解释的重要性。

## Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance
[Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance](https://arxiv.org/abs/2502.18772)

尽管希腊在全球经济中具有重要地位，但由于希腊语的语言复杂性和领域特定数据集的稀缺性，大语言模型 (LLMs) 在希腊金融领域的应用仍较少。先前在多语言金融自然语言处理 (NLP) 方面的努力已经揭示了显著的性能差距，但迄今为止尚未开发出专门的希腊金融基准测试或希腊特定的金融大语言模型。为了填补这一缺口，我们引入了 Plutus-ben，首个希腊金融评估基准，以及 Plutus-8B，首个经过希腊领域特定数据微调的希腊金融大语言模型。Plutus-ben 涵盖了希腊金融 NLP 的五个核心任务：数字和文本命名实体识别、问答、抽象摘要和主题分类，从而促进了系统化和可重复的大语言模型评估。为了支持这些任务，我们提出了三个新颖的高质量希腊金融数据集，这些数据集由母语为希腊语的专家全面注释，并补充了两个现有资源。我们对 22 个大语言模型在 Plutus-ben 上的评估表明，由于语言复杂性、领域特定术语和金融推理差距，希腊金融 NLP 仍然具有挑战性。这些发现表明跨语言迁移的局限性、希腊训练模型中金融专业知识的必要性，以及将金融大语言模型适应希腊文本的挑战。我们公开发布了 Plutus-ben、Plutus-8B 和所有相关数据集，以促进可重复的研究并推动希腊金融 NLP 的发展，从而在金融领域促进更广泛的多语言包容性。

## Towards an AI co-scientist
[迈向 AI 协作科学家](https://arxiv.org/abs/2502.18864)

科学发现依赖于科学家生成新颖的假设，并经过严格的实验验证。为了增强这一过程，我们引入了一个 AI 协作科学家，这是一个基于 Gemini 2.0 的多代理系统。AI 协作科学家的目的是帮助发现新的、原创的知识，并制定可证明新颖的研究假设和提案，基于先前的证据并与科学家提供的研究目标和指导保持一致。该系统的设计采用了生成、辩论和进化的假设生成方法，灵感来自科学方法，并通过扩展测试阶段的计算资源加速。关键贡献包括：(1) 一个多代理架构，具有异步任务执行框架，以实现灵活的计算扩展；(2) 一个锦标赛进化过程，用于自我改进的假设生成。自动化评估显示了测试阶段计算资源的持续提升，提高了假设质量。尽管具有通用性，但我们专注于三个生物医学领域的开发和验证：药物再利用、新靶点发现以及解释细菌进化和抗微生物耐药性的机制。对于药物再利用，系统提出了具有前景的验证结果的候选药物，包括在临床适用浓度下体外显示肿瘤抑制作用的急性髓性白血病候选药物。对于新靶点发现，AI 协作科学家提出了肝纤维化的新表观遗传靶点，通过抗纤维化活性和人肝类器官中的肝细胞再生得到了验证。最后，AI 协作科学家通过并行计算模拟发现了一种新的基因转移机制，重现了未发表的实验结果。这些结果在同时发布的独立报告中详细说明，展示了增强生物医学和科学发现的潜力，并开启了 AI 赋能科学家的时代。

## Language Models' Factuality Depends on the Language of Inquiry
[语言模型的事实性取决于查询所用的语言](https://arxiv.org/abs/2502.17955)

多语言语言模型 (LMs) 被期望在不同语言之间一致地提取事实知识，然而它们经常无法在语言之间迁移知识，即使它们在一种语言中拥有正确的信息。例如，我们发现，当用阿拉伯语询问时，一个语言模型可能正确地识别出 Rashed Al Shashai 来自沙特阿拉伯，但当用英语或斯瓦希里语询问时，却始终无法做到。为了系统地研究这一限制，我们构建了一个包含 13 种语言的 10,000 个国家相关事实的基准，并提出了三个新的指标：事实回忆分数、知识可转移性分数和跨语言事实知识可转移性分数，以量化不同语言之间语言模型的事实回忆和知识可转移性。我们的结果揭示了当今最先进的语言模型的基本弱点，特别是在跨语言泛化方面，模型无法有效地在不同语言之间迁移知识，导致性能不一致，且对使用的语言敏感。我们的发现强调了语言模型需要识别特定语言的事实可靠性，并整合跨语言中最可信的信息。我们发布了我们的基准和评估框架，以推动多语言知识迁移的未来研究。

## Self-rewarding correction for mathematical reasoning
[数学推理的自我奖励校正](https://arxiv.org/abs/2502.19613)

我们研究了具备自我奖励推理能力的大语言模型 (LLMs)，这些模型能够在推理过程中同时生成逐步推理并评估其输出的正确性，而无需外部反馈。这种综合方法使得单个模型能够独立地指导其推理过程，从而为模型部署提供了计算上的优势。我们特别关注自我校正这一代表性任务，其中模型能够自主检测其响应中的错误，修订输出，并决定何时终止迭代优化循环。为了实现这一目标，我们提出了一个两阶段的算法框架，仅使用自生成数据来构建具备自我奖励推理能力的模型。在第一阶段，我们采用顺序拒绝采样来合成包含自我奖励和自我校正机制的长链思维轨迹。通过对这些经过筛选的数据进行微调，模型能够学习自我奖励和自我校正的模式。在第二阶段，我们通过基于规则的信号强化学习进一步增强模型评估响应准确性和优化输出的能力。使用 Llama-3 和 Qwen-2.5 的实验表明，我们的方法超越了模型固有的自我校正能力，并实现了与依赖外部奖励模型的系统相当的性能。

## MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning
[MedVLM-R1: 通过强化学习增强视觉-语言模型 (VLMs) 的医学推理能力](https://arxiv.org/abs/2502.19634)

推理是推动医学图像分析发展的关键领域，其中透明度和可信度在临床医生信任和监管批准中至关重要。尽管医学视觉语言模型 (VLMs) 在放射学任务中显示出潜力，但大多数现有的 VLMs 仅生成最终答案，而不展示推理过程。为了解决这一问题，我们引入了 MedVLM-R1，这是一种通过生成自然语言推理来增强透明度和可信度的医学 VLM。MedVLM-R1 没有采用监督微调 (SFT)，因为后者容易过拟合训练数据，并且无法促进真正的推理，而是采用了强化学习框架，激励模型在不使用任何推理参考的情况下发现人类可解释的推理路径。尽管训练数据有限（600 个视觉问答样本）且模型参数量较少（20亿），MedVLM-R1 在 MRI、CT 和 X 射线基准测试中将准确率从 55.11% 提升至 78.22%，优于在超过一百万个样本上训练的更大模型。它还在分布外任务（out-of-distribution tasks）中展示了强大的领域泛化能力。通过将医学图像分析与显式推理相结合，MedVLM-R1 标志着在临床实践中迈向可信和可解释 AI 的关键一步。

## R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts
[R2-T2: 多模态专家混合模型的测试时重路由](https://arxiv.org/abs/2502.20395)

在大型多模态模型 (LMMs) 中，非语言模态（例如视觉表示）的感知通常无法与大语言模型 (LLMs) 的强大推理能力相匹配，这限制了 LMMs 在具有挑战性的下游任务中的表现。最近，通过用专家混合模型 (MoE) 替换视觉编码器，这一问题得到了缓解，MoE 提供了多样化下游任务所需的丰富、多粒度和多样化的表示。多模态 MoE 的性能在很大程度上取决于其路由器，该路由器为每个输入重新加权和混合不同专家的表示。然而，我们发现端到端训练的路由器并不总是为每个测试样本产生最优的路由权重。为了弥补这一差距，我们提出了一种新颖且高效的方法“测试时重路由 (R2-T2)”，该方法通过在测试样本的邻域中将其路由权重向量向正确预测样本的向量方向移动，从而在测试时局部优化路由权重向量。我们提出了三种具有不同优化目标和邻域搜索空间的 R2-T2 策略。R2-T2 在不训练任何基础模型参数的情况下，持续显著提升了最先进的 LMMs 在多样化任务的挑战性基准测试中的表现。

