## Towards Understanding Camera Motions in Any Video
[理解任意视频中的摄像机运动](https://arxiv.org/abs/2504.15376)

我们提出了CameraBench，这是一个用于评估和提升摄像机运动理解能力的大规模数据集与基准。CameraBench包含约3,000个多样化的互联网视频，所有视频均经过专家团队严格的多阶段质量控制流程标注。我们的核心贡献之一是与电影摄影师合作设计的摄像机运动基元分类体系。研究发现，诸如“跟随”（或追踪）等运动类型需要理解场景中的移动主体等语义内容。通过大规模人工标注实验，我们发现领域专业知识与教程式训练能显著提升标注准确性。例如，未经训练的人员可能混淆变焦（内参矩阵变化）与向前平移（外参位姿变化），但通过训练可以有效区分二者。基于CameraBench的评估表明，运动恢复结构（SfM）模型难以捕捉依赖场景内容的语义基元，而视频语言模型（VLMs）则难以准确估计需要精确轨迹的几何基元。我们进一步在CameraBench上微调了生成式视频语言模型，实现了两类优势的结合，并展示了其在运动增强视频描述生成、视频问答以及视频-文本检索等任务中的应用。期待本研究的分类体系、基准数据及教学资源能够推动实现"理解任意视频摄像机运动"的终极目标。

## Reinforcement Learning for Reasoning in Large Language Models with One Training Example
[基于单训练样本的大语言模型推理强化学习方法](https://arxiv.org/abs/2504.20571)

我们证明，使用单训练样本进行可验证奖励的强化学习（1-shot Reinforcement Learning with Verifiable Reward，简称1-shot RLVR）能有效增强大语言模型（LLM）的数学推理能力。将RLVR应用于基础模型Qwen2.5-Math-1.5B后，单个训练样本可使模型在MATH500上的准确率从36.0%提升至73.6%，同时在六个数学推理基准测试集上的平均性能从17.6%提升至35.7%。这一表现与使用包含该样本的1.2k规模DeepScaleR子集（MATH500：73.6%，平均：35.9%）相当。类似显著改进在多种模型（Qwen2.5-Math-7B、Llama3.2-3B-Instruct、DeepSeek-R1-Distill-Qwen-1.5B）、强化学习算法（GRPO和PPO）以及不同数学样本（多数样本作为单训练样本时可使MATH500性能提升约30%以上）中均得到验证。实验还揭示了1-shot RLVR的若干特性：跨领域泛化能力、自反思频率提升，以及训练准确率饱和后测试性能仍持续增强的"后饱和泛化（post-saturation generalization）"现象。通过分析策略梯度损失的主导作用，我们将1-shot RLVR与"grokking"现象进行了区分。研究还表明，在1-shot RLVR训练中，促进探索（如添加合适系数的熵损失）具有关键作用。值得注意的是，仅使用熵损失（不含结果奖励）就使Qwen2.5-Math-1.5B在MATH500上的性能提升27.4%。这些发现为RLVR的数据效率研究提供了新思路，并有助于深入理解RLVR的最新进展及其机制。代码、模型和数据已开源：https://github.com/ypwang61/One-Shot-RLVR

## The Leaderboard Illusion
[排行榜幻觉](https://arxiv.org/abs/2504.20879)

科学领域的进步离不开对发展进程的量化评估。随着基准测试重要性日益提升，其评估结果也更容易出现失真现象。Chatbot Arena 已成为评估最先进 AI 系统能力的权威排行榜。然而，本研究揭示了导致该评估体系失真的系统性缺陷。我们发现，非公开测试实践使得少数供应商能够获益——他们可以在公开发布前测试多个模型变体，并根据需要撤回评分结果。研究证实，这种选择性公开最佳性能结果的做法导致了 Arena 评分存在系统性偏差。极端案例中，我们识别出 Meta 在 Llama-4 发布前测试的 27 个未公开大语言模型变体。数据还表明，闭源商业模型相比开源(open-source)和开放权重(open-weight)模型具有更高对战频次，且被移出评估系统的概率更低。长期来看，这些政策导致了严重的数据获取不平等现象。例如，Google 和 OpenAI 分别获取了约 19.2% 和 20.4% 的 Arena 评估数据，而 83 个开放权重模型合计仅获得约 29.7% 的数据。我们的研究表明，获取 Arena 数据能带来显著优势：保守估计显示，即使少量额外数据也能使模型在 Arena 评估分布上的相对性能提升最高达 112%。这些机制共同导致了模型对 Arena 特定评估动态的过拟合，而非提升其通用质量。Arena 的建立离不开组织者和开源社区的共同努力。基于研究发现，我们提出可操作的改进建议，旨在优化 Chatbot Arena 评估框架，推动建立更公平、透明的行业基准测试体系。

## UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities
[UniversalRAG：面向多模态多粒度异构语料的检索增强生成](https://arxiv.org/abs/2504.20734)

检索增强生成（Retrieval-Augmented Generation, RAG）通过基于查询相关的外部知识生成响应，显著提升了模型的事实准确性。然而现有RAG方法大多仅支持纯文本语料库，虽近期研究已将其扩展至图像、视频等多模态数据，但仍局限于单一模态的专用语料库。现实场景中的查询需求具有高度多样性，单一类型知识源难以全面覆盖。为此，我们提出UniversalRAG框架，能够从多模态、多粒度的异构知识源中进行检索与知识融合。关键创新在于：研究发现将所有模态强行映射到统一表示空间会导致模态偏差（modality gap），即检索结果会偏向查询同源模态。为此我们设计模态感知路由机制，动态选择最优模态语料库进行定向检索。同时，每种模态数据还按粒度分级组织，实现查询复杂度自适应的精细化检索。在8个跨模态基准测试中，UniversalRAG性能显著优于单一模态和统一表示基线。

## Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning
[Skywork R1V2：多模态混合强化学习推理模型](https://arxiv.org/abs/2504.16656)

我们提出 Skywork R1V2，这是一款新一代多模态推理模型，相较于前代 Skywork R1V 实现了重大突破。R1V2 的核心创新在于引入混合强化学习范式，该范式结合了混合偏好优化 (Mixed Preference Optimization, MPO) 和组相对策略优化 (Group Relative Policy Optimization, GRPO)，通过协调奖励模型指导与基于规则的策略，解决了复杂推理能力与泛化性能难以兼顾的长期难题。为提升训练效率，我们设计了选择性样本缓冲 (Selective Sample Buffer, SSB) 机制，通过优化过程中优先选择高价值样本，有效缓解了 GRPO 固有的“优势消失”问题。研究发现，过强的强化信号会引发视觉幻觉现象，我们通过在训练过程中动态校准奖励阈值来系统监控和抑制这一现象。实验结果表明，R1V2 在多个基准测试中取得领先成绩：OlympiadBench 62.6 分、AIME2024 78.9 分、LiveCodeBench 63.6 分、MMMU 73.6 分。这些结果证明了 R1V2 超越现有开源模型的优势，并表明其在缩小与 Gemini 2.5、OpenAI-o4-mini 等顶级专有系统性能差距方面取得显著进展。为促进技术开放与可复现性，Skywork R1V2 模型权重已开源发布：https://huggingface.co/Skywork/Skywork-R1V2-38B。

## Sadeed: Advancing Arabic Diacritization Through Small Language Model
[Sadeed：基于小型语言模型的阿拉伯语变音标注技术突破](https://arxiv.org/abs/2504.21635)

由于阿拉伯语形态结构复杂，其文本变音标注始终是自然语言处理领域的重大挑战。本文提出Sadeed——一种基于Kuwain 1.5B（Hennara等人[2025]提出的紧凑型阿拉伯语多领域预训练模型）微调改进的纯解码器语言模型方案。我们通过严格的数据清洗与标准化流程构建了高质量的变音标注数据集用于模型微调。实验表明，尽管计算资源有限，Sadeed的性能不仅优于同领域的传统模型，还能与商业大语言模型相媲美。同时，本文揭示了当前阿拉伯语变音标注评测体系的主要缺陷，并为此提出了SadeedDiac-25评测基准，该基准支持跨文体、跨复杂度文本的公平全面评估。Sadeed模型与SadeedDiac-25基准共同为阿拉伯语机器翻译、语音合成及语言学习等NLP应用提供了可靠的技术基础。

## ReasonIR: Training Retrievers for Reasoning Tasks
[ReasonIR：面向推理任务的检索器训练](https://arxiv.org/abs/2504.20595)

我们推出ReasonIR-8B，这是首个专为通用推理任务训练的大规模检索器。现有检索器在推理任务中的性能提升有限，部分原因在于当前训练数据集主要包含与文档直接相关的简短事实性查询。我们开发了一套合成数据生成流程，可为每个文档生成具有挑战性的相关查询，同时构造看似相关但实际无效的困难负样本 (hard negative)。通过混合使用合成数据与公开数据集进行训练，ReasonIR-8B在广泛使用的推理密集型信息检索 (Information Retrieval, IR) 基准BRIGHT上取得了最新最优成绩：无重排器时nDCG@10达到29.9，使用重排器后提升至36.9。在检索增强生成 (Retrieval-Augmented Generation, RAG) 任务中，相较于闭卷基线 (closed-book baseline)，ReasonIR-8B将MMLU和GPQA的准确率分别提升6.4%和22.6%，表现优于其他检索器及搜索引擎。此外，ReasonIR-8B能更高效地利用推理时计算资源：在BRIGHT基准上，其性能随着查询语句的信息量增加而持续提升；与LLM (Large Language Model) 重排器结合使用时仍保持领先优势。我们的训练方案具有通用性，可轻松适配未来大语言模型。相关代码、数据及模型均已开源。

