## Feature-Level Insights into Artificial Text Detection with Sparse Autoencoders
[使用稀疏自编码器进行生成文本检测的特征级洞察](https://arxiv.org/abs/2503.03601)

随着先进的大语言模型的兴起，生成文本检测变得越来越重要。尽管有许多努力，但没有单一算法在不同类型的未见文本上表现一致，或保证对新的大语言模型的有效泛化。可解释性在实现这一目标中起着关键作用。在本研究中，我们通过使用稀疏自编码器从 Gemma-2-2b 残差流中提取特征来增强生成文本检测的可解释性。我们识别了具有可解释性和高效性的特征，通过领域和模型特定的统计、引导方法以及手动或基于大语言模型的解释来分析它们的语义和相关性。我们的方法提供了关于来自各种模型的文本与人类撰写内容如何不同的宝贵洞察。我们展示了现代大语言模型具有独特的写作风格，尤其是在信息密集的领域中，尽管它们可以通过个性化提示生成类似人类的输出。

## RuCCoD: Towards Automated ICD Coding in Russian
[RuCCoD: 俄语自动化 ICD 编码的探索](https://arxiv.org/abs/2502.21263)

本研究探讨了在生物医学资源有限的俄语环境中自动化临床编码的可行性。我们提出了一个新的用于 ICD 编码的数据集，该数据集包含来自电子健康记录 (EHRs) 的诊断字段，标注了超过 10,000 个实体和 1,500 多个独特的 ICD 编码。该数据集作为多个最先进模型的基准数据集，包括 BERT、带有 LoRA 的 LLaMA 和 RAG，并进行了额外的实验，考察跨领域（从 PubMed 摘要到医学诊断）和术语（从 UMLS 概念到 ICD 编码）的迁移学习。然后，我们将表现最佳的模型应用于标注一个包含 2017 年至 2021 年患者病史的内部 EHR 数据集。我们在精心策划的测试集上进行的实验表明，与医生手动标注的数据相比，使用自动化预测编码进行训练显著提高了准确性。我们相信，我们的研究结果为在资源有限的语言（如俄语）中自动化临床编码的潜力提供了宝贵的见解，这可以提高这些环境中的临床效率和数据准确性。

## Unified Reward Model for Multimodal Understanding and Generation
[统一奖励模型用于多模态理解和生成](https://arxiv.org/abs/2503.05236)

最近，人类偏好对齐技术的进展显著增强了多模态生成和理解能力。一个关键的方法是训练奖励模型以指导偏好优化过程。然而，现有模型通常是任务专用的，限制了它们在不同视觉应用中的适应性。我们还认为，联合学习多个任务的评估可能会产生协同效应，其中改进的图像理解能力可以增强图像生成评估，而精细的图像评估通过更好的帧分析使视频评估受益。为此，本文提出了UnifiedReward，第一个用于多模态理解和生成评估的统一奖励模型，支持成对排序和点评分，可用于视觉模型偏好对齐。具体来说，(1) 我们首先在我们构建的大规模人类偏好数据集上开发UnifiedReward，包括图像和视频生成/理解任务。(2) 然后，它被用于基于视觉模型自动构建高质量的偏好对数据，通过成对排序和点过滤逐步过滤它们的输出。(3) 最后，这些数据通过直接偏好优化(DPO)用于它们的偏好对齐。实验结果表明，联合学习评估多样视觉任务可以带来显著的相互益处，我们将我们的流程应用于图像和视频理解/生成任务，显著提高了每个领域的性能。

## Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural Vision-Language Dataset for Southeast Asia
[众包、爬取或生成？创建 SEA-VL，一个东南亚多文化视觉-语言数据集](https://arxiv.org/abs/2503.07920)

东南亚是一个具有非凡语言和文化多样性的地区，但在视觉-语言研究中仍然显著代表性不足。这通常导致人工智能模型无法捕捉东南亚文化的细微差别。为了填补这一空白，我们提出了 SEA-VL，一个致力于为东南亚语言开发高质量、文化契合度数据的开源项目。通过让来自东南亚国家的贡献者参与，SEA-VL 旨在确保更好的文化契合度和多样性，促进视觉-语言研究中代表性不足语言的更大包容性。除了众包，我们的项目在探索通过爬取和图像生成自动收集文化相关图像方面更进一步。首先，我们发现图像爬取实现了约 85% 的文化契合度，同时比众包更具成本和时间效率。其次，尽管生成视觉模型取得了显著进展，但生成图像在准确反映东南亚文化方面仍然不可靠。生成的图像通常无法反映该地区的文化细节和文化背景。总的来说，我们收集了 128 万张东南亚文化相关图像，比其他现有数据集大 50 倍以上。通过 SEA-VL，我们旨在弥合东南亚的代表性不足，促进开发更具包容性的人工智能系统，真实地代表东南亚的多元文化。

## Transformers without Normalization
[无需归一化的 Transformer](https://arxiv.org/abs/2503.10622)

归一化层在现代神经网络中广泛应用，并长期被视为不可或缺的组件。然而，本研究表明，无需归一化的 Transformer 可以通过一种简单的方法实现相同甚至更好的性能。我们提出了一种称为动态 Tanh (DyT) 的逐元素操作 $DyT($x$) = \tanh(\alpha $x$)$，作为 Transformer 中归一化层的替代方案。DyT 的灵感来源于 Transformer 中层归一化通常会产生类似 tanh 的 S 形输入输出映射。通过引入 DyT，无需归一化的 Transformer 在大多数情况下无需调整超参数即可匹配或超越其归一化对应模型的性能。我们在从识别到生成、从监督学习到自监督学习、从计算机视觉到语言模型等多种任务中验证了带有 DyT 的 Transformer 的有效性。这些发现挑战了传统观念，即归一化层在现代神经网络中是不可或缺的，并为深度网络中归一化层的作用提供了新的见解。

## LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL
[LMM-R1: 通过两阶段基于规则的强化学习提升 3B 参数大语言模型的推理能力](https://arxiv.org/abs/2503.07536)

提升大语言模型 (LMMs) 的推理能力面临独特挑战，主要源于视觉感知与逻辑推理之间的复杂交互，尤其是在 3B 参数的紧凑架构中，架构限制会制约推理能力和模态对齐。尽管基于规则的强化学习 (RL) 在纯文本领域表现优异，但其多模态扩展面临两大障碍：(1) 模糊答案和复杂推理示例稀缺导致的数据限制，(2) 多模态预训练引发的基础推理能力下降。为解决这些问题，我们提出了 \textbf{LMM-R1}，这是一个两阶段框架，通过 \textbf{基础推理增强 (FRE)} 和 \textbf{多模态泛化训练 (MGT)} 来适配多模态推理的基于规则的 RL。FRE 阶段首先利用纯文本数据和基于规则的 RL 增强推理能力，随后 MGT 阶段将这些能力泛化到多模态领域。

在 Qwen2.5-VL-Instruct-3B 上的实验表明，LMM-R1 在多模态和纯文本基准测试中分别比基线平均提升了 4.83\% 和 4.5\%，在复杂的足球游戏任务中提升了 3.63\%。这些结果验证了基于文本的推理增强能够有效实现多模态泛化，提供了一种高效的数据范式，避免了昂贵的高质量多模态训练数据需求。

## EuroBERT: Scaling Multilingual Encoders for European Languages
[EuroBERT: 为欧洲语言扩展多语言编码器](https://arxiv.org/abs/2503.05500)

通用多语言向量表示通常用于检索、回归和分类任务，这些表示传统上是通过双向编码器模型获得的。尽管编码器具有广泛的适用性，但近年来，仅生成解码器模型的进展使其逐渐被忽视。然而，推动这些进展的许多创新并非仅与解码器相关。在本文中，我们通过借鉴这些进展，重新审视多语言编码器的发展，并提出了 EuroBERT，这是一个涵盖欧洲及全球广泛使用语言的多语言编码器家族。我们的模型在多种任务上表现优异，包括多语言能力、数学和编码任务，并且原生支持长达 8,192 个 Token 的序列。我们还深入探讨了 EuroBERT 的设计决策，详细介绍了数据集组成和训练流程。我们公开发布了 EuroBERT 模型，包括中间训练检查点，以及完整的训练框架。

## SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models
[SEAP: 无需训练即可实现的稀疏专家激活剪枝，释放大语言模型的潜力](https://arxiv.org/abs/2503.07605)

大语言模型在各种自然语言处理任务中取得了显著的成功，但其在推理过程中的高计算成本仍然是一个主要瓶颈。本文提出了稀疏专家激活剪枝（SEAP），这是一种无需训练即可实现的剪枝方法，能够选择性保留任务相关参数以降低推理开销。受大语言模型中隐藏状态和激活的聚类模式启发，SEAP识别任务特定的专家激活模式，并在保持任务性能的同时提升计算效率。实验结果表明，SEAP显著降低了计算开销，同时保持了较高的准确性。值得注意的是，在50%的剪枝率下，SEAP的性能比WandA和FLAP高出20%以上；而在20%的剪枝率下，性能仅下降2.2%。这些结果表明SEAP具有良好的可扩展性和有效性，使其成为优化大规模大语言模型的有前途的方法。

## CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing
[CoSTAast: 用于多轮图像编辑的成本敏感工具路径智能体](https://arxiv.org/abs/2503.10613)

像稳定扩散（stable diffusion）和DALLE-3这样的文本到图像模型在多轮图像编辑方面仍然存在困难。我们将这样的任务分解为一个工具使用的智能工作流（路径），通过不同成本的AI工具解决一系列子任务。传统的搜索算法需要昂贵的探索来找到工具路径。虽然大语言模型（Large Language Models, LLMs）具备子任务规划的先验知识，但它们可能缺乏对工具能力和成本的准确估计，以确定在每个子任务中应用哪个工具。我们能否结合LLMs和图搜索的优势来找到成本效益高的工具路径？我们提出了一种三阶段方法“CoSTA*（Cost-Sensitive Toolpath Agent）”，利用LLMs创建子任务树，这有助于为给定任务修剪AI工具图，然后在小的子图上进行A*搜索（A* search）以找到工具路径。为了更好地平衡总成本和质量，CoSTA*结合了每个工具在每个子任务上的两个指标来指导A*搜索。然后，每个子任务的输出由视觉语言模型（Vision-Language Model, VLM）评估，失败将触发工具在该子任务上的成本和质量更新。因此，A*搜索可以快速从失败中恢复以探索其他路径。此外，CoSTA*可以在子任务之间自动切换模式，以实现更好的成本——质量权衡。我们构建了一个具有挑战性的多轮图像编辑的新基准，在该基准上，CoSTA*在成本和质量方面均优于最前沿的图像编辑模型或智能体，并根据用户偏好进行多种权衡。

## YuE: Scaling Open Foundation Models for Long-Form Music Generation
[YuE: 面向长篇幅音乐生成的开放基础模型扩展](https://arxiv.org/abs/2503.08638)

我们通过引入 YuE，一个基于 LLaMA2 架构的开放基础模型家族，来解决长篇幅音乐生成的任务——特别是具有挑战性的**歌词转歌曲**问题。具体来说，YuE 扩展到数万亿个 token，并生成长达五分钟的音乐，同时保持歌词对齐、连贯的音乐结构以及具有适当伴奏的引人入胜的声乐旋律。它通过以下方式实现这一目标：(1) 轨道解耦的下一 token 预测以克服密集混合信号，(2) 结构渐进条件化以实现长上下文歌词对齐，以及 (3) 多任务、多阶段的预训练方案以收敛和泛化。此外，我们重新设计了用于音乐生成的上下文学习技术，实现了多样化的风格转换（例如，将日本城市流行音乐转换为英语说唱，同时保留原始伴奏）和双向生成。通过广泛的评估，我们证明 YuE 在音乐性和声乐灵活性方面匹配甚至超越了一些专有系统。此外，微调 YuE 可以实现额外的控制和增强对尾部语言的支持。此外，除了生成之外，我们还展示了 YuE 学习到的表示在音乐理解任务中表现良好，其中 YuE 的结果在 MARBLE 基准测试中匹配或超越了最先进的方法。关键词：歌词转歌曲、歌曲生成、长篇幅、基础模型、音乐生成

## MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning
[MM-Eureka: 基于规则的大规模强化学习中的视觉顿悟探索](https://arxiv.org/abs/2503.07365)

我们提出了 MM-Eureka，这是一种多模态推理模型，成功地将基于规则的大规模强化学习 (RL) 扩展到了多模态推理领域。尽管基于规则的 RL 在提升大语言模型 (LLMs) 在文本领域的推理能力方面取得了显著成功，但其在多模态环境中的应用仍然具有挑战性。我们的工作在多模态空间中重现了基于文本的 RL 系统（如 DeepSeek-R1）的关键特征，包括准确度奖励和响应长度的稳步提升，以及反思行为的出现。我们展示了无论是经过指令调优还是预训练的模型，都可以通过基于规则的 RL 发展出强大的多模态推理能力，而无需监督微调，显示出相比其他方法更优的数据效率。我们开源了完整的流程，以促进该领域的进一步研究。我们在 https://github.com/ModalMinds/MM-EUREKA 发布了所有代码、模型、数据等。

## Charting and Navigating Hugging Face's Model Atlas
[绘制与探索 Hugging Face 的模型图谱](https://arxiv.org/abs/2503.10633)

随着数百万个公开可用的神经网络的出现，搜索和分析大型模型库的重要性日益凸显。面对如此众多的模型，需要一个图谱来进行有效的探索，但由于大多数模型的文档记录不完善，绘制这样的图谱颇具挑战性。为了挖掘模型库的潜在价值，我们绘制了一个初步的图谱，涵盖了 Hugging Face 中有文档记录的部分。该图谱提供了模型分布和演变的详细可视化展示。我们展示了该图谱的多种应用场景，包括预测模型属性（例如，准确度）以及分析计算机视觉模型的趋势。然而，由于当前的图谱仍不完整，我们提出了一种绘制未记录区域的方法。具体而言，我们基于现实世界中的主流模型训练实践，识别出高可靠性的结构先验。借助这些先验，我们的方法能够准确映射图谱中先前未记录的区域。我们公开了数据集、代码以及交互式图谱。

## Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models
[Block Diffusion: 在自回归和扩散语言模型之间进行插值](https://arxiv.org/abs/2503.09573)

扩散语言模型因其并行生成和可控性的潜力，提供了自回归模型所不具备的独特优势，但它们在似然建模方面表现不佳，且仅限于固定长度生成。在这项工作中，我们引入了一类块扩散语言模型，它们在离散去噪扩散和自回归模型之间进行插值。块扩散通过支持灵活长度生成，并利用 KV 缓存和并行 Token 采样提高推理效率，克服了这两种方法的关键限制。我们提出了一种构建高效块扩散模型的方案，包括高效训练算法、梯度方差估计器和数据驱动的噪声调度，以最小化方差。块扩散在语言建模基准测试中为扩散模型设定了新的最先进性能，并支持生成任意长度的序列。我们在项目页面上提供了代码、模型权重和博客文章：https://m-arriola.com/bd3lms/

## R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model
[R1-Zero 在 2B 非监督微调模型上的视觉推理中的“突破性进展”](https://arxiv.org/abs/2503.05132)

最近，DeepSeek R1 展示了如何通过简单的基于规则的激励进行强化学习，使大语言模型能够自主发展复杂的推理能力，其特征是“突破性进展”，即模型在训练过程中表现出自我反思和增加的回答长度。然而，尝试将这一成功扩展到多模态推理时，往往无法重现这些关键特征。在本报告中，我们首次成功地在仅使用非监督微调的 2B 模型上复制了这些多模态推理中的涌现特征。从 Qwen2-VL-2B 开始，并直接在 SAT 数据集上应用强化学习，我们的模型在 CVBench 上达到了 59.47% 的准确率，比基础模型高出约 30%，并且超过了监督微调设置约 2%。此外，我们分享了在使用 RL 与指令模型尝试实现类似 R1 的推理时的失败尝试和见解，旨在揭示其中的挑战。我们的关键观察包括：(1) 在指令模型上应用 RL 通常会导致简单的推理路径，(2) 单纯的长度激励在激发推理能力方面是无效的。项目代码可在 https://github.com/turningpoint-ai/VisualThinker-R1-Zero 获取。

## S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following with Paralinguistic Information
[S2S-Arena：评估带有副语言信息的语音到语音协议的指令跟随能力](https://arxiv.org/abs/2503.05085)

大语言模型（LLMs）的快速发展使得语音模型受到了广泛关注，尤其是最近支持语音输入和输出的语音到语音（Speech2Speech, S2S）协议的进展。然而，现有的基准测试主要采用基于文本的自动评估器来评估这些模型的指令跟随能力，缺乏对语音理解和生成中副语言信息的考虑。为了解决这些问题，我们提出了S2S-Arena，这是一个新颖的竞技场风格的S2S基准测试，旨在评估在真实世界任务中带有副语言信息的语音输入和语音输出的指令跟随能力。我们设计了154个样本，融合了四个领域的文本到语音（TTS）和现场录音，涉及21个任务，并以竞技场风格手动评估了现有的流行语音模型。实验结果表明：（1）除了GPT-4o的卓越表现外，级联自动语音识别（ASR）、大语言模型（LLM）和文本到语音（TTS）的语音模型在语音到语音协议中经过文本-语音对齐后，表现优于联合训练的模型；（2）考虑到副语言信息，语音模型的知识性主要依赖于LLM骨干，而其多语言支持则受限于语音模块；（3）优秀的语音模型已经能够理解语音输入中的副语言信息，但生成带有副语言信息的适当音频仍然是一个挑战。

